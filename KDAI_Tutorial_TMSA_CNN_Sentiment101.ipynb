{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pongpang-2102/Text_Minig_and_Sentiment_Analytics_KDAI_projects/blob/main/KDAI_Tutorial_TMSA_CNN_Sentiment101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWuLC2bsRM-h",
        "outputId": "1936a9b7-1db9-484d-b173-eb58b4c05ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-4.0.2-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2023.7.22)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl9KumP9UkXt",
        "outputId": "3785a51a-b23a-4011-afcc-10782e8be645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "Model = tf.keras.models.Model\n",
        "ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint\n",
        "ReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau\n",
        "load_model = tf.keras.models.load_model\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from pythainlp.tokenize import word_tokenize, Tokenizer\n",
        "KRTokenizer = tf.keras.preprocessing.text.Tokenizer\n",
        "\n",
        "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional, Embedding, Dropout, BatchNormalization\n",
        "# from tensorflow.keras.models import load_model\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle as p\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import string\n",
        "# from os import listdir\n",
        "from string import punctuation\n",
        "# from os import listdir\n",
        "\n",
        "#########################\n",
        "from pythainlp.tokenize import word_tokenize #, Tokenizer\n",
        "from pythainlp.corpus.common import thai_words\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu8YHwmGg44D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb4d074-fdd7-4c33-f1b9-d4a239b0afa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B7cw0-Zg8U7"
      },
      "outputs": [],
      "source": [
        "#os.chdir('drive/MyDrive/Sentiment Analysis Workshop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7_wednfhY7A"
      },
      "outputs": [],
      "source": [
        "#ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4dKi381hcGt"
      },
      "outputs": [],
      "source": [
        "train_input = \"/content/drive/MyDrive/# KDAI_TextMining/Sentiment 060823/train.txt\"\n",
        "train_label = \"/content/drive/MyDrive/# KDAI_TextMining/Sentiment 060823/train_label.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzPu6wjyVhjX"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "BS = 32\n",
        "DIMENSION = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGcTsIzVVtJR"
      },
      "outputs": [],
      "source": [
        "comments = []\n",
        "labels = []\n",
        "\n",
        "with open(train_input,encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        comments.append(line.strip())\n",
        "\n",
        "with open(train_label,encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        labels.append(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGvHOYIDV6K1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7e6cb393-cbbc-4729-bf5f-a1b3cc623c2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  category                                           comments\n",
              "0      neu  ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏£‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏¢‡∏≤‡∏™‡∏π‡∏ö‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏¥‡∏á‡∏õ‡πà‡∏≤‡∏ß‡∏Ñ‡∏±‡∏ö\n",
              "1      neu                                                 ‡∏Ñ‡∏∞\n",
              "2      neg                        ‡∏≠‡∏¥‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏≠‡∏≠‡∏°‡∏ó‡∏≥‡∏Å‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô‡πÄ‡∏≠‡πá‡∏°‡πÄ‡∏Ñ\n",
              "3      neu                                                üòÖüòÖüòÖ\n",
              "4      neu                            ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏ß‡∏±‡∏ô‡∏û‡∏∏‡∏ò ‡πÅ‡∏ô‡∏ô ‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15c72469-60e7-4bf1-b6aa-c7865582cd6d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏£‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏¢‡∏≤‡∏™‡∏π‡∏ö‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏à‡∏¥‡∏á‡∏õ‡πà‡∏≤‡∏ß‡∏Ñ‡∏±‡∏ö</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏Ñ‡∏∞</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏≠‡∏¥‡πÄ‡∏´‡∏µ‡πâ‡∏¢‡∏≠‡∏≠‡∏°‡∏ó‡∏≥‡∏Å‡∏π‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô‡πÄ‡∏≠‡πá‡∏°‡πÄ‡∏Ñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neu</td>\n",
              "      <td>üòÖüòÖüòÖ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏ß‡∏±‡∏ô‡∏û‡∏∏‡∏ò ‡πÅ‡∏ô‡∏ô ‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15c72469-60e7-4bf1-b6aa-c7865582cd6d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15c72469-60e7-4bf1-b6aa-c7865582cd6d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15c72469-60e7-4bf1-b6aa-c7865582cd6d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e85da495-d560-49a0-a521-1154a1200340\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e85da495-d560-49a0-a521-1154a1200340')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e85da495-d560-49a0-a521-1154a1200340 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df = pd.DataFrame({ \"category\": labels, \"comments\": comments })\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxK-C60cWUIU"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_KWNqdzWpO5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a2cc85f3-f5ac-4b55-80f0-f9984d4e5b20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                  comments\n",
              "1938       neu     ‡∏≠‡∏¢‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏ü‡∏£‡πà‡∏ô‡∏ü‡∏≤‡∏¢‡πÅ‡∏°‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏•‡∏î50% ‡∏°‡∏±‡∏ô‡∏≠‡πâ‡∏ß‡∏ôüòñüòñ\n",
              "14174      neu                            ‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÅ‡∏™‡∏á‡πÇ‡∏™‡∏°‡∏á‡∏±‡∏¢\n",
              "20120      neu  ‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏¢ ‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏≠‡∏∞‡πÑ‡∏£ ‡πÄ‡∏á‡∏¥‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô\n",
              "23299      neu                                   ‡∏™‡∏≤‡∏ò‡∏∏‡∏Ñ‡πà‡∏∞\n",
              "14882      neu                     ‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏à‡πà‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f51caddc-38fe-4780-a8ef-a543aec61966\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1938</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏≠‡∏¢‡πà‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏ü‡∏£‡πà‡∏ô‡∏ü‡∏≤‡∏¢‡πÅ‡∏°‡∏Ñ‡∏ï‡∏≠‡∏ô‡∏•‡∏î50% ‡∏°‡∏±‡∏ô‡∏≠‡πâ‡∏ß‡∏ôüòñüòñ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14174</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏Å‡πá‡πÄ‡∏≠‡∏≤‡πÅ‡∏™‡∏á‡πÇ‡∏™‡∏°‡∏á‡∏±‡∏¢</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20120</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏≤‡∏¢ ‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏≠‡∏∞‡πÑ‡∏£ ‡πÄ‡∏á‡∏¥‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23299</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡∏™‡∏≤‡∏ò‡∏∏‡∏Ñ‡πà‡∏∞</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14882</th>\n",
              "      <td>neu</td>\n",
              "      <td>‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡πÄ‡∏•‡∏¢ ‡∏à‡πà‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f51caddc-38fe-4780-a8ef-a543aec61966')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f51caddc-38fe-4780-a8ef-a543aec61966 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f51caddc-38fe-4780-a8ef-a543aec61966');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7953f0ad-cff7-4ee6-8d02-7567dc0b85dc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7953f0ad-cff7-4ee6-8d02-7567dc0b85dc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7953f0ad-cff7-4ee6-8d02-7567dc0b85dc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "neu_df = df[df.category == \"neu\"].sample(4300)\n",
        "neu_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J09d7ODYWuzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "aef8f393-d579-4f1d-ead2-21482d10a677"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   category                                           comments\n",
              "10      pos                          ‡∏™‡∏ô‡πÉ‡∏à ‡∏ü‡∏≠‡∏à‡∏π‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö\n",
              "16      pos                                         ‡πÑ‡∏õ‡∏î‡∏¥..‡∏£‡∏≠‡πÑ‡∏£\n",
              "19      pos                           ‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô‡∏ö‡∏≤‡∏ö‡∏µ‡∏Å‡πâ‡∏≠‡∏ô‡∏´‡∏£‡∏≠555555\n",
              "22      pos                                     ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ô‡∏∞\n",
              "25      pos  ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ SHEENe ‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏£‡∏∏‡πà‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡∏á ‡∏Ç‡∏≤‡∏¢‡∏î‡∏µ‡πÜ ‡∏¢‡∏≠‡∏î‡πÑ‡∏•‡∏ó‡πå‡πÄ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a8fa597-fc4b-489a-92d4-59a52304cbbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏™‡∏ô‡πÉ‡∏à ‡∏ü‡∏≠‡∏à‡∏π‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡πÑ‡∏õ‡∏î‡∏¥..‡∏£‡∏≠‡πÑ‡∏£</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏¥‡∏ô‡∏ö‡∏≤‡∏ö‡∏µ‡∏Å‡πâ‡∏≠‡∏ô‡∏´‡∏£‡∏≠555555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ô‡∏∞</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏Ç‡∏≠‡πÉ‡∏´‡πâ SHEENe ‡∏°‡∏µ‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏£‡∏∏‡πà‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡∏á ‡∏Ç‡∏≤‡∏¢‡∏î‡∏µ‡πÜ ‡∏¢‡∏≠‡∏î‡πÑ‡∏•‡∏ó‡πå‡πÄ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a8fa597-fc4b-489a-92d4-59a52304cbbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a8fa597-fc4b-489a-92d4-59a52304cbbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a8fa597-fc4b-489a-92d4-59a52304cbbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-301359f1-1084-49a2-bdaa-549ea99ddb98\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-301359f1-1084-49a2-bdaa-549ea99ddb98')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-301359f1-1084-49a2-bdaa-549ea99ddb98 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "pos_df = df[df.category == \"pos\"]\n",
        "pos_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9Q6j0MbXNc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "430b9f94-8506-4002-8fbe-86820b28b2ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments\n",
              "16747      neg  ‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...\n",
              "1415       neg                                  ‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555\n",
              "21426      neg                                 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ\n",
              "16476      neg                     ‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555\n",
              "10493      neg                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fc1d1ba-adcf-4fff-9124-f59f240352b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16747</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21426</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16476</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10493</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fc1d1ba-adcf-4fff-9124-f59f240352b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fc1d1ba-adcf-4fff-9124-f59f240352b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fc1d1ba-adcf-4fff-9124-f59f240352b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-342e95fc-b0d8-4249-afca-fa17dc3be383\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-342e95fc-b0d8-4249-afca-fa17dc3be383')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-342e95fc-b0d8-4249-afca-fa17dc3be383 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "neg_df = df[df.category == \"neg\"].sample(4300)\n",
        "neg_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8Wfnp3kXRpp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "14d83ab6-60b6-43a3-c9fb-71989d69686d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments\n",
              "16747      neg  ‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...\n",
              "1415       neg                                  ‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555\n",
              "21426      neg                                 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ\n",
              "16476      neg                     ‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555\n",
              "10493      neg                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dae578f1-7296-4258-a301-62e78895e226\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16747</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21426</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16476</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10493</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dae578f1-7296-4258-a301-62e78895e226')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dae578f1-7296-4258-a301-62e78895e226 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dae578f1-7296-4258-a301-62e78895e226');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6bad5bd-f918-48c0-a5c6-63888911f0de\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6bad5bd-f918-48c0-a5c6-63888911f0de')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6bad5bd-f918-48c0-a5c6-63888911f0de button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sentiment_df = pd.concat([neg_df, pos_df])\n",
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfnzdGlDXqkX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d3a53a78-b1ab-4a52-d8ff-86c4d4f8fc27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments  \\\n",
              "16747      neg  ‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...   \n",
              "1415       neg                                  ‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555   \n",
              "21426      neg                                 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ   \n",
              "16476      neg                     ‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555   \n",
              "10493      neg                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á   \n",
              "\n",
              "                                          clean_comments  \n",
              "16747  ‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...  \n",
              "1415                                   ‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555  \n",
              "21426                                 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ  \n",
              "16476                     ‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555  \n",
              "10493                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a88e8488-4e37-407e-a0da-0e02ebef5bfa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "      <th>clean_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16747</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555</td>\n",
              "      <td>‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21426</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16476</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555</td>\n",
              "      <td>‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10493</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a88e8488-4e37-407e-a0da-0e02ebef5bfa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a88e8488-4e37-407e-a0da-0e02ebef5bfa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a88e8488-4e37-407e-a0da-0e02ebef5bfa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4702eec9-8493-4008-8696-ddf86f84b22d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4702eec9-8493-4008-8696-ddf86f84b22d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4702eec9-8493-4008-8696-ddf86f84b22d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "sentiment_df['clean_comments'] = sentiment_df['comments'].fillna('').apply(lambda x: x.lower())\n",
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n94WgDNBYGg_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "213949ec-4689-40aa-8a2b-a932e48088bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"#\\'()*,-.;<=>[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "pun = '\"#\\'()*,-.;<=>[\\\\]^_`{|}~'\n",
        "pun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc-j5QkbYVnl"
      },
      "outputs": [],
      "source": [
        "sentiment_df['clean_comments'] = sentiment_df['clean_comments'].str.replace(r'[%s]' % (pun), '', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJoOTwbsYbw7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fc355b90-b087-4441-b70f-892872dd4629"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments  \\\n",
              "3055       pos  ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢‡∏™‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡πÄ‡∏ó‡πÄ‡∏ß‡∏®‡∏ô‡πå ‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô‡∏ï‡∏£‡∏á‡∏Ç...   \n",
              "12518      pos                                ‡∏ó‡∏≥‡πÑ‡∏°‡∏ä‡∏≠‡∏ö‡∏ä‡∏ß‡∏ô‡∏Å‡∏¥‡∏ô‡πÜ‡πÜ‡∏ï‡∏•‡∏≠‡∏î   \n",
              "22147      neg                                              ‡∏≠‡∏µ‡∏Å‡∏•‡∏∞   \n",
              "16469      neg  ‡πÇ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏õ‡∏£ 299 ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏õ 4 ‡∏Ñ‡∏ô ‡πÇ‡∏õ‡∏£ 29...   \n",
              "2918       neg                   ‡∏ö‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà.......   \n",
              "\n",
              "                                          clean_comments  \n",
              "3055   ‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢‡∏™‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡πÄ‡∏ó‡πÄ‡∏ß‡∏®‡∏ô‡πå ‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô‡∏ï‡∏£‡∏á‡∏Ç...  \n",
              "12518                                ‡∏ó‡∏≥‡πÑ‡∏°‡∏ä‡∏≠‡∏ö‡∏ä‡∏ß‡∏ô‡∏Å‡∏¥‡∏ô‡πÜ‡πÜ‡∏ï‡∏•‡∏≠‡∏î  \n",
              "22147                                              ‡∏≠‡∏µ‡∏Å‡∏•‡∏∞  \n",
              "16469  ‡πÇ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏õ‡∏£ 299 ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏õ 4 ‡∏Ñ‡∏ô ‡πÇ‡∏õ‡∏£ 29...  \n",
              "2918                           ‡∏ö‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0e6a18d-d579-4b3b-a60b-055a6ddd1f96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "      <th>clean_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3055</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢‡∏™‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡πÄ‡∏ó‡πÄ‡∏ß‡∏®‡∏ô‡πå ‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô‡∏ï‡∏£‡∏á‡∏Ç...</td>\n",
              "      <td>‡∏°‡∏±‡∏ô‡∏à‡∏∞‡∏ó‡∏≥‡∏£‡πâ‡∏≤‡∏¢‡∏™‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡πÄ‡∏ó‡πÄ‡∏ß‡∏®‡∏ô‡πå ‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô‡∏ï‡∏£‡∏á‡∏Ç...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12518</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡∏ä‡∏≠‡∏ö‡∏ä‡∏ß‡∏ô‡∏Å‡∏¥‡∏ô‡πÜ‡πÜ‡∏ï‡∏•‡∏≠‡∏î</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡∏ä‡∏≠‡∏ö‡∏ä‡∏ß‡∏ô‡∏Å‡∏¥‡∏ô‡πÜ‡πÜ‡∏ï‡∏•‡∏≠‡∏î</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22147</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏≠‡∏µ‡∏Å‡∏•‡∏∞</td>\n",
              "      <td>‡∏≠‡∏µ‡∏Å‡∏•‡∏∞</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16469</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÇ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏õ‡∏£ 299 ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏õ 4 ‡∏Ñ‡∏ô ‡πÇ‡∏õ‡∏£ 29...</td>\n",
              "      <td>‡πÇ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏õ‡∏£ 299 ‡πÅ‡∏ï‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏õ 4 ‡∏Ñ‡∏ô ‡πÇ‡∏õ‡∏£ 29...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2918</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏ö‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà.......</td>\n",
              "      <td>‡∏ö‡πâ‡∏≤‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡∏ß‡πÄ‡∏à‡πâ‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0e6a18d-d579-4b3b-a60b-055a6ddd1f96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d0e6a18d-d579-4b3b-a60b-055a6ddd1f96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d0e6a18d-d579-4b3b-a60b-055a6ddd1f96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5ba8e5ed-3ae3-41c1-b7a6-e15c29bcf106\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ba8e5ed-3ae3-41c1-b7a6-e15c29bcf106')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5ba8e5ed-3ae3-41c1-b7a6-e15c29bcf106 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3soiCaAYeru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02157806-fc5c-4df7-ef61-fcf6ba127347"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62068"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "custom_words_list = set(thai_words())\n",
        "len(custom_words_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqWIxo5Ymi8"
      },
      "outputs": [],
      "source": [
        "text = \"‡πÇ‡∏≠‡πÄ‡∏Ñ‡∏ö‡πà‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤‡∏£‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US-R98SuYxd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04ffff8-70ab-4bf6-e74e-9c7a1884a518"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡πÇ‡∏≠‡πÄ‡∏Ñ', '‡∏ö‡πà', '‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤', '‡∏£‡∏±‡∏Å', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏ö‡πâ‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏î']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "custom_tokenizer = Tokenizer(custom_words_list)\n",
        "custom_tokenizer.word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1x8Qgc-Y714"
      },
      "outputs": [],
      "source": [
        "sentiment_df['clean_comments'] = sentiment_df['clean_comments'].apply(lambda x: custom_tokenizer.word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHtG9gMBZJYo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "817933d4-7e07-4a91-a6f1-a646f6759df1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments  \\\n",
              "18713      neg  ‡πÑ‡∏õ‡πÜ ‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡πÑ‡∏õ‡πÄ‡∏î‡∏≠‡∏∞‡πÄ‡∏ü‡∏™‡∏ä‡∏≠‡∏õ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏•‡πá‡∏ö‡∏ú‡∏°‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏à‡∏∞‡∏ó...   \n",
              "18857      pos     ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥ ‡∏™‡∏¥‡∏á‡∏´‡πå‡∏Ñ‡πà‡∏∞‡∏û‡∏π‡∏î //‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏°‡∏≤‡∏Å   \n",
              "2952       pos  ‡∏ú‡∏°‡∏°‡∏µstarex‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏±‡∏ö ‡πÄ‡∏•‡πá‡∏ácx3‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏™‡∏ô‡∏°‡∏≤‡∏™...   \n",
              "3538       neg  ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏™‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏•‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏û‡πâ‡∏Å‡∏≤‡∏ô‡∏¥‡πÄ‡∏¢‡πà‚Äã ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑...   \n",
              "1728       pos                                      ‡∏™‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô   \n",
              "\n",
              "                                          clean_comments  \n",
              "18713  [‡πÑ‡∏õ, ‡πÜ,  , ‡∏Å‡∏•‡∏±‡∏ö, ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û, ‡πÑ‡∏õ, ‡πÄ‡∏î‡∏≠‡∏∞, ‡πÄ‡∏ü‡∏™, ‡∏ä‡∏≠‡∏õ, ...  \n",
              "18857  [‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ, ‡∏î‡∏∑‡πà‡∏°, ‡∏ô‡πâ‡∏≥,  , ‡∏™‡∏¥‡∏á‡∏´‡πå, ‡∏Ñ‡πà‡∏∞, ‡∏û‡∏π‡∏î,  , ...  \n",
              "2952   [‡∏ú‡∏°, ‡∏°‡∏µ, starex, ‡πÅ‡∏•‡πâ‡∏ß, ‡∏Ñ‡∏±‡∏ö,  , ‡πÄ‡∏•‡πá‡∏á, cx, 3, ‡πÑ‡∏ß...  \n",
              "3538   [‡πÄ‡∏Ç‡πâ‡∏≤, ‡∏Ñ, ‡∏™‡∏≤‡∏°, ‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å, ‡πÄ‡∏•‡∏¢, ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô, ‡∏ô‡∏µ‡πà, ‡∏ó‡∏µ‡πà, ...  \n",
              "1728                               [‡∏™‡∏≤‡∏°‡∏µ, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡πÑ‡∏õ, ‡∏Å‡∏¥‡∏ô]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-741b5616-f194-4f8d-9160-85b774f4923c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "      <th>clean_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18713</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÑ‡∏õ‡πÜ ‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡πÑ‡∏õ‡πÄ‡∏î‡∏≠‡∏∞‡πÄ‡∏ü‡∏™‡∏ä‡∏≠‡∏õ‡πÄ‡∏•‡∏¢ ‡πÄ‡∏•‡πá‡∏ö‡∏ú‡∏°‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å‡∏à‡∏∞‡∏ó...</td>\n",
              "      <td>[‡πÑ‡∏õ, ‡πÜ,  , ‡∏Å‡∏•‡∏±‡∏ö, ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û, ‡πÑ‡∏õ, ‡πÄ‡∏î‡∏≠‡∏∞, ‡πÄ‡∏ü‡∏™, ‡∏ä‡∏≠‡∏õ, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18857</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏î‡∏∑‡πà‡∏°‡∏ô‡πâ‡∏≥ ‡∏™‡∏¥‡∏á‡∏´‡πå‡∏Ñ‡πà‡∏∞‡∏û‡∏π‡∏î //‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡∏ñ‡∏π‡∏Å‡πÉ‡∏à‡∏°‡∏≤‡∏Å</td>\n",
              "      <td>[‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ, ‡∏î‡∏∑‡πà‡∏°, ‡∏ô‡πâ‡∏≥,  , ‡∏™‡∏¥‡∏á‡∏´‡πå, ‡∏Ñ‡πà‡∏∞, ‡∏û‡∏π‡∏î,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏ú‡∏°‡∏°‡∏µstarex‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏±‡∏ö ‡πÄ‡∏•‡πá‡∏ácx3‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏™‡∏ô‡∏°‡∏≤‡∏™...</td>\n",
              "      <td>[‡∏ú‡∏°, ‡∏°‡∏µ, starex, ‡πÅ‡∏•‡πâ‡∏ß, ‡∏Ñ‡∏±‡∏ö,  , ‡πÄ‡∏•‡πá‡∏á, cx, 3, ‡πÑ‡∏ß...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3538</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏™‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏•‡∏¢‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏û‡πâ‡∏Å‡∏≤‡∏ô‡∏¥‡πÄ‡∏¢‡πà‚Äã ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑...</td>\n",
              "      <td>[‡πÄ‡∏Ç‡πâ‡∏≤, ‡∏Ñ, ‡∏™‡∏≤‡∏°, ‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å, ‡πÄ‡∏•‡∏¢, ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô, ‡∏ô‡∏µ‡πà, ‡∏ó‡∏µ‡πà, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1728</th>\n",
              "      <td>pos</td>\n",
              "      <td>‡∏™‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ‡∏Å‡∏¥‡∏ô</td>\n",
              "      <td>[‡∏™‡∏≤‡∏°‡∏µ, ‡∏≠‡∏¢‡∏≤‡∏Å, ‡πÑ‡∏õ, ‡∏Å‡∏¥‡∏ô]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-741b5616-f194-4f8d-9160-85b774f4923c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-741b5616-f194-4f8d-9160-85b774f4923c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-741b5616-f194-4f8d-9160-85b774f4923c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-956a696c-a617-4d2c-8b3b-1c9dcd283b44\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-956a696c-a617-4d2c-8b3b-1c9dcd283b44')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-956a696c-a617-4d2c-8b3b-1c9dcd283b44 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "sentiment_df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW4aB4UtZNge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6e036a-1d20-4edc-f50a-d7b770f2abe5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16747    [‡πÄ‡∏õ‡πá‡∏ô, ‡∏â‡∏≤‡∏Å, ‡∏à‡∏ö, ‡∏ó‡∏µ‡πà, ‡πÄ‡∏´‡∏µ‡πâ‡∏¢,  , ‡πÄ‡∏≠‡∏≤, ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á, ‡∏Ñ‡∏∑...\n",
              "1415                         [‡∏Å‡∏∏, ‡∏Å‡πá, ‡πÑ‡∏°‡πà, ‡∏Å‡∏¥‡∏ô, ‡∏õ‡∏∞, 55555]\n",
              "21426                           [‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á, ‡πÅ‡∏î‡∏Å, ‡πÄ‡∏•‡∏¢, ‡∏ó‡∏µ‡∏ô‡∏µ‡πâ]\n",
              "16476     [‡∏Å‡∏π, ‡πÑ‡∏°‡πà, ‡πÅ‡∏î‡∏Å, ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ,  , ‡πÑ‡∏°‡πà, ‡πÅ‡∏î‡∏Å, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, 5555]\n",
              "10493                 [‡∏ó‡∏≥‡πÑ‡∏°, ‡πÅ‡∏°‡πá‡∏Ñ, ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå, ‡∏°‡∏±‡∏ô, ‡πÅ‡∏û‡∏á, ‡∏à‡∏±‡∏á]\n",
              "11323    [‡πÉ‡∏ä‡πà,  , ‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô, ‡πÑ‡∏õ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÄ‡∏•‡∏ü, ‡∏•‡∏≠‡∏ô,...\n",
              "3690     [‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô, ‡∏≠‡∏¢‡πà‡∏≤‡∏á, ‡∏≠‡∏∑‡πà‡∏ô, ‡πÑ‡∏î‡πâ, ‡πÄ‡∏õ‡∏•‡πà‡∏≤,  , ‡∏ó‡∏µ‡πà,...\n",
              "7240                                         [‡πÅ‡∏£‡∏á, ‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢]\n",
              "2924               [‡∏Ç‡∏≤‡∏¢, ‡∏≠‡∏±‡∏•‡∏ï‡∏¥‡∏™, ‡∏ó‡∏¥‡πâ‡∏á, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏ü‡∏≠‡∏£‡πå‡∏î, ‡πÇ‡∏ü‡∏Å‡∏±‡∏™]\n",
              "19394       [‡∏ã‡∏∑‡πâ‡∏≠, ‡∏¢‡∏≤‡∏Å, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞,  , ‡πÅ‡∏≠, ‡∏ü, ‡πÄ‡∏î‡πâ‡∏á, ‡∏ï‡∏•‡∏≠‡∏î, ‡πÜ]\n",
              "Name: clean_comments, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "tokenized_doc = sentiment_df['clean_comments']\n",
        "tokenized_doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JhEHI07ZbUP"
      },
      "outputs": [],
      "source": [
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdX3NE-TZpx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f89dd79-deed-44b0-be8c-111a041c4282"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16747    [‡πÄ‡∏õ‡πá‡∏ô, ‡∏â‡∏≤‡∏Å, ‡∏à‡∏ö, ‡∏ó‡∏µ‡πà, ‡πÄ‡∏´‡∏µ‡πâ‡∏¢,  , ‡πÄ‡∏≠‡∏≤, ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á, ‡∏Ñ‡∏∑...\n",
              "1415                         [‡∏Å‡∏∏, ‡∏Å‡πá, ‡πÑ‡∏°‡πà, ‡∏Å‡∏¥‡∏ô, ‡∏õ‡∏∞, 55555]\n",
              "21426                           [‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á, ‡πÅ‡∏î‡∏Å, ‡πÄ‡∏•‡∏¢, ‡∏ó‡∏µ‡∏ô‡∏µ‡πâ]\n",
              "16476     [‡∏Å‡∏π, ‡πÑ‡∏°‡πà, ‡πÅ‡∏î‡∏Å, ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ,  , ‡πÑ‡∏°‡πà, ‡πÅ‡∏î‡∏Å, ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠, 5555]\n",
              "10493                 [‡∏ó‡∏≥‡πÑ‡∏°, ‡πÅ‡∏°‡πá‡∏Ñ, ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå, ‡∏°‡∏±‡∏ô, ‡πÅ‡∏û‡∏á, ‡∏à‡∏±‡∏á]\n",
              "11323    [‡πÉ‡∏ä‡πà,  , ‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô, ‡πÑ‡∏õ, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô, ‡πÄ‡∏•‡∏ü, ‡∏•‡∏≠‡∏ô,...\n",
              "3690     [‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô, ‡∏≠‡∏¢‡πà‡∏≤‡∏á, ‡∏≠‡∏∑‡πà‡∏ô, ‡πÑ‡∏î‡πâ, ‡πÄ‡∏õ‡∏•‡πà‡∏≤,  , ‡∏ó‡∏µ‡πà,...\n",
              "7240                                         [‡πÅ‡∏£‡∏á, ‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢]\n",
              "2924               [‡∏Ç‡∏≤‡∏¢, ‡∏≠‡∏±‡∏•‡∏ï‡∏¥‡∏™, ‡∏ó‡∏¥‡πâ‡∏á, ‡∏ã‡∏∑‡πâ‡∏≠, ‡∏ü‡∏≠‡∏£‡πå‡∏î, ‡πÇ‡∏ü‡∏Å‡∏±‡∏™]\n",
              "19394       [‡∏ã‡∏∑‡πâ‡∏≠, ‡∏¢‡∏≤‡∏Å, ‡∏°‡∏≤‡∏Å, ‡∏Ñ‡πà‡∏∞,  , ‡πÅ‡∏≠, ‡∏ü, ‡πÄ‡∏î‡πâ‡∏á, ‡∏ï‡∏•‡∏≠‡∏î, ‡πÜ]\n",
              "Name: clean_comments, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tokenized_doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwPdNyucZtiQ"
      },
      "outputs": [],
      "source": [
        "tokenized_doc = tokenized_doc.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6KdVt0iZ0kZ"
      },
      "outputs": [],
      "source": [
        "# de-tokenization\n",
        "detokenized_doc = []\n",
        "for i in range(len(tokenized_doc)):\n",
        "#     print(tokenized_doc[i])\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "sentiment_df['clean_comments'] = detokenized_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqjV6pC-aFJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a95ba1dd-52cf-46e6-d352-bd103ddc992c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      category                                           comments  \\\n",
              "16747      neg  ‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...   \n",
              "1415       neg                                  ‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555   \n",
              "21426      neg                                 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ   \n",
              "16476      neg                     ‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555   \n",
              "10493      neg                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á   \n",
              "\n",
              "                                          clean_comments  \n",
              "16747  ‡πÄ‡∏õ‡πá‡∏ô ‡∏â‡∏≤‡∏Å ‡∏à‡∏ö ‡∏ó‡∏µ‡πà ‡πÄ‡∏´‡∏µ‡πâ‡∏¢   ‡πÄ‡∏≠‡∏≤ ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏π ‡πÑ‡∏°‡πà ...  \n",
              "1415                              ‡∏Å‡∏∏ ‡∏Å‡πá ‡πÑ‡∏°‡πà ‡∏Å‡∏¥‡∏ô ‡∏õ‡∏∞ 55555  \n",
              "21426                              ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏î‡∏Å ‡πÄ‡∏•‡∏¢ ‡∏ó‡∏µ‡∏ô‡∏µ‡πâ  \n",
              "16476             ‡∏Å‡∏π ‡πÑ‡∏°‡πà ‡πÅ‡∏î‡∏Å ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ   ‡πÑ‡∏°‡πà ‡πÅ‡∏î‡∏Å ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ 5555  \n",
              "10493                      ‡∏ó‡∏≥‡πÑ‡∏° ‡πÅ‡∏°‡πá‡∏Ñ ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå ‡∏°‡∏±‡∏ô ‡πÅ‡∏û‡∏á ‡∏à‡∏±‡∏á  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5a2649a-2ade-4fc7-9448-f6163888c846\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>comments</th>\n",
              "      <th>clean_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16747</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô‡∏â‡∏≤‡∏Å‡∏à‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏µ‡πâ‡∏¢ ‡πÄ‡∏≠‡∏≤‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏π‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏£‡∏∑‡πà...</td>\n",
              "      <td>‡πÄ‡∏õ‡πá‡∏ô ‡∏â‡∏≤‡∏Å ‡∏à‡∏ö ‡∏ó‡∏µ‡πà ‡πÄ‡∏´‡∏µ‡πâ‡∏¢   ‡πÄ‡∏≠‡∏≤ ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏π ‡πÑ‡∏°‡πà ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏∏‡∏Å‡πá‡πÑ‡∏°‡πà‡∏Å‡∏¥‡∏ô‡∏õ‡∏∞55555</td>\n",
              "      <td>‡∏Å‡∏∏ ‡∏Å‡πá ‡πÑ‡∏°‡πà ‡∏Å‡∏¥‡∏ô ‡∏õ‡∏∞ 55555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21426</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏î‡∏Å‡πÄ‡∏•‡∏¢‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "      <td>‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á ‡πÅ‡∏î‡∏Å ‡πÄ‡∏•‡∏¢ ‡∏ó‡∏µ‡∏ô‡∏µ‡πâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16476</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏Å‡∏π‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ ‡πÑ‡∏°‡πà‡πÅ‡∏î‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠5555</td>\n",
              "      <td>‡∏Å‡∏π ‡πÑ‡∏°‡πà ‡πÅ‡∏î‡∏Å ‡∏≠‡∏¢‡∏π‡πà‡∏î‡∏µ   ‡πÑ‡∏°‡πà ‡πÅ‡∏î‡∏Å ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ 5555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10493</th>\n",
              "      <td>neg</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏°‡πá‡∏Ñ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå‡∏°‡∏±‡∏ô‡πÅ‡∏û‡∏á‡∏à‡∏±‡∏á</td>\n",
              "      <td>‡∏ó‡∏≥‡πÑ‡∏° ‡πÅ‡∏°‡πá‡∏Ñ ‡πÇ‡∏î‡∏ô‡∏±‡∏•‡∏î‡πå ‡∏°‡∏±‡∏ô ‡πÅ‡∏û‡∏á ‡∏à‡∏±‡∏á</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5a2649a-2ade-4fc7-9448-f6163888c846')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5a2649a-2ade-4fc7-9448-f6163888c846 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5a2649a-2ade-4fc7-9448-f6163888c846');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2cd5c6d6-0192-4d75-aa5f-1c26d6da19ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2cd5c6d6-0192-4d75-aa5f-1c26d6da19ea')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2cd5c6d6-0192-4d75-aa5f-1c26d6da19ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uus21RmZaIWR"
      },
      "outputs": [],
      "source": [
        "cleaned_words = sentiment_df['clean_comments'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HryGlqx4abxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db3ba97-88e7-4592-ed30-8429c74ccc05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‡πÄ‡∏õ‡πá‡∏ô ‡∏â‡∏≤‡∏Å ‡∏à‡∏ö ‡∏ó‡∏µ‡πà ‡πÄ‡∏´‡∏µ‡πâ‡∏¢   ‡πÄ‡∏≠‡∏≤ ‡∏ï‡∏≤‡∏°‡∏ï‡∏£‡∏á ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏π ‡πÑ‡∏°‡πà ‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏ô‡∏µ‡πâ ‡πÄ‡∏•‡∏¢ ‡∏≠‡∏∞   ‡∏≠‡πà‡∏≤‡∏ô ‡∏°‡∏≤ ‡∏Ñ‡∏£‡∏∂‡πà‡∏á ‡∏ï‡∏≠‡∏ô ‡∏ß‡πà‡∏≤ ‡∏à‡∏∞ ‡πÑ‡∏°‡πà ‡∏≠‡πà‡∏≤‡∏ô ‡∏ï‡πà‡∏≠ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ‡∏´‡∏•‡∏≠‡∏ô   ‡πÅ‡∏ï‡πà ‡∏î‡πâ‡∏ß‡∏¢ ‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏Ç‡∏µ‡πâ ‡πÄ‡∏™‡∏∑‡∏≠‡∏Å ‡∏Ç‡∏≠‡∏á ‡∏Å‡∏π ‡∏Å‡πá ‡πÅ‡∏°‡πà ‡∏á ‡∏ï‡πâ‡∏≠‡∏á ‡∏à‡∏≥‡πÉ‡∏à ‡∏≠‡πà‡∏≤‡∏ô ‡∏ï‡πà‡∏≠ ‡∏≠‡∏∞   ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏π ‡∏Å‡πá ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô ‡∏Ñ‡∏ô ‡∏°‡∏µ‡∏≠‡∏Ñ‡∏ï‡∏¥ ‡∏Å‡∏∞ ‡∏≠‡∏¥ ‡πÄ‡∏û‡∏•‡∏á ‡∏à‡∏±‡∏ó‡∏£‡πå ‡πÄ‡∏≠‡πã‡∏¢ ‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå ‡πÄ‡∏à‡πâ‡∏≤   ‡πÄ‡∏û‡∏¥‡πà‡∏á ‡∏≠‡πà‡∏≤‡∏ô ‡∏à‡∏ö ‡∏°‡∏≤ ‡∏´‡∏°‡∏≤‡∏î‡πÜ   ‡πÄ‡∏Å‡∏£‡πá‡∏á ‡πÑ‡∏õ ‡∏´‡∏°‡∏î ‡πÑ‡∏°‡πà ‡∏Å‡∏•‡πâ‡∏≤ ‡∏Ç‡∏¢‡∏±‡∏ö ‡∏≠‡∏∞   ‡∏Å‡∏π ‡πÑ‡∏°‡πà ‡∏Å‡∏•‡πâ‡∏≤ ‡πÑ‡∏õ ‡∏Ç ‡∏£‡∏µ‡πâ !!!   ‡∏î‡∏≠‡∏Å‡πÑ‡∏°‡πâ ‡∏£‡∏≤‡∏ï‡∏£‡∏µ']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "cleaned_words[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YFwenwpaesN"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(words, filters = ''):\n",
        "    token = KRTokenizer()\n",
        "    token.fit_on_texts(words)\n",
        "    return token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_iFKeana3gQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2aa3a8-80ad-4178-9b1f-e3b52cb5b324"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'‡πÑ‡∏°‡πà': 1,\n",
              " '‡πÑ‡∏õ': 2,\n",
              " '‡πÄ‡∏•‡∏¢': 3,\n",
              " '‡∏ó‡∏µ‡πà': 4,\n",
              " '‡∏°‡∏≤': 5,\n",
              " '‡πÑ‡∏î‡πâ': 6,\n",
              " '‡∏Å‡πá': 7,\n",
              " '‡∏°‡∏µ': 8,\n",
              " '‡∏Å‡∏¥‡∏ô': 9,\n",
              " '‡∏à‡∏∞': 10,\n",
              " '‡πÜ': 11,\n",
              " '‡πÅ‡∏•‡πâ‡∏ß': 12,\n",
              " '‡πÅ‡∏ï‡πà': 13,\n",
              " '‡∏°‡∏≤‡∏Å': 14,\n",
              " '‡∏Ñ‡∏£‡∏±‡∏ö': 15,\n",
              " '‡πÉ‡∏´‡πâ': 16,\n",
              " '‡∏ß‡πà‡∏≤': 17,\n",
              " '‡∏Ñ‡πà‡∏∞': 18,\n",
              " '‡πÄ‡∏õ‡πá‡∏ô': 19,\n",
              " '‡∏Ç‡∏≠‡∏á': 20,\n",
              " '‡∏°‡∏±‡∏ô': 21,\n",
              " '‡∏ô‡∏∞': 22,\n",
              " '‡∏ô‡∏µ‡πâ': 23,\n",
              " '‡∏≠‡∏¢‡∏≤‡∏Å': 24,\n",
              " '‡πÉ‡∏ä‡πâ': 25,\n",
              " '‡∏î‡∏µ': 26,\n",
              " '‡∏Å‡∏±‡∏ö': 27,\n",
              " '‡∏Ñ‡∏∑‡∏≠': 28,\n",
              " '‡πÉ‡∏ô': 29,\n",
              " '‡πÄ‡∏£‡∏≤': 30,\n",
              " '‡πÅ‡∏•‡∏∞': 31,\n",
              " '‡∏ï‡πâ‡∏≠‡∏á': 32,\n",
              " '‡∏£‡∏ñ': 33,\n",
              " '‡∏Ñ‡∏ô': 34,\n",
              " '‡∏ä‡∏≠‡∏ö': 35,\n",
              " '‡∏î‡πâ‡∏ß‡∏¢': 36,\n",
              " '‡∏¢‡∏±‡∏á': 37,\n",
              " '‡∏Å‡∏ß‡πà‡∏≤': 38,\n",
              " '‡∏Å‡∏±‡∏ô': 39,\n",
              " '‡∏ô‡∏µ‡πà': 40,\n",
              " '‡∏ñ‡πâ‡∏≤': 41,\n",
              " '‡∏≠‡∏µ‡∏Å': 42,\n",
              " '‡∏•‡∏∞': 43,\n",
              " '‡∏Å‡∏Å': 44,\n",
              " '‡∏ú‡∏°': 45,\n",
              " '‡πÄ‡∏û‡∏£‡∏≤‡∏∞': 46,\n",
              " '‡πÄ‡∏≠‡∏≤': 47,\n",
              " '‡∏Å‡∏π': 48,\n",
              " '‡πÅ‡∏ö‡∏ö': 49,\n",
              " '‡∏ã‡∏∑‡πâ‡∏≠': 50,\n",
              " '‡∏ó‡∏≥': 51,\n",
              " '‡∏≠‡∏∞‡πÑ‡∏£': 52,\n",
              " '‡∏≠‡∏¢‡∏π‡πà': 53,\n",
              " '‡πÅ‡∏°‡πà': 54,\n",
              " '‡∏á': 55,\n",
              " '‡πÄ‡∏ö‡∏µ‡∏¢‡∏£‡πå': 56,\n",
              " '‡∏ö‡∏≠‡∏Å': 57,\n",
              " '2': 58,\n",
              " '‡∏≠‡∏∞': 59,\n",
              " '‡∏î‡∏π': 60,\n",
              " '‡πÅ‡∏û‡∏á': 61,\n",
              " '‡∏Å‡πâ‡∏≠‡∏ô': 62,\n",
              " '‡πÄ‡∏Ñ‡∏¢': 63,\n",
              " '‡∏û‡∏≠': 64,\n",
              " '‡∏™‡∏ß‡∏¢': 65,\n",
              " '‡∏ä‡πâ‡∏≤‡∏á': 66,\n",
              " '‡∏´‡∏°‡∏î': 67,\n",
              " '‡πÅ‡∏Ñ‡πà': 68,\n",
              " '‡∏≠‡∏£‡πà‡∏≠‡∏¢': 69,\n",
              " '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô': 70,\n",
              " '‡∏û‡∏µ‡πà': 71,\n",
              " '‡∏ï‡∏±‡∏ß': 72,\n",
              " '‡πÄ‡∏´‡πá‡∏ô': 73,\n",
              " '‡∏ß‡∏±‡∏ô': 74,\n",
              " '‡∏´‡∏ô‡πâ‡∏≤': 75,\n",
              " '‡∏≠‡πà‡∏∞': 76,\n",
              " '‡∏≠‡∏¢‡πà‡∏≤‡∏á': 77,\n",
              " '‡∏à‡∏≤‡∏Å': 78,\n",
              " '‡∏•‡∏≠‡∏á': 79,\n",
              " '‡∏£‡∏≤‡∏Ñ‡∏≤': 80,\n",
              " '‡πÅ‡∏î‡∏Å': 81,\n",
              " '‡πÑ‡∏´‡∏ô': 82,\n",
              " '‡∏Å‡∏≤‡∏£': 83,\n",
              " '3': 84,\n",
              " '‡∏ó‡∏≥‡πÑ‡∏°': 85,\n",
              " '‡∏à‡∏±‡∏î': 86,\n",
              " '‡πÉ‡∏ä‡πà': 87,\n",
              " '‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà': 88,\n",
              " '‡πÄ‡∏¢‡∏≠‡∏∞': 89,\n",
              " '‡∏™‡∏≤‡∏Ç‡∏≤': 90,\n",
              " '555': 91,\n",
              " '‡∏ú‡πâ‡∏≤‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢': 92,\n",
              " '‡∏°‡∏∂‡∏á': 93,\n",
              " '‡∏ñ‡∏∂‡∏á': 94,\n",
              " '‡∏Ñ‡∏∞': 95,\n",
              " '‡∏´‡∏£‡∏∑‡∏≠': 96,\n",
              " '‡∏à‡∏£‡∏¥‡∏á‡πÜ': 97,\n",
              " '‡∏Ç‡∏∂‡πâ‡∏ô': 98,\n",
              " '‡∏£‡∏≠': 99,\n",
              " '‡πÄ‡∏Ñ': 100,\n",
              " '‡∏Ç‡∏≤‡∏¢': 101,\n",
              " '‡∏ó‡∏±‡πâ‡∏á': 102,\n",
              " '‡πÉ‡∏Ñ‡∏£': 103,\n",
              " '‡∏≠‡∏µ': 104,\n",
              " '‡∏´‡∏ô‡πà‡∏≠‡∏¢': 105,\n",
              " '‡∏Å‡πà‡∏≠‡∏ô': 106,\n",
              " '‡∏á‡∏á': 107,\n",
              " '‡∏õ‡∏µ': 108,\n",
              " '‡∏ó‡∏∏‡∏Å': 109,\n",
              " '‡πÄ‡∏Ñ‡πâ‡∏≤': 110,\n",
              " '‡∏ï‡∏¥‡∏î': 111,\n",
              " '‡∏ô': 112,\n",
              " '‡∏£‡∏π‡πâ': 113,\n",
              " '‡πÑ‡∏ó‡∏¢': 114,\n",
              " '‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á': 115,\n",
              " '‡∏à‡∏ô': 116,\n",
              " '‡∏Ç‡∏±‡∏ö': 117,\n",
              " '‡πÄ‡∏Ç‡∏≤': 118,\n",
              " '‡∏™': 119,\n",
              " '‡∏Ç‡∏≠': 120,\n",
              " '‡∏™‡∏∏‡∏î': 121,\n",
              " '‡πÉ‡∏´‡∏°‡πà': 122,\n",
              " '‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ': 123,\n",
              " '‡∏≠‡∏≠‡∏Å': 124,\n",
              " '‡∏Ñ‡∏±‡∏ö': 125,\n",
              " '‡∏°‡∏±‡πâ‡∏¢': 126,\n",
              " '‡∏Ñ‡∏¥‡∏î': 127,\n",
              " '‡∏≠‡∏≠': 128,\n",
              " '‡∏Ñ‡∏±‡∏ô': 129,\n",
              " '‡∏ú‡∏¥‡∏ß': 130,\n",
              " '‡∏ö‡πâ‡∏≤‡∏ô': 131,\n",
              " '‡πÄ‡∏≠‡∏á': 132,\n",
              " '‡πÄ‡∏à‡∏≠': 133,\n",
              " '‡πÇ‡∏î‡∏ô': 134,\n",
              " '‡∏Ñ‡πà‡∏≤': 135,\n",
              " '‡∏ö‡∏≤‡∏ö‡∏µ': 136,\n",
              " '‡∏Å‡∏∏': 137,\n",
              " '‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô': 138,\n",
              " '‡∏ô‡∏¥‡∏™‡∏™‡∏±‡∏ô': 139,\n",
              " '‡πÑ‡∏ü‡∏ü‡πâ‡∏≤': 140,\n",
              " '‡∏ô‡∏∞‡∏Ñ‡∏∞': 141,\n",
              " '‡πÑ‡∏´‡∏°': 142,\n",
              " '‡∏Ç‡∏ß‡∏î': 143,\n",
              " '‡πÅ‡∏û‡πâ': 144,\n",
              " '1': 145,\n",
              " '‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ': 146,\n",
              " '‡∏™‡∏µ': 147,\n",
              " '‡∏£‡πâ‡∏≤‡∏ô': 148,\n",
              " '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ': 149,\n",
              " '‡πÄ‡∏Ç‡πâ‡∏≤': 150,\n",
              " '‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤': 151,\n",
              " '‡∏ï‡∏≠‡∏ô': 152,\n",
              " '‡∏ô‡πâ‡∏≥': 153,\n",
              " '‡∏û‡∏≤': 154,\n",
              " '‡∏ô‡πà‡∏≤': 155,\n",
              " '‡πÄ‡∏î‡∏µ‡∏¢‡∏ß': 156,\n",
              " '‡∏´‡∏°‡∏π': 157,\n",
              " '5555': 158,\n",
              " '‡∏ö‡πâ‡∏≤‡∏á': 159,\n",
              " '‡πÄ‡∏≠‡πá‡∏°': 160,\n",
              " '‡∏û‡∏ß‡∏Å': 161,\n",
              " '‡∏ô‡πâ‡∏≠‡∏á': 162,\n",
              " '‡∏à‡∏£‡∏¥‡∏á': 163,\n",
              " '‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤': 164,\n",
              " '‡∏Ñ‡∏∏‡∏ì': 165,\n",
              " '‡πÉ‡∏™‡πà': 166,\n",
              " '‡∏ö‡∏≤‡∏ó': 167,\n",
              " '‡∏Ñ‡∏ß‡∏≤‡∏°': 168,\n",
              " '‡∏ñ‡∏π‡∏Å': 169,\n",
              " '‡∏ô‡∏±‡πà‡∏á': 170,\n",
              " '4': 171,\n",
              " '‡πÑ‡∏á': 172,\n",
              " '‡∏ô‡∏¥': 173,\n",
              " '‡∏•‡πà‡∏∞': 174,\n",
              " '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô': 175,\n",
              " '‡∏ä‡πà‡∏ß‡∏¢': 176,\n",
              " '‡πÄ‡∏´‡∏µ‡πâ‡∏¢': 177,\n",
              " '‡∏Ñ‡∏á': 178,\n",
              " '‡∏™‡∏±‡πà‡∏á': 179,\n",
              " '‡πÄ‡∏•‡∏¥‡∏Å': 180,\n",
              " '‡∏ä‡∏∏‡∏î': 181,\n",
              " '‡πÄ‡∏ô‡∏∑‡πâ‡∏≠': 182,\n",
              " '‡∏á‡πà‡∏≤‡∏¢': 183,\n",
              " '‡∏ó‡∏≥‡πÉ‡∏´‡πâ': 184,\n",
              " '‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì': 185,\n",
              " '‡∏´‡∏•‡∏≤‡∏¢': 186,\n",
              " '‡πÄ‡∏ß‡∏•‡∏≤': 187,\n",
              " '‡∏ô‡∏≤‡∏ô': 188,\n",
              " '‡∏•‡∏î': 189,\n",
              " '‡∏•‡∏µ‡πÇ‡∏≠': 190,\n",
              " '‡∏™‡∏¥': 191,\n",
              " '‡πÄ‡∏û‡∏∑‡πà‡∏≠': 192,\n",
              " '‡∏Å‡πâ‡∏≠': 193,\n",
              " '‡∏´‡∏£‡∏≠': 194,\n",
              " '‡∏™‡∏π‡πâ': 195,\n",
              " '‡∏£‡∏∏‡πà‡∏ô': 196,\n",
              " '‡∏≠‡∏¢‡πà‡∏≤': 197,\n",
              " '‡∏ö‡∏≠': 198,\n",
              " '‡πÑ‡∏ß‡πâ': 199,\n",
              " '‡∏Ç‡∏ô‡∏≤‡∏î': 200,\n",
              " '‡∏à‡πâ‡∏≤': 201,\n",
              " '‡∏Å‡∏∞': 202,\n",
              " '‡∏ñ‡∏≤‡∏°': 203,\n",
              " '5': 204,\n",
              " '‡πÅ‡∏ñ‡∏°': 205,\n",
              " '‡πÅ‡∏™‡∏á‡πÇ‡∏™‡∏°': 206,\n",
              " '‡∏ó‡∏≤': 207,\n",
              " '‡∏ô‡πâ‡∏≠‡∏¢': 208,\n",
              " '‡∏ï‡πà‡∏≠': 209,\n",
              " '‡∏ß': 210,\n",
              " '‡∏™‡∏¥‡∏á‡∏´‡πå': 211,\n",
              " '‡∏ó‡∏≤‡∏ô': 212,\n",
              " '‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô': 213,\n",
              " '‡πÑ‡∏£': 214,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢': 215,\n",
              " '‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å': 216,\n",
              " '‡∏á‡∏≤‡∏ô': 217,\n",
              " '‡πÜ‡πÜ': 218,\n",
              " '‡∏ô‡∏∂‡∏á': 219,\n",
              " '‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ': 220,\n",
              " '‡πÄ‡∏î‡∏∑‡∏≠‡∏ô': 221,\n",
              " '‡∏£‡∏µ': 222,\n",
              " '‡πÄ‡∏°': 223,\n",
              " '‡∏•‡∏á': 224,\n",
              " 'honda': 225,\n",
              " '‡πÄ‡∏õ‡∏¥‡∏î': 226,\n",
              " '‡∏ï‡∏≤‡∏°': 227,\n",
              " '‡∏ó‡∏µ': 228,\n",
              " '‡∏£‡∏±‡∏Å': 229,\n",
              " '‡∏™‡∏ö‡∏≤‡∏¢': 230,\n",
              " '‡∏ö‡∏≤‡∏á': 231,\n",
              " '‡∏ü‡∏£‡∏µ': 232,\n",
              " '‡∏¢': 233,\n",
              " '‡∏Æ‡∏≠‡∏ô‡∏î‡πâ‡∏≤': 234,\n",
              " '‡∏°‡∏≤‡∏™‡∏î‡πâ‡∏≤': 235,\n",
              " '‡πÄ‡∏•‡πà‡∏ô': 236,\n",
              " '‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£': 237,\n",
              " '‡πÄ‡∏á‡∏¥‡∏ô': 238,\n",
              " '‡∏ô‡∏±‡πâ‡∏ô': 239,\n",
              " '‡πÅ‡∏£‡∏á': 240,\n",
              " '‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà': 241,\n",
              " '‡πÄ‡∏•‡∏∑‡∏≠‡∏Å': 242,\n",
              " '‡∏≠‡∏∑‡πà‡∏ô': 243,\n",
              " '‡∏´‡∏≤': 244,\n",
              " '‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á': 245,\n",
              " '‡∏Å‡∏•‡∏±‡∏ö': 246,\n",
              " '‡πÄ‡∏¢‡πá‡∏ô': 247,\n",
              " '‡∏£‡∏±‡∏ö': 248,\n",
              " '‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥': 249,\n",
              " '‡πÄ‡∏°‡∏≤': 250,\n",
              " '‡πÇ‡∏õ‡∏£': 251,\n",
              " '‡∏Å‡∏•‡∏¥‡πà‡∏ô': 252,\n",
              " '‡∏î‡∏∑‡πà‡∏°': 253,\n",
              " '‡∏û‡πà‡∏≠': 254,\n",
              " '‡∏¢‡∏±‡∏á‡πÑ‡∏á': 255,\n",
              " '‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢': 256,\n",
              " '‡∏´‡∏≤‡∏¢': 257,\n",
              " '‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î': 258,\n",
              " '‡πÄ‡∏î‡πá‡∏Å': 259,\n",
              " '‡∏£‡πà‡∏≤': 260,\n",
              " '‡∏ó‡∏≤‡∏á': 261,\n",
              " '55555': 262,\n",
              " '‡∏ï‡∏•‡∏≠‡∏î': 263,\n",
              " '‡∏Ñ‡∏ß‡∏£': 264,\n",
              " '‡∏ï‡∏£‡∏á': 265,\n",
              " '‡∏õ‡∏Å‡∏ï‡∏¥': 266,\n",
              " '‡∏ô‡πà‡∏≤‡∏à‡∏∞': 267,\n",
              " '‡∏õ‡πà‡∏∞': 268,\n",
              " '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á': 269,\n",
              " '‡∏î‡∏µ‡∏°‡∏≤‡∏Å': 270,\n",
              " 'mk': 271,\n",
              " '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®': 272,\n",
              " '‡∏à‡∏±‡∏á': 273,\n",
              " '\\u200b': 274,\n",
              " '‡∏û‡∏£‡πâ‡∏≠‡∏°': 275,\n",
              " '‡∏ô‡∏≤‡∏ß‡∏≤': 276,\n",
              " '‡∏ß‡∏∞': 277,\n",
              " '‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠': 278,\n",
              " '‡πÄ‡∏´‡∏•‡πâ‡∏≤': 279,\n",
              " '‡∏ä‡πà‡∏ß‡∏á': 280,\n",
              " '‡∏ö‡∏±‡∏ï‡∏£': 281,\n",
              " '‡∏™‡∏≠‡∏á': 282,\n",
              " '‡∏û‡∏π‡∏î': 283,\n",
              " '‡∏™‡∏±‡∏™': 284,\n",
              " '‡∏≠': 285,\n",
              " '‡πÄ‡∏ö‡∏∑‡πà‡∏≠': 286,\n",
              " '‡∏™‡πà‡∏á': 287,\n",
              " '‡πÄ‡∏ã': 288,\n",
              " '‡∏•': 289,\n",
              " '‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á': 290,\n",
              " '‡∏à‡∏±‡∏ö': 291,\n",
              " '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô': 292,\n",
              " '‡πÇ‡∏ï‡πÇ‡∏¢‡∏ï‡πâ‡∏≤': 293,\n",
              " '‡∏¢‡∏¥‡πà‡∏á': 294,\n",
              " '‡∏≠‡∏≠‡∏Å‡∏°‡∏≤': 295,\n",
              " '‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤': 296,\n",
              " '‡πÄ‡∏ô‡∏µ‡πà‡∏¢': 297,\n",
              " '‡πÄ': 298,\n",
              " '‡∏®‡∏π‡∏ô‡∏¢‡πå': 299,\n",
              " '‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á': 300,\n",
              " '‡∏î‡∏≥': 301,\n",
              " '‡∏Å‡∏•‡∏±‡∏ß': 302,\n",
              " '‡πÄ‡∏ñ‡∏≠‡∏∞': 303,\n",
              " '‡∏Å‡∏≤': 304,\n",
              " '‡∏ú‡πà‡∏≤‡∏ô': 305,\n",
              " '‡πÄ‡∏Å‡∏¥‡∏î': 306,\n",
              " '‡∏™‡∏¥‡∏ß': 307,\n",
              " '‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ': 308,\n",
              " '‡πÄ‡∏Å‡∏¥‡∏ô': 309,\n",
              " '‡πÄ‡∏û‡∏¥‡πà‡∏°': 310,\n",
              " '‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö': 311,\n",
              " '10': 312,\n",
              " '‡πÉ‡∏´‡∏ç‡πà': 313,\n",
              " '‡∏¢‡∏≤‡∏Å': 314,\n",
              " '‡∏≠‡∏¥‡∏ô': 315,\n",
              " '‡∏™‡∏±‡∏Å': 316,\n",
              " '‡∏¢‡∏¢': 317,\n",
              " '‡∏≠‡∏¥': 318,\n",
              " '‡∏´‡∏¥‡∏ß': 319,\n",
              " '‡∏™‡πà‡∏ß‡∏ô': 320,\n",
              " '‡πÑ‡∏≠‡πâ': 321,\n",
              " '‡πÅ‡∏™‡∏ô': 322,\n",
              " '‡∏£‡∏≠‡∏ö': 323,\n",
              " '‡πÇ‡∏ó‡∏£': 324,\n",
              " '‡πÄ‡∏ó': 325,\n",
              " '‡∏Ñ‡∏£‡∏±‡πâ‡∏á': 326,\n",
              " '‡πÄ‡∏î‡∏¥‡∏ô': 327,\n",
              " '‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à': 328,\n",
              " '‡∏´‡∏•‡∏±‡∏á': 329,\n",
              " '‡πÄ‡∏î‡∏¥‡∏°': 330,\n",
              " '‡πÄ‡∏û‡∏¥‡πà‡∏á': 331,\n",
              " '‡∏£‡∏µ‡∏ß‡∏¥‡∏ß': 332,\n",
              " '‡∏Ñ‡∏∏‡πâ‡∏°': 333,\n",
              " '‡∏ó‡∏ô': 334,\n",
              " '‡∏ô‡πà‡∏∞': 335,\n",
              " '‡πÄ‡∏£‡∏µ‡∏¢‡∏Å': 336,\n",
              " '‡∏à‡πà‡∏≤‡∏¢': 337,\n",
              " '‡∏ï‡∏±‡πâ‡∏á': 338,\n",
              " '‡πÅ‡∏õ‡πâ‡∏á': 339,\n",
              " '‡∏ô‡πà‡∏≤‡∏£‡∏±‡∏Å': 340,\n",
              " '‡πÅ‡∏à‡πâ‡∏á': 341,\n",
              " '‡πÄ‡∏£': 342,\n",
              " '‡∏™‡∏ô‡πÉ‡∏à': 343,\n",
              " '‡∏Å': 344,\n",
              " '‡πÉ‡∏à': 345,\n",
              " '‡πÄ‡∏ï‡πá‡∏°': 346,\n",
              " '‡πÅ‡∏£‡∏Å': 347,\n",
              " '555555': 348,\n",
              " '‡∏õ‡∏±‡∏ç‡∏´‡∏≤': 349,\n",
              " '‡∏ö‡∏¥': 350,\n",
              " 'mazda': 351,\n",
              " '‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤': 352,\n",
              " '‡∏Ñ': 353,\n",
              " '‡∏ô‡∏∏‡πà‡∏°': 354,\n",
              " '‡∏Å‡πá‡πÑ‡∏î‡πâ': 355,\n",
              " 'üòÇ': 356,\n",
              " 'nissan': 357,\n",
              " '‡∏£‡∏≤': 358,\n",
              " '‡πÄ‡∏´‡∏£‡∏≠': 359,\n",
              " '‡∏à‡∏ö': 360,\n",
              " '‡∏Å‡∏≤‡∏ß': 361,\n",
              " '‡∏≠‡∏≤‡∏´‡∏≤‡∏£': 362,\n",
              " '‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤': 363,\n",
              " '‡∏≠‡πà‡∏≤‡∏ô': 364,\n",
              " '‡∏Å‡∏µ‡πà': 365,\n",
              " '‡∏ö‡∏≤': 366,\n",
              " '‡πÄ‡∏°‡∏∑‡πà‡∏≠': 367,\n",
              " '‡∏ó‡∏±‡∏ô': 368,\n",
              " '‡∏ï‡∏±‡∏á': 369,\n",
              " '‡∏Å‡πâ': 370,\n",
              " '‡πÅ‡∏¢‡πà': 371,\n",
              " '‡∏î': 372,\n",
              " '‡πÄ‡∏ï‡∏¥‡∏°': 373,\n",
              " '‡∏°‡∏≠‡∏á': 374,\n",
              " '‡πÇ‡∏Ñ‡∏ï‡∏£': 375,\n",
              " '‡∏°‡∏∞': 376,\n",
              " '‡∏õ‡∏∞': 377,\n",
              " '‡∏ï': 378,\n",
              " '‡∏ô‡∏≠‡∏ô': 379,\n",
              " '285': 380,\n",
              " '‡∏Ñ‡∏£‡∏µ‡∏°': 381,\n",
              " '‡∏ï‡∏≠‡∏ö': 382,\n",
              " '‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î': 383,\n",
              " '‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ': 384,\n",
              " '‡∏¢‡πà‡∏≤‡∏á': 385,\n",
              " '‡∏û‡∏ô': 386,\n",
              " '‡∏ï‡∏≤‡∏¢': 387,\n",
              " '‡πÅ‡∏Å': 388,\n",
              " '‡∏´‡∏±‡∏ß': 389,\n",
              " '‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥': 390,\n",
              " '‡πÄ‡∏õ‡πá‡∏î': 391,\n",
              " '‡πÑ‡∏≠': 392,\n",
              " '‡∏ô‡πâ‡∏≥‡∏°‡∏±‡∏ô': 393,\n",
              " '‡∏´‡∏ô‡∏π': 394,\n",
              " '‡∏ô‡∏≤‡∏á': 395,\n",
              " '‡∏™‡∏£‡∏∏‡∏õ': 396,\n",
              " '‡∏™‡∏π‡∏ï‡∏£': 397,\n",
              " '‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤': 398,\n",
              " '‡∏≤': 399,\n",
              " '‡∏ö‡∏µ': 400,\n",
              " '‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá': 401,\n",
              " '‡∏´‡∏≠‡∏°': 402,\n",
              " '‡∏Æ‡∏∑‡∏≠': 403,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏≤‡∏¢': 404,\n",
              " '‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô': 405,\n",
              " '‡∏•‡πâ‡∏≤‡∏ô': 406,\n",
              " '‡∏•‡∏π‡∏Å': 407,\n",
              " '‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß': 408,\n",
              " '‡∏£‡πâ‡∏≤': 409,\n",
              " '‡∏•‡∏∑‡∏°': 410,\n",
              " '‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á': 411,\n",
              " '‡πÜ‡πÜ‡πÜ': 412,\n",
              " '‡∏ú‡∏¥‡∏î': 413,\n",
              " '‡∏ü‡∏≠‡∏£‡πå‡∏î': 414,\n",
              " '‚Ä¶': 415,\n",
              " '‡∏™‡∏≤‡∏°': 416,\n",
              " '‡∏õ‡πà‡∏≤‡∏ß': 417,\n",
              " '‡πÄ‡∏£‡πá‡∏ß': 418,\n",
              " '‡∏Å‡∏±‡∏ô‡πÅ‡∏î‡∏î': 419,\n",
              " '‡∏ó': 420,\n",
              " '‡∏´‡∏£‡∏≠‡∏Å': 421,\n",
              " 'mg': 422,\n",
              " '‡∏Ñ‡∏£‡∏ö': 423,\n",
              " '‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ': 424,\n",
              " '‡∏ö': 425,\n",
              " '‡∏ï‡∏≤': 426,\n",
              " '‡πÅ‡∏Å‡πâ‡∏ß': 427,\n",
              " '‡∏ô‡∏∂‡∏Å': 428,\n",
              " '‡πÅ‡∏î‡∏á': 429,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢‡∏á': 430,\n",
              " '‡πÄ‡∏ò‡∏≠': 431,\n",
              " '‡∏ã‡∏∞': 432,\n",
              " '‡∏•‡∏¥‡∏õ': 433,\n",
              " '‡∏£‡πà‡∏ß‡∏°': 434,\n",
              " '‡∏Å‡∏•‡πâ‡∏≤': 435,\n",
              " '‡∏°': 436,\n",
              " '‡∏Å‡∏≥‡∏•‡∏±‡∏á': 437,\n",
              " '‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô': 438,\n",
              " '‡πÄ‡∏Å‡πá‡∏ö': 439,\n",
              " '‡∏ô‡πà‡∏≤‡∏Å‡∏¥‡∏ô': 440,\n",
              " '‡∏ö‡∏ô': 441,\n",
              " '55': 442,\n",
              " '‡πÄ‡∏ô‡πá‡∏ï': 443,\n",
              " '‡∏¢‡∏≠‡∏°': 444,\n",
              " '‡πÄ‡∏•‡πá‡∏Å': 445,\n",
              " '‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô': 446,\n",
              " '‡∏£‡∏π‡∏õ': 447,\n",
              " '‡∏†‡∏≤‡∏©‡∏µ': 448,\n",
              " '‡∏´‡∏ß‡∏≤‡∏ô': 449,\n",
              " '‡πÄ‡∏´‡∏•‡∏∑‡∏≠': 450,\n",
              " '‡∏Å‡∏î': 451,\n",
              " '7': 452,\n",
              " '‡∏£‡∏µ‡∏ö': 453,\n",
              " '‡∏†‡∏≤‡∏¢‡πÉ‡∏ô': 454,\n",
              " '‡∏£': 455,\n",
              " '‡∏Å‡∏¥‡∏ô‡∏Å‡∏±‡∏ô': 456,\n",
              " '‡πÉ‡∏™': 457,\n",
              " '‡∏û‡∏¢‡∏≤‡∏ò‡∏¥': 458,\n",
              " '‡πÇ‡∏ã': 459,\n",
              " '‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡πâ‡∏á': 460,\n",
              " '100': 461,\n",
              " '‡∏ï‡∏≥‡∏£‡∏ß‡∏à': 462,\n",
              " '‡∏≠‡∏±‡∏ô': 463,\n",
              " '‡∏ä‡πâ‡∏≤': 464,\n",
              " '‡∏Ñ‡πà‡∏≤‡∏¢': 465,\n",
              " '‡∏´‡∏ô‡∏±‡∏Å': 466,\n",
              " '‡∏î‡∏¥': 467,\n",
              " '‡∏õ‡∏≤‡∏Å': 468,\n",
              " '‡∏´‡πà‡∏ß‡∏¢': 469,\n",
              " '‡∏≠‡∏¥‡πà‡∏°': 470,\n",
              " '‡∏™‡∏π‡∏ö': 471,\n",
              " '‡πÇ‡∏≠‡πÄ‡∏Ñ': 472,\n",
              " '‡∏≠‡∏≤': 473,\n",
              " '‡∏à‡∏∂‡∏á': 474,\n",
              " '‡∏™‡∏õ‡∏≤‡∏¢': 475,\n",
              " '‡∏ï‡πà‡∏≠‡πÑ‡∏õ': 476,\n",
              " '‡∏≠‡∏¥‡∏≠‡∏¥': 477,\n",
              " '‡∏ß‡∏¥‡πà‡∏á': 478,\n",
              " '‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö': 479,\n",
              " '‡πÑ‡∏´‡∏ß': 480,\n",
              " '‡∏•‡∏≤': 481,\n",
              " '‡∏Ñ‡∏≠': 482,\n",
              " '‡πÇ‡∏≠': 483,\n",
              " '‡πÄ‡∏≠‡∏≠': 484,\n",
              " '‡∏õ‡∏¥‡∏î': 485,\n",
              " '‡∏≠‡∏ô': 486,\n",
              " '‡∏á‡∏µ‡πâ': 487,\n",
              " '‡∏ã‡∏∂‡πà‡∏á': 488,\n",
              " '‡∏Å‡∏≠': 489,\n",
              " 'civic': 490,\n",
              " '‡∏õ‡πã‡∏≠‡∏á': 491,\n",
              " '‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á': 492,\n",
              " '‡πÅ‡∏î‡∏î': 493,\n",
              " '‡πÄ‡∏™‡∏∑‡∏≠‡∏Å': 494,\n",
              " '‡πÅ‡∏ó‡∏ô': 495,\n",
              " '‡∏£‡∏∂': 496,\n",
              " '‡∏≠‡πâ‡∏ß‡∏Å': 497,\n",
              " '‡∏Å‡∏≤‡∏Å': 498,\n",
              " '‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß': 499,\n",
              " '50': 500,\n",
              " '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó': 501,\n",
              " '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà': 502,\n",
              " '‡πÄ‡∏°‡∏ô‡∏π': 503,\n",
              " '‡∏ö‡∏∏': 504,\n",
              " '‡πÄ‡∏à‡πâ‡∏≤': 505,\n",
              " '‡πÇ‡∏ï‡πä‡∏∞': 506,\n",
              " '‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß': 507,\n",
              " '‡∏Ç‡πà‡∏≤‡∏ß': 508,\n",
              " '‡πÄ‡∏ó‡πà‡∏≤': 509,\n",
              " '‡∏õ‡∏£‡∏±‡∏ö': 510,\n",
              " '‡∏≤‡∏≤': 511,\n",
              " '‡∏õ': 512,\n",
              " '‡∏Ç‡∏≤‡∏ß': 513,\n",
              " '‡∏î‡πà‡∏≤': 514,\n",
              " '‡∏´‡∏°‡∏≤‡∏¢': 515,\n",
              " '‡∏ö‡πà‡∏≠‡∏¢': 516,\n",
              " '‡∏£‡πâ‡∏≠‡∏ô': 517,\n",
              " '‡∏™‡∏≤‡∏¢': 518,\n",
              " '‡πÉ‡∏Å‡∏•‡πâ': 519,\n",
              " '6': 520,\n",
              " '‡∏ä‡∏¥‡∏ö‡∏´‡∏≤‡∏¢': 521,\n",
              " '‡πÉ‡∏ö': 522,\n",
              " '‡πÑ‡∏ü': 523,\n",
              " '‡∏ß‡∏à‡∏∞': 524,\n",
              " '‡∏™‡∏¥‡πà‡∏á': 525,\n",
              " '‡∏´‡∏ô‡∏∂‡πà‡∏á': 526,\n",
              " '‡∏°‡∏¥': 527,\n",
              " '‡∏°‡∏±‡πâ‡∏á': 528,\n",
              " '‡∏ä‡∏¥‡πâ‡∏ô': 529,\n",
              " '‡∏Ñ‡∏ô‡πÑ‡∏ó‡∏¢': 530,\n",
              " '‡∏ä‡∏µ‡∏™': 531,\n",
              " '‡πÑ‡∏õ‡∏î‡∏π': 532,\n",
              " '‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ': 533,\n",
              " '‡πÇ‡∏ß‡πà': 534,\n",
              " '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô': 535,\n",
              " '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û': 536,\n",
              " '‡πÅ‡∏ó‡πá‡∏Å‡∏ã‡∏µ‡πà': 537,\n",
              " '‡∏™‡∏π‡∏á': 538,\n",
              " '‡∏î‡∏π‡∏î': 539,\n",
              " '‡∏ä‡∏±‡πâ‡∏ô': 540,\n",
              " '‡∏¢‡∏¢‡∏¢': 541,\n",
              " '‡πÄ‡∏¢‡πà': 542,\n",
              " '‡∏ã‡∏∏‡∏õ': 543,\n",
              " 'cx': 544,\n",
              " 'toyota': 545,\n",
              " '‡∏†‡∏≤‡∏û': 546,\n",
              " '‡∏£‡πâ‡∏≠‡∏¢': 547,\n",
              " '‡∏™‡∏î': 548,\n",
              " '‡∏ä‡∏±‡πà‡∏ô': 549,\n",
              " '‡πÅ‡∏ô‡πà‡∏ô': 550,\n",
              " '‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡πå': 551,\n",
              " '‡∏ô‡πâ‡∏≤': 552,\n",
              " '‡∏£‡∏£': 553,\n",
              " '‡∏ã‡∏µ': 554,\n",
              " '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô': 555,\n",
              " '‡∏â‡∏±‡∏ô': 556,\n",
              " '‡∏Ñ‡∏∑‡∏ô': 557,\n",
              " '‡∏õ‡πâ‡∏≤‡∏¢': 558,\n",
              " '‡∏•‡∏≠‡∏£‡∏µ‡πÄ‡∏≠‡∏∞': 559,\n",
              " '‡πÄ‡∏≠‡∏°': 560,\n",
              " '‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß': 561,\n",
              " '‡πÅ‡∏´‡πâ‡∏á': 562,\n",
              " '‡∏™‡∏ô‡∏∏‡∏Å': 563,\n",
              " '‡∏ó‡∏µ‡πà‡∏à‡∏∞': 564,\n",
              " '‡∏Ñ‡∏≥': 565,\n",
              " '‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå': 566,\n",
              " '‡∏ñ‡∏ô‡∏ô': 567,\n",
              " '‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢': 568,\n",
              " '‡∏ß‡πà‡∏≤‡∏á': 569,\n",
              " '‡∏ä‡∏∑‡πà‡∏≠': 570,\n",
              " '‡∏Ñ‡∏¥‡∏ß': 571,\n",
              " '‡∏¢‡∏≤‡∏ß': 572,\n",
              " '‡∏û‡∏±‡∏á': 573,\n",
              " '‡∏ô‡πâ‡∏≥‡∏à‡∏¥‡πâ‡∏°': 574,\n",
              " '‡∏õ‡πâ‡∏≤': 575,\n",
              " '‡∏ï‡∏•‡∏≤‡∏î': 576,\n",
              " '‡∏£‡∏™': 577,\n",
              " '‡∏ß‡πà‡∏∞': 578,\n",
              " '‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î': 579,\n",
              " '‡∏ß‡∏µ‡∏≠‡∏≠‡∏™': 580,\n",
              " '‡∏≠‡∏µ‡∏Å‡πÅ‡∏•‡πâ‡∏ß': 581,\n",
              " '‡πÇ‡∏î‡∏¢': 582,\n",
              " '‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢': 583,\n",
              " '‡∏´‡∏•‡∏∏‡∏î': 584,\n",
              " '‡∏ä‡∏ß‡∏ô': 585,\n",
              " '‡∏Ç‡∏≤‡∏î': 586,\n",
              " '‡πÇ‡∏≠‡πâ‡∏¢': 587,\n",
              " '‡πÄ‡∏™‡∏£‡πá‡∏à': 588,\n",
              " '‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à': 589,\n",
              " '‡∏£‡∏µ‡πà': 590,\n",
              " '‡∏ô‡∏±‡πà‡∏ô': 591,\n",
              " '‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô': 592,\n",
              " '‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å': 593,\n",
              " '‡∏¢‡∏Å': 594,\n",
              " '‚Äù': 595,\n",
              " '‡∏ü': 596,\n",
              " '‡∏£‡∏¥': 597,\n",
              " '‡πÄ‡∏â‡∏¢': 598,\n",
              " '‡πÄ‡∏´': 599,\n",
              " '‡∏ã‡∏µ‡∏ß‡∏¥‡∏Ñ': 600,\n",
              " '‡∏≠‡∏ü': 601,\n",
              " '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà': 602,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à': 603,\n",
              " '‡∏•‡πà‡∏≤': 604,\n",
              " '‡∏Å‡∏£‡∏∞‡∏ö‡∏∞': 605,\n",
              " '‡∏ü‡∏±‡∏á': 606,\n",
              " '‡∏ó‡∏≠‡∏á': 607,\n",
              " '‡∏ü‡πâ‡∏≤': 608,\n",
              " '‡∏ô‡∏ô': 609,\n",
              " '‡πÅ‡∏ï‡πà‡∏á': 610,\n",
              " '‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå': 611,\n",
              " '‡∏´‡∏¢‡∏∏‡∏î': 612,\n",
              " '‡∏ß‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô': 613,\n",
              " '‡∏™‡∏á‡∏™‡∏±‡∏¢': 614,\n",
              " '‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô': 615,\n",
              " '‡πÄ‡∏Å‡πà‡∏≤': 616,\n",
              " '‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß': 617,\n",
              " '‡πÅ‡∏Å‡πà': 618,\n",
              " '‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß': 619,\n",
              " '‡∏ä‡∏≤': 620,\n",
              " '‡πÄ‡∏ô‡∏≠‡∏∞': 621,\n",
              " '‡∏ó‡πà‡∏≤‡∏ô': 622,\n",
              " '‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô': 623,\n",
              " '‡∏ö‡∏≤‡∏£‡πå': 624,\n",
              " '‡πÄ‡∏£‡∏¢': 625,\n",
              " '‡∏£‡∏∞‡∏î‡∏±‡∏ö': 626,\n",
              " '‡πÄ‡∏ö‡∏≠‡∏£‡πå': 627,\n",
              " '‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ': 628,\n",
              " '‡πÄ‡∏û‡∏•‡∏á': 629,\n",
              " '‡∏û‡∏£‡∏£‡∏Ñ': 630,\n",
              " '‡πÅ‡∏´‡∏•‡∏∞': 631,\n",
              " '‡∏ü‡∏µ': 632,\n",
              " '‡∏´‡∏ô‡∏±‡∏á': 633,\n",
              " '‡∏û‡∏¥‡πÄ‡∏®‡∏©': 634,\n",
              " '‡πÄ‡∏õ‡∏ô': 635,\n",
              " '‡∏ï‡∏±‡∏î': 636,\n",
              " '‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô': 637,\n",
              " '‡∏•‡πà‡∏≤‡∏á': 638,\n",
              " '‡∏£‡∏∞‡∏ö‡∏ö': 639,\n",
              " '‡∏≤‡∏≤‡∏≤': 640,\n",
              " '‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°': 641,\n",
              " '‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì': 642,\n",
              " '‡∏ä‡∏°': 643,\n",
              " 'u': 644,\n",
              " '‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î': 645,\n",
              " '‡πÅ‡∏≠': 646,\n",
              " '‡∏ß‡∏≤‡∏á': 647,\n",
              " '‡∏´‡πâ‡∏≤‡∏°': 648,\n",
              " '‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô': 649,\n",
              " '‡πÅ‡∏ï‡∏Å': 650,\n",
              " '‡∏à‡∏≠‡∏á': 651,\n",
              " '‡∏î‡∏±‡∏ô': 652,\n",
              " '‡∏≠‡πà‡∏≤': 653,\n",
              " '‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô': 654,\n",
              " '‡πÄ‡πÄ‡∏•‡πâ‡∏ß': 655,\n",
              " '‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô': 656,\n",
              " '‡πÅ‡∏ú‡πà‡∏ô': 657,\n",
              " '‡∏û‡∏≠‡∏î‡∏µ': 658,\n",
              " '‡∏ä‡∏±‡∏î': 659,\n",
              " '‡πÄ‡∏ä‡∏∑‡πà‡∏≠': 660,\n",
              " '‡πÅ‡∏ü‡∏ô': 661,\n",
              " '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤': 662,\n",
              " '‡∏ã‡∏±‡∏Å': 663,\n",
              " '‡∏Ñ‡∏∏‡∏¢': 664,\n",
              " '‡∏™‡∏≤‡∏ß': 665,\n",
              " '‡πÜ‡πÜ‡πÜ‡πÜ': 666,\n",
              " '‡∏ó‡∏¥‡πâ‡∏á': 667,\n",
              " '‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤': 668,\n",
              " '‡∏õ‡∏£‡∏∞‡∏ï‡∏π': 669,\n",
              " '‡∏ä‡πà‡∏≤‡∏á': 670,\n",
              " '‡πÅ‡∏•‡πâ': 671,\n",
              " '‡πÇ‡∏ï': 672,\n",
              " '‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà': 673,\n",
              " '‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö': 674,\n",
              " '‡πÅ‡∏Å‡πâ': 675,\n",
              " '‡∏û‡∏±‡∏ô': 676,\n",
              " '‡πÇ‡∏•‡∏Å': 677,\n",
              " '‡πÄ‡∏£‡∏¥‡πà‡∏°': 678,\n",
              " '‡∏Ç‡πâ‡∏≤‡∏ß': 679,\n",
              " '‡∏ù‡∏≤‡∏Å': 680,\n",
              " '‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö': 681,\n",
              " '‡∏¢‡∏π': 682,\n",
              " '‡∏Å‡∏•‡∏∏‡πà‡∏°': 683,\n",
              " '‡∏Å‡∏∏‡πâ‡∏á': 684,\n",
              " '‡∏ô‡∏≥': 685,\n",
              " '‡πÄ‡∏°‡∏∑‡∏≠‡∏á': 686,\n",
              " '‡∏ã‡πà‡∏≠‡∏°': 687,\n",
              " '‡∏¢‡∏≤‡∏£‡∏¥‡∏™': 688,\n",
              " '‡∏ü‡πâ‡∏≠‡∏á': 689,\n",
              " '‚Äú': 690,\n",
              " '‡πÅ‡∏ñ‡∏ß': 691,\n",
              " '‡∏™‡∏≤‡∏£': 692,\n",
              " '‡∏ô‡πâ‡∏≥‡∏´‡∏≠‡∏°': 693,\n",
              " '‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î': 694,\n",
              " '‡∏á‡∏±‡πâ‡∏ô': 695,\n",
              " '‡∏ó‡∏µ‡πÑ‡∏£': 696,\n",
              " '‡∏ä‡∏≤‡∏ï‡∏¥': 697,\n",
              " '‡∏™‡∏ô': 698,\n",
              " '‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï': 699,\n",
              " '‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢': 700,\n",
              " '‡∏à': 701,\n",
              " '‡πÄ‡∏î‡πâ': 702,\n",
              " '‡∏≠‡∏≤‡∏¢': 703,\n",
              " '‡πÑ‡∏ß': 704,\n",
              " '‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ': 705,\n",
              " '‡πÇ‡∏Ñ': 706,\n",
              " '62': 707,\n",
              " '‡∏ü‡∏π': 708,\n",
              " '‡∏£‡∏ß‡∏°': 709,\n",
              " '‡∏°‡∏≤‡∏à‡∏≤‡∏Å': 710,\n",
              " '‡∏à‡∏≠‡∏î': 711,\n",
              " '‡πÅ‡∏£‡πâ‡∏ß': 712,\n",
              " '‡πÄ‡∏ä‡πâ‡∏≤': 713,\n",
              " '5555555': 714,\n",
              " '‡πÄ‡∏ö‡∏≤': 715,\n",
              " '‡∏ä‡∏¥': 716,\n",
              " '‡∏Ñ‡∏£‡∏∂‡πà‡∏á': 717,\n",
              " '‡∏°‡∏∑‡∏≠': 718,\n",
              " '‡πÅ‡∏ô‡πà‡πÜ': 719,\n",
              " '‡πÄ‡∏î‡∏≠‡∏∞': 720,\n",
              " '‡∏¢‡∏≤': 721,\n",
              " '‡∏ß‡∏ß‡∏ß': 722,\n",
              " '‡πÅ‡∏õ‡∏•‡∏Å': 723,\n",
              " '‡πÄ‡∏î': 724,\n",
              " '‡∏´‡πà‡∏≤': 725,\n",
              " '‡∏´‡πâ‡∏≠‡∏á': 726,\n",
              " '‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏µ‡πâ': 727,\n",
              " '‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û': 728,\n",
              " '‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å': 729,\n",
              " '‡πÄ‡∏î‡πâ‡∏≠': 730,\n",
              " 'chr': 731,\n",
              " '‡πÑ‡∏î‡πâ‡∏°‡∏≤': 732,\n",
              " '‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß': 733,\n",
              " '‡∏ô‡∏µ‡πà‡πÅ‡∏´‡∏•‡∏∞': 734,\n",
              " '‡∏Å‡∏•‡∏±‡∏ö‡∏ö‡πâ‡∏≤‡∏ô': 735,\n",
              " '‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö': 736,\n",
              " '‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å': 737,\n",
              " '‡πÇ‡∏•‡∏ï‡∏±‡∏™': 738,\n",
              " '‡∏ã‡∏¥': 739,\n",
              " '‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö': 740,\n",
              " '‡πÇ‡∏•': 741,\n",
              " '‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å': 742,\n",
              " '‡∏à‡∏≤‡∏ô': 743,\n",
              " '‡∏î‡∏±‡∏á': 744,\n",
              " '‡πÄ‡∏≠': 745,\n",
              " '‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô': 746,\n",
              " '‡∏ó‡∏£‡∏≤‡∏ö': 747,\n",
              " '‡∏Ñ‡∏ß‡∏¢': 748,\n",
              " '‡∏¢‡∏∏': 749,\n",
              " '‡∏û': 750,\n",
              " '‡∏ô‡∏≤‡∏ó‡∏µ': 751,\n",
              " '‡∏Ç‡πâ‡∏≤': 752,\n",
              " '‡∏™‡∏†‡∏≤‡∏û': 753,\n",
              " '‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô': 754,\n",
              " '‡∏Ñ‡∏∏': 755,\n",
              " '‡∏Å‡∏≤‡∏£‡πå': 756,\n",
              " '‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤': 757,\n",
              " '‡∏°‡∏π‡∏ô': 758,\n",
              " '‡∏≠‡∏î': 759,\n",
              " '‡∏¢‡∏±‡∏á‡∏°‡∏µ': 760,\n",
              " '15': 761,\n",
              " '‡∏ú‡∏™‡∏°': 762,\n",
              " '‡∏Ñ‡∏ß‡∏≤‡∏¢': 763,\n",
              " '‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß': 764,\n",
              " '‡πÇ‡∏ß‡πâ‡∏¢': 765,\n",
              " 'netflix': 766,\n",
              " '‡πÄ‡∏ä‡πá‡∏Ñ': 767,\n",
              " '‡∏ú‡∏¥‡∏ß‡∏´‡∏ô‡πâ‡∏≤': 768,\n",
              " '‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå': 769,\n",
              " '‡πÇ‡∏ä‡∏ß‡πå': 770,\n",
              " '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û': 771,\n",
              " '2018': 772,\n",
              " '‡πÄ‡∏≠‡πâ‡∏¢': 773,\n",
              " '‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤': 774,\n",
              " '‡∏ö‡πà': 775,\n",
              " '‡πÄ‡∏û‡∏à': 776,\n",
              " 'spy': 777,\n",
              " '‡∏ß‡∏ß': 778,\n",
              " '‡πÄ‡∏´‡∏≠‡∏∞': 779,\n",
              " '‡∏ß‡∏±‡∏ô‡πÄ‡∏Å‡∏¥‡∏î': 780,\n",
              " '‡∏≠‡∏≤‡∏Å‡∏≤‡∏®': 781,\n",
              " '‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô': 782,\n",
              " '‡∏ú‡∏•‡∏¥‡∏ï': 783,\n",
              " '‡πÄ‡∏ã‡πá‡∏ô‡∏ó‡∏£‡∏±‡∏•': 784,\n",
              " '‡∏Å‡πá‡∏î‡∏µ': 785,\n",
              " '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç': 786,\n",
              " '‡∏™‡∏∞‡∏≠‡∏≤‡∏î': 787,\n",
              " '‡∏Ç‡πâ‡∏≤‡∏á': 788,\n",
              " '‡∏ï‡∏±‡∏á‡∏Ñ‡πå': 789,\n",
              " '‡∏´‡∏°‡∏≠': 790,\n",
              " '‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ': 791,\n",
              " '‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô': 792,\n",
              " '‡∏û‡∏ö': 793,\n",
              " '‡∏•‡∏∏‡∏á': 794,\n",
              " '‡∏°‡∏∑‡πâ‡∏≠': 795,\n",
              " '‡∏´‡∏ô‡∏≤‡∏ß': 796,\n",
              " '‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à': 797,\n",
              " '12': 798,\n",
              " '‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à': 799,\n",
              " '‡∏Ç‡∏°': 800,\n",
              " '‡∏ö‡πâ‡∏≤': 801,\n",
              " '‡∏≠‡∏µ‡∏î‡∏≠‡∏Å': 802,\n",
              " '‡∏î‡∏π‡πÅ‡∏•': 803,\n",
              " '‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà': 804,\n",
              " '‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£': 805,\n",
              " '‡πÄ‡∏´‡∏°‡πá‡∏ô': 806,\n",
              " '‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î': 807,\n",
              " '‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô': 808,\n",
              " '‡∏™‡∏±‡∏î': 809,\n",
              " '‡∏à‡∏∏‡∏î': 810,\n",
              " '‡∏£‡∏±‡∏ê': 811,\n",
              " '‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á': 812,\n",
              " '‡∏û‡∏∂‡πà‡∏á': 813,\n",
              " '‡∏Æ‡∏≤': 814,\n",
              " '8': 815,\n",
              " '‡∏≠‡∏Å': 816,\n",
              " '‡πÅ‡∏≠‡∏£‡πå': 817,\n",
              " '‡∏ô‡∏µ‡πÄ‡∏ß‡∏µ‡∏¢': 818,\n",
              " '‡∏ã‡πà‡∏≤': 819,\n",
              " '‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á': 820,\n",
              " '‡∏ï‡∏∑‡πà‡∏ô': 821,\n",
              " '‡∏õ‡∏•‡πà‡∏≠‡∏¢': 822,\n",
              " '‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß': 823,\n",
              " 'show': 824,\n",
              " '‡πÅ‡∏ö': 825,\n",
              " '‡∏û‡∏±‡∏í‡∏ô‡∏≤': 826,\n",
              " '‡∏¢‡∏≠‡∏î': 827,\n",
              " '20': 828,\n",
              " '‡∏Å‡∏£‡∏≠‡∏ö': 829,\n",
              " '‡∏´‡∏°‡∏∑‡πà‡∏ô': 830,\n",
              " '‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß': 831,\n",
              " '‡∏û‡∏Å': 832,\n",
              " '‡∏¢‡∏∂‡∏î': 833,\n",
              " '‡πÅ‡∏ï‡πà‡∏•‡∏∞': 834,\n",
              " '299': 835,\n",
              " '‡∏ã‡∏≠‡∏á': 836,\n",
              " '‡πÑ‡∏Æ‡πÄ‡∏ô‡πÄ‡∏Å‡πâ‡∏ô': 837,\n",
              " '‡πÅ‡∏ô‡πà': 838,\n",
              " '‡πÇ‡∏°‡∏á': 839,\n",
              " '‡∏≠‡πâ‡∏ß‡∏ô': 840,\n",
              " '‡∏ó‡πà‡∏≤': 841,\n",
              " '‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á': 842,\n",
              " '‡πÅ‡∏™‡∏á': 843,\n",
              " '25': 844,\n",
              " '‡πÇ‡∏Æ': 845,\n",
              " '‡∏è': 846,\n",
              " '‡∏£‡∏≠‡∏¢': 847,\n",
              " '‡∏™‡∏µ‡πÅ‡∏î‡∏á': 848,\n",
              " 'innisfree': 849,\n",
              " '‡∏ö‡∏≥‡∏£‡∏∏‡∏á': 850,\n",
              " '‡πÅ‡∏°‡πâ': 851,\n",
              " '‡∏ï‡∏Å': 852,\n",
              " '‡πÄ‡∏•‡∏∞': 853,\n",
              " '‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•': 854,\n",
              " '‡πÄ‡∏Å‡πà‡∏á': 855,\n",
              " '‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô': 856,\n",
              " '‡∏Ç‡∏≠‡πÉ‡∏´‡πâ': 857,\n",
              " '‡πÅ‡∏°‡∏Ñ': 858,\n",
              " '‡∏ö‡∏¥‡πä‡∏Å‡∏ã‡∏µ': 859,\n",
              " '‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£': 860,\n",
              " '‡∏•‡∏¥': 861,\n",
              " '‡∏´‡∏á‡∏™‡πå‡∏ó‡∏≠‡∏á': 862,\n",
              " '‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå': 863,\n",
              " '‡∏ô‡∏ô‡∏ô': 864,\n",
              " '‡∏à‡∏∏': 865,\n",
              " '‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ': 866,\n",
              " 'x': 867,\n",
              " '‡πÄ‡∏ã‡πÄ‡∏ß‡πà‡∏ô': 868,\n",
              " '‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£': 869,\n",
              " '‡∏à‡πâ‡∏∞': 870,\n",
              " '‡∏™‡∏™‡∏™': 871,\n",
              " '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡πà‡∏≠‡∏ô': 872,\n",
              " '‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô': 873,\n",
              " '‡πÅ‡∏á': 874,\n",
              " '‡∏≠‡∏≠‡πÄ‡∏à‡πâ‡∏≤': 875,\n",
              " '‡∏Å‡∏£‡∏∞‡∏ó‡∏∞': 876,\n",
              " '‡∏™‡∏∞‡∏î‡∏ß‡∏Å': 877,\n",
              " '‡πÅ‡∏ã‡πà‡∏ö': 878,\n",
              " '‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå': 879,\n",
              " '‡πÅ‡∏à‡∏Å': 880,\n",
              " '‡∏õ‡∏¥‡πâ‡∏á': 881,\n",
              " '‡∏´‡∏•‡∏≠‡∏Å': 882,\n",
              " '‡πÄ‡∏ã‡πá‡∏á': 883,\n",
              " '‡πÄ‡∏â‡∏û‡∏≤‡∏∞': 884,\n",
              " '‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°': 885,\n",
              " '‡∏™‡∏µ‡πà': 886,\n",
              " 'smirnoff': 887,\n",
              " '‡πÄ‡∏ö‡∏£‡∏Ñ': 888,\n",
              " '‡∏ï‡πà‡∏≤‡∏á': 889,\n",
              " '‡∏Å‡∏¥‡∏ô‡∏ó‡∏µ‡πà': 890,\n",
              " '‡∏ß‡∏¥': 891,\n",
              " '‡∏à‡∏¥‡∏á‡πÜ': 892,\n",
              " '9': 893,\n",
              " '‡πÄ‡∏≠‡πá‡∏Å‡∏ã‡πå': 894,\n",
              " '‡∏ä‡∏≤‡∏ö‡∏π': 895,\n",
              " '‡πÄ‡∏ã‡∏ü': 896,\n",
              " '‡∏à‡∏∑‡∏î': 897,\n",
              " '‡∏û‡∏•‡∏≤‡∏î': 898,\n",
              " '‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å': 899,\n",
              " '‡∏ú‡∏π‡πâ': 900,\n",
              " '‡πÄ‡∏´‡∏ï‡∏∏': 901,\n",
              " '‡πÄ‡∏™‡∏≤‡∏£‡πå': 902,\n",
              " '‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà': 903,\n",
              " '‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö': 904,\n",
              " '‡πÄ‡∏à‡πâ': 905,\n",
              " '‡∏Ñ‡∏£‡∏≤‡∏ö': 906,\n",
              " '‡∏ß‡πâ‡∏≤‡∏ß': 907,\n",
              " '‡∏Ç‡∏µ‡πâ': 908,\n",
              " '‡πÅ‡∏≠‡∏ô‡∏î‡πå': 909,\n",
              " '30': 910,\n",
              " '‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô': 911,\n",
              " '300': 912,\n",
              " '‡∏£‡∏∏‡πâ': 913,\n",
              " '‡πÅ‡∏™‡∏ö': 914,\n",
              " '‡∏û‡∏±‡∏Å': 915,\n",
              " '‡∏´‡∏°‡∏≤': 916,\n",
              " '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': 917,\n",
              " '‡∏£‡∏≤‡∏¢': 918,\n",
              " '‡πÄ‡∏ö‡∏•': 919,\n",
              " '‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î': 920,\n",
              " '‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà': 921,\n",
              " 'dc': 922,\n",
              " '‡∏´‡∏¢‡∏¥‡∏ö': 923,\n",
              " '‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö': 924,\n",
              " '‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà': 925,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤': 926,\n",
              " '‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô': 927,\n",
              " '‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ': 928,\n",
              " '‡∏ï‡∏∞': 929,\n",
              " '‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà': 930,\n",
              " '‡∏Å‡∏£‡∏∞‡∏õ‡πã‡∏≠‡∏á': 931,\n",
              " '‡πÑ‡∏õ‡∏´‡∏≤': 932,\n",
              " '‡∏•‡πâ‡∏≠': 933,\n",
              " '‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô': 934,\n",
              " '‡∏ó‡πâ‡∏≤': 935,\n",
              " '‡∏¢‡∏∑‡∏ô': 936,\n",
              " '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏¢‡∏ô‡∏ï‡πå': 937,\n",
              " '‡∏ó‡∏µ‡∏°': 938,\n",
              " '‡∏≠‡∏¢‡∏∏‡πà': 939,\n",
              " '‡∏™‡∏∞‡∏™‡∏°': 940,\n",
              " '‡∏ó‡∏±‡∏ô‡∏ó‡∏µ': 941,\n",
              " '‡∏û‡∏•‡∏≤‡∏ã‡πà‡∏≤': 942,\n",
              " '‡∏ü‡∏¥‡∏ô': 943,\n",
              " 'l': 944,\n",
              " '‡∏à‡∏≥': 945,\n",
              " '‡∏¢‡∏∏‡∏Ñ': 946,\n",
              " '‡πÄ‡∏à': 947,\n",
              " '‡∏£‡∏±‡∏Å‡∏©‡∏≤': 948,\n",
              " '‡∏ô‡∏°': 949,\n",
              " '‡∏ó‡∏µ‡πà‡∏°‡∏≤': 950,\n",
              " '61': 951,\n",
              " '‡∏ï‡∏µ': 952,\n",
              " '‡∏™‡∏≠‡∏î': 953,\n",
              " '‡∏ú‡πà‡∏≠‡∏ô': 954,\n",
              " '‡∏õ‡∏≤': 955,\n",
              " '‡∏´‡∏•‡∏≠‡∏î': 956,\n",
              " '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à': 957,\n",
              " '‡∏ó‡∏π': 958,\n",
              " '‡∏î‡∏µ‡πâ': 959,\n",
              " '‡∏à‡∏¥': 960,\n",
              " '‡∏≠‡∏≤‡∏Å‡∏≤‡∏£': 961,\n",
              " '‡∏ã‡∏∂‡∏°': 962,\n",
              " '‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ': 963,\n",
              " '‡∏•‡πâ‡∏≤‡∏á': 964,\n",
              " '‡∏≠‡∏≤‡∏¢‡∏∏': 965,\n",
              " '‡∏™‡∏±‡∏á‡∏Ñ‡∏°': 966,\n",
              " 'üò≠': 967,\n",
              " '‡∏´‡∏£‡∏π': 968,\n",
              " '‡πÄ‡∏Å': 969,\n",
              " '‡∏î‡∏µ‡∏à‡∏£‡∏¥‡∏á': 970,\n",
              " '‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô': 971,\n",
              " '‡∏Ñ‡∏≤': 972,\n",
              " '‡∏´‡πâ‡∏≤‡∏á': 973,\n",
              " '‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß': 974,\n",
              " '‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô': 975,\n",
              " '‡∏Å‡∏£‡∏∞': 976,\n",
              " '‡πÇ‡∏´': 977,\n",
              " '‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà': 978,\n",
              " '‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤': 979,\n",
              " '60': 980,\n",
              " '‡∏õ‡∏•': 981,\n",
              " '‡∏Ñ‡∏π‡πà': 982,\n",
              " '‡∏à‡∏¥‡∏ö': 983,\n",
              " '‡∏ô‡πà‡∏≤‡∏™‡∏ô': 984,\n",
              " '‡∏≠‡∏≤‡∏à': 985,\n",
              " '‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠': 986,\n",
              " '‡πÑ‡∏°': 987,\n",
              " '‡∏™‡∏≠‡∏ö': 988,\n",
              " '‡πÑ‡∏Ç‡πà': 989,\n",
              " '120': 990,\n",
              " '‡∏ú‡∏•': 991,\n",
              " '‡∏ô‡∏≤': 992,\n",
              " '‡∏ô‡∏±‡∏Å': 993,\n",
              " '‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î': 994,\n",
              " '‡πÇ‡∏á‡πà': 995,\n",
              " '‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢': 996,\n",
              " '‡∏Ñ‡πà‡∏≠‡∏¢': 997,\n",
              " '‡∏õ‡∏µ‡πÉ‡∏´‡∏°‡πà': 998,\n",
              " 'hotpot': 999,\n",
              " '‡∏ü‡∏≠‡∏£‡πå‡∏à‡∏π‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "train_word_tokenizer = create_tokenizer(cleaned_words)\n",
        "vocab_size = len(train_word_tokenizer.word_index) + 1\n",
        "\n",
        "train_word_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n29YCYJa9uW"
      },
      "outputs": [],
      "source": [
        "def max_length(words):\n",
        "    return(len(max(words, key = len)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMQTNeXlc9yK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0afea45-5abe-4bb4-e485-b39fa9525c56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "506"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "max_length = max_length(tokenized_doc)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUTi3KcydAgw"
      },
      "outputs": [],
      "source": [
        "def encoding_doc(token, words):\n",
        "    return(token.texts_to_sequences(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4gP7D-qdSfs"
      },
      "outputs": [],
      "source": [
        "encoded_doc = encoding_doc(train_word_tokenizer, cleaned_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrCzmqfddXqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6666d0e6-7620-4370-d9a6-d94cda8e2952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡∏≠‡∏∑‡πà‡∏ô ‡πÑ‡∏î‡πâ ‡πÄ‡∏õ‡∏•‡πà‡∏≤   ‡∏ó‡∏µ‡πà ‡πÑ‡∏°‡πà ‡πÉ‡∏ä‡πà ‡πÅ‡∏™‡∏á ‡πÇ‡∏™‡∏°‡∏° ‡∏°‡∏° 555555\n",
            "[1834, 77, 243, 6, 1544, 4, 1, 87, 843, 4892, 1364, 348]\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_words[6])\n",
        "print(encoded_doc[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4yidPTRdbYW"
      },
      "outputs": [],
      "source": [
        "def padding_doc(encoded_doc, max_length):\n",
        "   return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrFgvVNld0K4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6892ca33-bc1e-4c4a-ed34-bdd9c70a77d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded docs =  (8600, 506)\n"
          ]
        }
      ],
      "source": [
        "padded_doc = padding_doc(encoded_doc, max_length)\n",
        "print(\"Shape of padded docs = \",padded_doc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV2rvSbQeBIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a4e1a6-d8ae-4c93-cefb-eb5546484230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡∏à‡∏∞ ‡∏Å‡∏¥‡∏ô ‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ ‡∏ö‡∏±‡∏ï‡∏£ ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÄ‡∏¢‡∏≠‡∏∞\n",
            "[10, 9, 32, 25, 281, 115, 89]\n",
            "[ 10   9  32  25 281 115  89   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_words[200])\n",
        "print(encoded_doc[200])\n",
        "print(padded_doc[200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada1qnjheEEF"
      },
      "outputs": [],
      "source": [
        "category = sentiment_df['category'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMJZ0MwceXdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bafa67-6fa9-4537-fa3a-74f0d79ae07d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neg', 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "unique_category = list(set(category))\n",
        "unique_category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nr9ixvYea5u"
      },
      "outputs": [],
      "source": [
        "output_tokenizer = create_tokenizer(unique_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih73UvsEerq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2beddcaf-6593-46d4-9c51-39bcc38de8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos', 'pos']\n",
            "[[2], [2]]\n"
          ]
        }
      ],
      "source": [
        "encoded_output = encoding_doc(output_tokenizer, category)\n",
        "print(category[6000:6002])\n",
        "print(encoded_output[6000:6002])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buE14KBGewjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203cfead-3985-432c-e490-50569451c4bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8600, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)\n",
        "encoded_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kcPp6v2fK-x"
      },
      "outputs": [],
      "source": [
        "def one_hot(encode):\n",
        "  oh = OneHotEncoder(sparse = False)\n",
        "  return(oh.fit_transform(encode))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjYd3u1dfU2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac75588f-9503-4295-ca21-a3701bdbdfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[1. 0.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_one_hot = one_hot(encoded_output)\n",
        "print(encoded_output[0])\n",
        "print(output_one_hot[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vij8vDuNff1z"
      },
      "outputs": [],
      "source": [
        "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2, stratify=output_one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEF18Fl_f1ZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace8c75b-fbc7-4c63-fbec-eaf641f57b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_X = (6880, 506) and train_Y = (6880, 2)\n",
            "Shape of val_X = (1720, 506) and val_Y = (1720, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
        "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip7Q6Tfsf5YZ"
      },
      "outputs": [],
      "source": [
        "num_classes = len(unique_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBouckt6gFTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd44a5c-12fc-4690-bbd4-414e65ce9e01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNY2UXTPgH8k"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.optimizers import Adam\n",
        "# adam = Adam(learning_rate=0.0001)\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "adam = Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YOhd1e0gSre"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    inputs1 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding1 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs1)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = tf.keras.layers.Flatten()(pool1)\n",
        "    # channel 2\n",
        "    inputs2 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding2 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs2)\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "    drop2 = tf.keras.layers.Dropout(0.5)(conv2)\n",
        "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
        "    flat2 = tf.keras.layers.Flatten()(pool2)\n",
        "    # channel 3\n",
        "    inputs3 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding3 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = True)(inputs3)\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = tf.keras.layers.Dropout(0.5)(conv3)\n",
        "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
        "    flat3 = tf.keras.layers.Flatten()(pool3)\n",
        "    # merge\n",
        "    merged = tf.keras.layers.concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KCAp5XXhog6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d885224-bfd5-4f50-9c98-9df1c30cd7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 506, 256)     3341056     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 506, 256)     3341056     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 506, 256)     3341056     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 503, 32)      32800       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 501, 32)      49184       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 499, 32)      65568       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 503, 32)      0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 501, 32)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 499, 32)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 251, 32)      0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 250, 32)     0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 249, 32)     0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8032)         0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 8000)         0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 7968)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 24000)        0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           240010      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            22          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,410,752\n",
            "Trainable params: 10,410,752\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = define_model(max_length, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALyBQyHIhudA"
      },
      "outputs": [],
      "source": [
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXn842SBh5jT"
      },
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 3, verbose=1,factor=0.1, min_lr=0.000001)\n",
        "callbacks_list = [checkpoint, learning_rate_reduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHPLrqFLiNvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd5480c-c25f-4205-d161-918b1093d03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4975\n",
            "Epoch 1: val_loss improved from inf to 0.69315, saving model to model.h5\n",
            "215/215 [==============================] - 15s 23ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 2: val_loss did not improve from 0.69315\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4942\n",
            "Epoch 3: val_loss improved from 0.69315 to 0.69315, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6932 - accuracy: 0.4945 - val_loss: 0.6931 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6933 - accuracy: 0.4969\n",
            "Epoch 4: val_loss did not improve from 0.69315\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6933 - accuracy: 0.4972 - val_loss: 0.6934 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6907 - accuracy: 0.5075\n",
            "Epoch 5: val_loss improved from 0.69315 to 0.68914, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6907 - accuracy: 0.5078 - val_loss: 0.6891 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 6/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6869 - accuracy: 0.5402\n",
            "Epoch 6: val_loss improved from 0.68914 to 0.68641, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6868 - accuracy: 0.5404 - val_loss: 0.6864 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 7/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5575\n",
            "Epoch 7: val_loss improved from 0.68641 to 0.68364, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.6831 - accuracy: 0.5573 - val_loss: 0.6836 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 8/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.6244\n",
            "Epoch 8: val_loss improved from 0.68364 to 0.67567, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.6772 - accuracy: 0.6247 - val_loss: 0.6757 - val_accuracy: 0.6657 - lr: 1.0000e-05\n",
            "Epoch 9/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.6640\n",
            "Epoch 9: val_loss improved from 0.67567 to 0.65870, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6630 - accuracy: 0.6640 - val_loss: 0.6587 - val_accuracy: 0.6674 - lr: 1.0000e-05\n",
            "Epoch 10/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6417 - accuracy: 0.6631\n",
            "Epoch 10: val_loss improved from 0.65870 to 0.64413, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6417 - accuracy: 0.6631 - val_loss: 0.6441 - val_accuracy: 0.6762 - lr: 1.0000e-05\n",
            "Epoch 11/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6229 - accuracy: 0.6693\n",
            "Epoch 11: val_loss improved from 0.64413 to 0.62921, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.6227 - accuracy: 0.6696 - val_loss: 0.6292 - val_accuracy: 0.6738 - lr: 1.0000e-05\n",
            "Epoch 12/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6057 - accuracy: 0.6784\n",
            "Epoch 12: val_loss improved from 0.62921 to 0.61700, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.6057 - accuracy: 0.6783 - val_loss: 0.6170 - val_accuracy: 0.6814 - lr: 1.0000e-05\n",
            "Epoch 13/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.6830\n",
            "Epoch 13: val_loss improved from 0.61700 to 0.60328, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.5898 - accuracy: 0.6830 - val_loss: 0.6033 - val_accuracy: 0.6872 - lr: 1.0000e-05\n",
            "Epoch 14/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.6950\n",
            "Epoch 14: val_loss improved from 0.60328 to 0.59218, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.5737 - accuracy: 0.6952 - val_loss: 0.5922 - val_accuracy: 0.6942 - lr: 1.0000e-05\n",
            "Epoch 15/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5581 - accuracy: 0.7097\n",
            "Epoch 15: val_loss improved from 0.59218 to 0.58150, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.5584 - accuracy: 0.7092 - val_loss: 0.5815 - val_accuracy: 0.7012 - lr: 1.0000e-05\n",
            "Epoch 16/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7208\n",
            "Epoch 16: val_loss improved from 0.58150 to 0.57161, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.5441 - accuracy: 0.7219 - val_loss: 0.5716 - val_accuracy: 0.7105 - lr: 1.0000e-05\n",
            "Epoch 17/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7466\n",
            "Epoch 17: val_loss improved from 0.57161 to 0.56051, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.5275 - accuracy: 0.7462 - val_loss: 0.5605 - val_accuracy: 0.7198 - lr: 1.0000e-05\n",
            "Epoch 18/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5128 - accuracy: 0.7557\n",
            "Epoch 18: val_loss improved from 0.56051 to 0.54991, saving model to model.h5\n",
            "215/215 [==============================] - 5s 24ms/step - loss: 0.5133 - accuracy: 0.7551 - val_loss: 0.5499 - val_accuracy: 0.7331 - lr: 1.0000e-05\n",
            "Epoch 19/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4991 - accuracy: 0.7727\n",
            "Epoch 19: val_loss improved from 0.54991 to 0.54269, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.4985 - accuracy: 0.7738 - val_loss: 0.5427 - val_accuracy: 0.7483 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7921\n",
            "Epoch 20: val_loss improved from 0.54269 to 0.53524, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.4852 - accuracy: 0.7926 - val_loss: 0.5352 - val_accuracy: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4713 - accuracy: 0.7979\n",
            "Epoch 21: val_loss improved from 0.53524 to 0.52361, saving model to model.h5\n",
            "215/215 [==============================] - 4s 21ms/step - loss: 0.4708 - accuracy: 0.7983 - val_loss: 0.5236 - val_accuracy: 0.7640 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4572 - accuracy: 0.8078\n",
            "Epoch 22: val_loss improved from 0.52361 to 0.51841, saving model to model.h5\n",
            "215/215 [==============================] - 5s 22ms/step - loss: 0.4568 - accuracy: 0.8086 - val_loss: 0.5184 - val_accuracy: 0.7744 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4416 - accuracy: 0.8191\n",
            "Epoch 23: val_loss improved from 0.51841 to 0.50807, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.4421 - accuracy: 0.8186 - val_loss: 0.5081 - val_accuracy: 0.7814 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.8249\n",
            "Epoch 24: val_loss improved from 0.50807 to 0.50206, saving model to model.h5\n",
            "215/215 [==============================] - 4s 21ms/step - loss: 0.4299 - accuracy: 0.8254 - val_loss: 0.5021 - val_accuracy: 0.7860 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8344\n",
            "Epoch 25: val_loss did not improve from 0.50206\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.4182 - accuracy: 0.8343 - val_loss: 0.5029 - val_accuracy: 0.7866 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8394\n",
            "Epoch 26: val_loss improved from 0.50206 to 0.49185, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.4072 - accuracy: 0.8391 - val_loss: 0.4919 - val_accuracy: 0.7936 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8459\n",
            "Epoch 27: val_loss improved from 0.49185 to 0.48558, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.3976 - accuracy: 0.8458 - val_loss: 0.4856 - val_accuracy: 0.7977 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3862 - accuracy: 0.8481\n",
            "Epoch 28: val_loss improved from 0.48558 to 0.47738, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.3865 - accuracy: 0.8481 - val_loss: 0.4774 - val_accuracy: 0.7988 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3772 - accuracy: 0.8515\n",
            "Epoch 29: val_loss improved from 0.47738 to 0.47266, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.3775 - accuracy: 0.8513 - val_loss: 0.4727 - val_accuracy: 0.8041 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8563\n",
            "Epoch 30: val_loss improved from 0.47266 to 0.47256, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.3664 - accuracy: 0.8558 - val_loss: 0.4726 - val_accuracy: 0.8023 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3574 - accuracy: 0.8653\n",
            "Epoch 31: val_loss did not improve from 0.47256\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.3575 - accuracy: 0.8653 - val_loss: 0.4741 - val_accuracy: 0.8052 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8656\n",
            "Epoch 32: val_loss improved from 0.47256 to 0.46247, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.3488 - accuracy: 0.8656 - val_loss: 0.4625 - val_accuracy: 0.8087 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8681\n",
            "Epoch 33: val_loss improved from 0.46247 to 0.46137, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.3410 - accuracy: 0.8686 - val_loss: 0.4614 - val_accuracy: 0.8081 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8756\n",
            "Epoch 34: val_loss did not improve from 0.46137\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.3305 - accuracy: 0.8756 - val_loss: 0.4617 - val_accuracy: 0.8128 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8788\n",
            "Epoch 35: val_loss improved from 0.46137 to 0.45713, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.3234 - accuracy: 0.8789 - val_loss: 0.4571 - val_accuracy: 0.8128 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.8797\n",
            "Epoch 36: val_loss improved from 0.45713 to 0.44908, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.3161 - accuracy: 0.8797 - val_loss: 0.4491 - val_accuracy: 0.8134 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.8839\n",
            "Epoch 37: val_loss did not improve from 0.44908\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.3096 - accuracy: 0.8839 - val_loss: 0.4514 - val_accuracy: 0.8186 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.8857\n",
            "Epoch 38: val_loss improved from 0.44908 to 0.44213, saving model to model.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.3022 - accuracy: 0.8859 - val_loss: 0.4421 - val_accuracy: 0.8145 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8864\n",
            "Epoch 39: val_loss did not improve from 0.44213\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2945 - accuracy: 0.8865 - val_loss: 0.4442 - val_accuracy: 0.8140 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.8890\n",
            "Epoch 40: val_loss improved from 0.44213 to 0.43998, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2874 - accuracy: 0.8890 - val_loss: 0.4400 - val_accuracy: 0.8169 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8950\n",
            "Epoch 41: val_loss did not improve from 0.43998\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2811 - accuracy: 0.8952 - val_loss: 0.4458 - val_accuracy: 0.8186 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.8950\n",
            "Epoch 42: val_loss did not improve from 0.43998\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2760 - accuracy: 0.8952 - val_loss: 0.4441 - val_accuracy: 0.8192 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.9003\n",
            "Epoch 43: val_loss did not improve from 0.43998\n",
            "\n",
            "Epoch 43: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2688 - accuracy: 0.9003 - val_loss: 0.4431 - val_accuracy: 0.8192 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9023\n",
            "Epoch 44: val_loss improved from 0.43998 to 0.43769, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2632 - accuracy: 0.9023 - val_loss: 0.4377 - val_accuracy: 0.8186 - lr: 1.0000e-06\n",
            "Epoch 45/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2630 - accuracy: 0.9017\n",
            "Epoch 45: val_loss did not improve from 0.43769\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2634 - accuracy: 0.9017 - val_loss: 0.4381 - val_accuracy: 0.8180 - lr: 1.0000e-06\n",
            "Epoch 46/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2611 - accuracy: 0.9030\n",
            "Epoch 46: val_loss improved from 0.43769 to 0.43653, saving model to model.h5\n",
            "215/215 [==============================] - 5s 21ms/step - loss: 0.2611 - accuracy: 0.9033 - val_loss: 0.4365 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 47/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.8991\n",
            "Epoch 47: val_loss did not improve from 0.43653\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2607 - accuracy: 0.8999 - val_loss: 0.4367 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 48/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9025\n",
            "Epoch 48: val_loss improved from 0.43653 to 0.43597, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2601 - accuracy: 0.9025 - val_loss: 0.4360 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 49/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9029\n",
            "Epoch 49: val_loss did not improve from 0.43597\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2601 - accuracy: 0.9031 - val_loss: 0.4366 - val_accuracy: 0.8192 - lr: 1.0000e-06\n",
            "Epoch 50/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9029\n",
            "Epoch 50: val_loss improved from 0.43597 to 0.43588, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2591 - accuracy: 0.9029 - val_loss: 0.4359 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 51/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9055\n",
            "Epoch 51: val_loss improved from 0.43588 to 0.43553, saving model to model.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2581 - accuracy: 0.9055 - val_loss: 0.4355 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 52/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9029\n",
            "Epoch 52: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2584 - accuracy: 0.9029 - val_loss: 0.4368 - val_accuracy: 0.8186 - lr: 1.0000e-06\n",
            "Epoch 53/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9032\n",
            "Epoch 53: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2585 - accuracy: 0.9031 - val_loss: 0.4356 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 54/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2566 - accuracy: 0.9026\n",
            "Epoch 54: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2566 - accuracy: 0.9026 - val_loss: 0.4357 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 55/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9032\n",
            "Epoch 55: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2570 - accuracy: 0.9032 - val_loss: 0.4368 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9049\n",
            "Epoch 56: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2545 - accuracy: 0.9049 - val_loss: 0.4364 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 57/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9023\n",
            "Epoch 57: val_loss did not improve from 0.43553\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2531 - accuracy: 0.9029 - val_loss: 0.4357 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 58/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9049\n",
            "Epoch 58: val_loss improved from 0.43553 to 0.43424, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2545 - accuracy: 0.9049 - val_loss: 0.4342 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 59/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.9043\n",
            "Epoch 59: val_loss did not improve from 0.43424\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2548 - accuracy: 0.9039 - val_loss: 0.4350 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 60/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.9055\n",
            "Epoch 60: val_loss did not improve from 0.43424\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2547 - accuracy: 0.9051 - val_loss: 0.4356 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 61/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9055\n",
            "Epoch 61: val_loss improved from 0.43424 to 0.43345, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2536 - accuracy: 0.9055 - val_loss: 0.4335 - val_accuracy: 0.8221 - lr: 1.0000e-06\n",
            "Epoch 62/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9029\n",
            "Epoch 62: val_loss did not improve from 0.43345\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2535 - accuracy: 0.9029 - val_loss: 0.4341 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9084\n",
            "Epoch 63: val_loss improved from 0.43345 to 0.43236, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2517 - accuracy: 0.9084 - val_loss: 0.4324 - val_accuracy: 0.8221 - lr: 1.0000e-06\n",
            "Epoch 64/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9042\n",
            "Epoch 64: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2512 - accuracy: 0.9042 - val_loss: 0.4350 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 65/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9065\n",
            "Epoch 65: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2499 - accuracy: 0.9071 - val_loss: 0.4347 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 66/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9067\n",
            "Epoch 66: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2487 - accuracy: 0.9067 - val_loss: 0.4333 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 67/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9068\n",
            "Epoch 67: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2498 - accuracy: 0.9062 - val_loss: 0.4358 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 68/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9096\n",
            "Epoch 68: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2490 - accuracy: 0.9096 - val_loss: 0.4337 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 69/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9087\n",
            "Epoch 69: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2479 - accuracy: 0.9087 - val_loss: 0.4340 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 70/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9060\n",
            "Epoch 70: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2469 - accuracy: 0.9064 - val_loss: 0.4335 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 71/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2447 - accuracy: 0.9083\n",
            "Epoch 71: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2446 - accuracy: 0.9080 - val_loss: 0.4351 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 72/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2455 - accuracy: 0.9089\n",
            "Epoch 72: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2455 - accuracy: 0.9089 - val_loss: 0.4348 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 73/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2487 - accuracy: 0.9079\n",
            "Epoch 73: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2478 - accuracy: 0.9081 - val_loss: 0.4340 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 74/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.9087\n",
            "Epoch 74: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2449 - accuracy: 0.9083 - val_loss: 0.4360 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 75/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9104\n",
            "Epoch 75: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.2437 - accuracy: 0.9099 - val_loss: 0.4348 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 76/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9096\n",
            "Epoch 76: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2432 - accuracy: 0.9092 - val_loss: 0.4335 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 77/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.9068\n",
            "Epoch 77: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2453 - accuracy: 0.9068 - val_loss: 0.4345 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 78/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2456 - accuracy: 0.9055\n",
            "Epoch 78: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2452 - accuracy: 0.9060 - val_loss: 0.4345 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 79/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9089\n",
            "Epoch 79: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2422 - accuracy: 0.9093 - val_loss: 0.4340 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 80/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9089\n",
            "Epoch 80: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2415 - accuracy: 0.9092 - val_loss: 0.4337 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 81/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9105\n",
            "Epoch 81: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2421 - accuracy: 0.9102 - val_loss: 0.4327 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 82/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9093\n",
            "Epoch 82: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2414 - accuracy: 0.9092 - val_loss: 0.4339 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 83/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.9113\n",
            "Epoch 83: val_loss did not improve from 0.43236\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2389 - accuracy: 0.9113 - val_loss: 0.4339 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 84/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9108\n",
            "Epoch 84: val_loss improved from 0.43236 to 0.43218, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2403 - accuracy: 0.9108 - val_loss: 0.4322 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 85/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9118\n",
            "Epoch 85: val_loss improved from 0.43218 to 0.43201, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2391 - accuracy: 0.9118 - val_loss: 0.4320 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 86/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9099\n",
            "Epoch 86: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2387 - accuracy: 0.9102 - val_loss: 0.4332 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 87/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2369 - accuracy: 0.9131\n",
            "Epoch 87: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2366 - accuracy: 0.9132 - val_loss: 0.4335 - val_accuracy: 0.8203 - lr: 1.0000e-06\n",
            "Epoch 88/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9100\n",
            "Epoch 88: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2372 - accuracy: 0.9102 - val_loss: 0.4331 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 89/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9108\n",
            "Epoch 89: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2367 - accuracy: 0.9108 - val_loss: 0.4323 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 90/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2350 - accuracy: 0.9139\n",
            "Epoch 90: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2352 - accuracy: 0.9140 - val_loss: 0.4329 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 91/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9116\n",
            "Epoch 91: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2372 - accuracy: 0.9116 - val_loss: 0.4325 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 92/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9131\n",
            "Epoch 92: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2359 - accuracy: 0.9128 - val_loss: 0.4342 - val_accuracy: 0.8192 - lr: 1.0000e-06\n",
            "Epoch 93/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9127\n",
            "Epoch 93: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.2338 - accuracy: 0.9132 - val_loss: 0.4346 - val_accuracy: 0.8180 - lr: 1.0000e-06\n",
            "Epoch 94/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9114\n",
            "Epoch 94: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.2341 - accuracy: 0.9115 - val_loss: 0.4325 - val_accuracy: 0.8215 - lr: 1.0000e-06\n",
            "Epoch 95/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9126\n",
            "Epoch 95: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2339 - accuracy: 0.9126 - val_loss: 0.4333 - val_accuracy: 0.8186 - lr: 1.0000e-06\n",
            "Epoch 96/100\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9133\n",
            "Epoch 96: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2325 - accuracy: 0.9126 - val_loss: 0.4335 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 97/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2321 - accuracy: 0.9136\n",
            "Epoch 97: val_loss did not improve from 0.43201\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2316 - accuracy: 0.9140 - val_loss: 0.4324 - val_accuracy: 0.8209 - lr: 1.0000e-06\n",
            "Epoch 98/100\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9122\n",
            "Epoch 98: val_loss improved from 0.43201 to 0.43040, saving model to model.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.2333 - accuracy: 0.9122 - val_loss: 0.4304 - val_accuracy: 0.8198 - lr: 1.0000e-06\n",
            "Epoch 99/100\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9131\n",
            "Epoch 99: val_loss did not improve from 0.43040\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.2333 - accuracy: 0.9129 - val_loss: 0.4335 - val_accuracy: 0.8186 - lr: 1.0000e-06\n",
            "Epoch 100/100\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9127\n",
            "Epoch 100: val_loss did not improve from 0.43040\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.2312 - accuracy: 0.9125 - val_loss: 0.4322 - val_accuracy: 0.8186 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit([train_X, train_X, train_X], train_Y, epochs = EPOCHS, batch_size = BS, validation_data = ([val_X, val_X, val_X], val_Y), callbacks = [callbacks_list], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br786GfEiYyv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "0b4ff9f3-0eb8-4ccb-e525-54d41bb4f8d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"feab1cca-ef1e-4d47-b9c9-e2372682f41f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"feab1cca-ef1e-4d47-b9c9-e2372682f41f\")) {                    Plotly.newPlot(                        \"feab1cca-ef1e-4d47-b9c9-e2372682f41f\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[0.693211555480957,0.693153440952301,0.6931554675102234,0.6932951211929321,0.6907304525375366,0.6868482232093811,0.6831464767456055,0.6771753430366516,0.6630261540412903,0.6416980624198914,0.6227403283119202,0.6057389378547668,0.5898192524909973,0.5737249255180359,0.5583713054656982,0.5441150069236755,0.527462363243103,0.5132623910903931,0.498541921377182,0.4851534962654114,0.470842570066452,0.456753671169281,0.44212910532951355,0.42994189262390137,0.4182050824165344,0.40719324350357056,0.39762240648269653,0.3865344524383545,0.37745627760887146,0.3663560748100281,0.3574560582637787,0.3487801253795624,0.3409924805164337,0.3304885923862457,0.3234007954597473,0.3160918951034546,0.3095681369304657,0.30216410756111145,0.294461727142334,0.2873505651950836,0.2810837924480438,0.27596670389175415,0.26882484555244446,0.2631649672985077,0.2633877992630005,0.2611159682273865,0.26071420311927795,0.2601011395454407,0.26009607315063477,0.259141743183136,0.25806310772895813,0.258357435464859,0.25851333141326904,0.25660574436187744,0.25701528787612915,0.2544790506362915,0.2531084418296814,0.25452306866645813,0.2548201382160187,0.2547287046909332,0.25364118814468384,0.2534659802913666,0.25170132517814636,0.25115713477134705,0.24986381828784943,0.24870696663856506,0.24975380301475525,0.2489827424287796,0.24792444705963135,0.24693909287452698,0.2446393519639969,0.24551185965538025,0.24779094755649567,0.24485760927200317,0.2436591237783432,0.24323004484176636,0.24533140659332275,0.24516913294792175,0.2422219067811966,0.24149586260318756,0.2420514076948166,0.24141444265842438,0.2389257252216339,0.2402878850698471,0.2391163855791092,0.23874139785766602,0.23663389682769775,0.2371794581413269,0.23668989539146423,0.23515170812606812,0.2371627241373062,0.23586887121200562,0.23384790122509003,0.23410052061080933,0.23387043178081512,0.2324942946434021,0.23162107169628143,0.23328642547130585,0.23332764208316803,0.2312396764755249],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[0.6931471228599548,0.6931472420692444,0.6931470036506653,0.6934158802032471,0.6891412734985352,0.6864132285118103,0.6836356520652771,0.6756667494773865,0.6587021350860596,0.6441290378570557,0.6292147040367126,0.6169988512992859,0.6032776832580566,0.5921846628189087,0.5815021395683289,0.5716084837913513,0.5605064034461975,0.5499117970466614,0.5426926016807556,0.5352388024330139,0.5236068964004517,0.5184100866317749,0.5080662369728088,0.5020573139190674,0.5029273629188538,0.49185389280319214,0.48558473587036133,0.47738486528396606,0.4726635217666626,0.4725557863712311,0.47407785058021545,0.4624745845794678,0.46136847138404846,0.461688756942749,0.4571342170238495,0.44908034801483154,0.45141562819480896,0.4421280026435852,0.4441823661327362,0.4399794340133667,0.4458097219467163,0.44413670897483826,0.44309383630752563,0.4376872181892395,0.43808838725090027,0.4365254342556,0.43671318888664246,0.4359731674194336,0.43658334016799927,0.4358753263950348,0.43552666902542114,0.4367620050907135,0.43564313650131226,0.4357388913631439,0.4368126392364502,0.4363759160041809,0.4357152283191681,0.4342383146286011,0.4350224435329437,0.43563830852508545,0.43345198035240173,0.4340645670890808,0.43236055970191956,0.4350496232509613,0.4346601366996765,0.433338463306427,0.43580421805381775,0.4336965084075928,0.4340204894542694,0.433513879776001,0.43505364656448364,0.43480488657951355,0.4340401887893677,0.43598493933677673,0.43483367562294006,0.43347737193107605,0.43449196219444275,0.43454575538635254,0.43397998809814453,0.43373456597328186,0.43267467617988586,0.433898389339447,0.43389415740966797,0.4321759343147278,0.43201228976249695,0.4331549406051636,0.4334680736064911,0.4330773651599884,0.4323003590106964,0.432868093252182,0.4324581027030945,0.4341842532157898,0.4345710575580597,0.43254777789115906,0.4332754909992218,0.4334726631641388,0.4324027895927429,0.43039828538894653,0.43348565697669983,0.4321821331977844],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('feab1cca-ef1e-4d47-b9c9-e2372682f41f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist.history['loss'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist.history['val_loss'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYj4Q0Kum9lU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "468eb9f6-5a30-4af5-8614-defe9158b91e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"8d49f999-c855-4467-b108-e6890715c3d0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8d49f999-c855-4467-b108-e6890715c3d0\")) {                    Plotly.newPlot(                        \"8d49f999-c855-4467-b108-e6890715c3d0\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.4972383677959442,0.5,0.4944767355918884,0.4972383677959442,0.507848858833313,0.5404070019721985,0.557267427444458,0.6247093081474304,0.6639534831047058,0.6630814075469971,0.6696220636367798,0.6783429980278015,0.6829941868782043,0.6952034831047058,0.7091569900512695,0.7219476699829102,0.7462209463119507,0.7550871968269348,0.7738372087478638,0.7925872206687927,0.7982558012008667,0.8085755705833435,0.8186046481132507,0.8254360556602478,0.8343023061752319,0.8390988111495972,0.8457849025726318,0.8481104373931885,0.8513081669807434,0.8558139801025391,0.8652616143226624,0.8655523061752319,0.8686046600341797,0.8755813837051392,0.8789244294166565,0.8796511888504028,0.8838662505149841,0.885901153087616,0.8864825367927551,0.8889535069465637,0.8952034711837769,0.8952034711837769,0.9002906680107117,0.9023255705833435,0.9017441868782043,0.9033430218696594,0.8998546600341797,0.9024709463119507,0.9030523300170898,0.9029069542884827,0.9055232405662537,0.9029069542884827,0.9030523300170898,0.9026162624359131,0.9031976461410522,0.9049418568611145,0.9029069542884827,0.9049418568611145,0.9039244055747986,0.9050872325897217,0.9055232405662537,0.9029069542884827,0.9084302186965942,0.9042150974273682,0.9071220755577087,0.9066860675811768,0.90625,0.9095930457115173,0.9087209105491638,0.9063953757286072,0.9079942107200623,0.908866286277771,0.9081395268440247,0.9082849025726318,0.9098837375640869,0.9091569781303406,0.9068313837051392,0.9059593081474304,0.9093023538589478,0.9091569781303406,0.9101744294166565,0.9091569781303406,0.9113371968269348,0.9107558131217957,0.9117732644081116,0.9101744294166565,0.9132267236709595,0.9101744294166565,0.9107558131217957,0.9139534831047058,0.9116278886795044,0.9127907156944275,0.9132267236709595,0.911482572555542,0.9126453399658203,0.9126453399658203,0.9139534831047058,0.9122093319892883,0.9129360318183899,0.9125000238418579],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.6656976938247681,0.6674418449401855,0.6761627793312073,0.6738371849060059,0.6813953518867493,0.6872093081474304,0.6941860318183899,0.7011628150939941,0.710465133190155,0.7197674512863159,0.7331395149230957,0.7482557892799377,0.7546511888504028,0.7639535069465637,0.7744185924530029,0.7813953757286072,0.7860465049743652,0.7866278886795044,0.7936046719551086,0.7976744174957275,0.7988371849060059,0.8040697574615479,0.8023256063461304,0.805232584476471,0.8087209463119507,0.8081395626068115,0.8127906918525696,0.8127906918525696,0.8133720755577087,0.8186046481132507,0.8145349025726318,0.8139534592628479,0.8168604373931885,0.8186046481132507,0.8191860318183899,0.8191860318183899,0.8186046481132507,0.8180232644081116,0.820348858833313,0.819767415523529,0.8215116262435913,0.8191860318183899,0.8215116262435913,0.8215116262435913,0.8186046481132507,0.8209302425384521,0.820348858833313,0.819767415523529,0.819767415523529,0.819767415523529,0.8215116262435913,0.819767415523529,0.820348858833313,0.8220930099487305,0.819767415523529,0.8220930099487305,0.820348858833313,0.820348858833313,0.819767415523529,0.8209302425384521,0.819767415523529,0.820348858833313,0.819767415523529,0.820348858833313,0.820348858833313,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.820348858833313,0.8209302425384521,0.8209302425384521,0.8215116262435913,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.8209302425384521,0.820348858833313,0.8209302425384521,0.8215116262435913,0.8215116262435913,0.8215116262435913,0.8191860318183899,0.8180232644081116,0.8215116262435913,0.8186046481132507,0.819767415523529,0.8209302425384521,0.819767415523529,0.8186046481132507,0.8186046481132507],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8d49f999-c855-4467-b108-e6890715c3d0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist.history['accuracy'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist.history['val_accuracy'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGjt4q_EnLrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28181b7e-bef8-4a10-bf7d-f261fc16f89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 506, 256)     3341056     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 506, 256)     3341056     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 506, 256)     3341056     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 503, 32)      32800       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 501, 32)      49184       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 499, 32)      65568       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 503, 32)      0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 501, 32)      0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 499, 32)      0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 251, 32)      0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 250, 32)     0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 249, 32)     0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8032)         0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 8000)         0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 7968)         0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 24000)        0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           240010      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            22          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,410,752\n",
            "Trainable params: 10,410,752\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "predict_model = load_model(filename)\n",
        "predict_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOmn2dwJnWK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4d0cdb-dd62-4cc3-b46f-fccdf15ce49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate loss: 0.43039828538894653\n",
            "Validate accuracy: 0.819767415523529\n"
          ]
        }
      ],
      "source": [
        "score = predict_model.evaluate([val_X, val_X, val_X], val_Y, verbose=0)\n",
        "print('Validate loss:', score[0])\n",
        "print('Validate accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cakBR36WndbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c134cac9-597c-402c-931e-bce8c07264b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54/54 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1720,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "predicted_classes = np.argmax(predict_model.predict([val_X, val_X, val_X]), axis=-1)\n",
        "predicted_classes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoaySXggoXhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c3d8a7-3161-4db3-9b8d-f5823e89fd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0.]\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "y_true = np.argmax(val_Y,axis = 1)\n",
        "print(val_Y[0])\n",
        "print(y_true[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf2Knnqkofnf"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, predicted_classes)\n",
        "np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R3KidSoonn5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "056c54ae-8f2e-428b-e8be-ed77a8c73ab4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABb8AAARpCAYAAADulXplAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZvUlEQVR4nOzde5iVdb028HsQYYaDgoIcBskjWipKqaVZKZSWmqPmIX0tN5q9lmaUZVbudketdCNWitUWdGceUgxQM2xjpZUled7uLM0jo6AmKIfhuNb7hy+znVii46jL+fn5dHnNxfP81jPfde1/trdf7qehWq1WAwAAAAAABelR7wEAAAAAAOCVJvwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4wm8AAAAAAIrTs94DvBQrn3qg3iMAALAOTcPfVe8RAABYh1UrWus9QrdUai65/qAt6j3Ca8LmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF6Rad3wAAAAAAr7nK6npPQBfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqVbqPQFdYPMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKWi87s7s/kNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHGE3wAAAAAAFMcLLwEAAAAAaqhWvfCyO7P5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRS0fndndn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqp6vzuzmx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAALVUVtd7ArrA5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQS7VS7wnoApvfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHG88BIAAAAAoJaKF152Zza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBqqVZ3f3ZnNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKo/MbAAAAAKCWis7v7szmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBLVed3d2bzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOF14CAAAAANRSWV3vCegCm98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEfnNwAAAABALdVKvSegC2x+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAALVUdH53Zza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgFqqOr+7M5vfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH5zcAAAAAQC0Vnd/dmc1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDheeAkAAAAAUEO1urreI9AFNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAWqqVek9AF9j8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqp6Px+vu9///v5wQ9+sM4zN954Y4YMGZIkWbVqVaZMmZJp06altbU1AwYMyLhx4zJhwoQMHDhwrc8uWLAgkyZNyuzZs7Nw4cI0NzfnkEMOyfjx49OzZ+ejbOE3AAAAAAAv6n3ve19Gjhy51vXHHnsskyZNynbbbdcefCfJF7/4xcycOTN77bVXjj322MydOzcXXXRRbrvttlx++eXp06dP+9nFixfnqKOOyoMPPpgjjzwy22yzTebMmZOzzjorDzzwQM4444xOzyv8BgAAAADgRW277bbZdttt17o+adKkJMlhhx3Wfu3mm2/OzJkzM3bs2EyePLn9+nbbbZeTTjopU6ZMyYknnth+/YILLsj999+fU089NePHj0+SHHrooenfv38uvvjiHHzwwdlll106Na/ObwAAAAAAXpbVq1fnqquuSp8+fbL//vu3X58xY0aStAfZa+yzzz5pbm5uv//8801NTTniiCM6XF/z+enTp3d6NuE3AAAAAEAt1UqZ/7yCbrzxxsyfPz8f+MAH0q9fv/brd955Z3r06JGddtpprc+MGTMmjzzySBYuXJgkeeqpp9La2pptt902jY2NHc6OGDEigwcPzl133dXp2dSeAAAAAAC8gYwbN26d92fPnv2Sn/Wzn/0sSXL44Yd3uD5v3rwMHDgwvXr1Wusza3rB582blwEDBmTevHlJkqFDh9b8HUOHDs0jjzzykmdaw+Y3AAAAAACd9sQTT+S3v/1tRo0alR133LHDvWXLltUMvpOkd+/e7Wee/3Nd59va2jo9n81vAAAAAIA3kM5sdq/LVVddldWrV3d40eUajY2NWbFiRc3PLV++vP3M83+u63xTU1On5xN+AwAAAADUUlld7wlet6rVaq688so0NjampaVlrftDhw7NQw89lBUrVqy10T1//vz2M8//uab+5J/NmzevvSqlM9SeAAAAAADQKTfffHMeffTR7LPPPtlggw3Wuj969OhUKpXceeeda927/fbbM3LkyAwYMCBJMmjQoAwfPjz33ntvewXKGq2trXnyySczevToTs8o/AYAAAAAoFOuuOKKJKlZeZKkfRt8ypQpHa5ff/31aW1tXWtb/IADDkhbW1suvfTSDtenTp3a4XmdofYEAAAAAICX7Omnn86vfvWrbLHFFtl5551rntl9992z//7755prrsnxxx+fcePGZe7cubnwwguz1VZbZfz48R3OH3fccZk1a1bOPPPMtLa2ZptttsmcOXMyY8aMtLS0ZNddd+30nMJvAAAAAABeshkzZmTlypUvuPW9xre//e2MGjUqV111Vb72ta9lwIABaWlpyYQJE9K3b98OZ/v165dLLrkkkyZNyi9/+ctcdtllaW5uzsknn5xjjjnmZc3ZUK1Wqy/rk6+hlU89UO8RAABYh6bh76r3CAAArMOqFa31HqFbWnbLFfUe4VXRuOuh9R7hNaHzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDieOElAAAAAEAtlUq9J6ALbH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAAtVR1fndnNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAWio6v7szm98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcbzwEgAAAACgFi+87NZsfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHJ3fAAAAAAA1VKur6z0CXWDzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDi6PwGAAAAAKilUqn3BHSBzW8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiqPzGwAAAACglqrO7+7M5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQS0Xnd3dm8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojhdeAgAAAADUUvXCy+7M5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQS0Xnd3dm8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACoparzuzuz+Q0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcXR+AwAAAADUUtH53Z3Z/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABq0fndrdn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4wm8AAAAAAIrjhZcAAAAAALVUvfCyO7P5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRS0fndndn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqp6vzuzmx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAALVUdH53Zza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgFqqOr+7M5vfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHG88BIAAAAAoJaKF152Zza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgFp0fndrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAWqrVek9AF9j8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqpVOo9AV1g8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojhdeAgAAAADU4oWX3ZrNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKo/MbAAAAAKCWqs7v7szmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBLRed3d2bzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDi6PwGAAAAAKilWq33BHSBzW8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiqPzGwAAAACglkql3hPQBTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOJ44SUAAAAAQC1eeNmt2fwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDg6vwEAAAAAaqnq/O7ObH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVQr1XqPQBfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqVTqPQFdYPMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKWq87s7s/kNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHGE3wAAAAAAFMcLLwEAAAAAaqlU6z0BXWDzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDi6PwGAAAAAKilUqn3BHSBzW8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiqPzGwAAAACgFp3f3ZrNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKo/MbAAAAAKCWarXeE9AFNr8BAAAAACiOzW8AAAAAAF6yxYsX58c//nGuv/76tLa2prGxMW9605ty1FFHpaWlpf1cW1tbzj333PziF7/IE088kU022ST77bdfPvnJT6apqWmt57a2tmbixIn5/e9/n6VLl2bzzTfPUUcdlUMPPfRlzSn8BgAAAADgJZk/f34++tGPZsGCBTnooIOy1VZbpa2tLQ899FAee+yx9nOrV6/Oxz/+8dxyyy1paWnJLrvsknvvvTcXXHBB7rrrrkydOjU9evxvMcm8efNy+OGHZ9GiRTn66KMzYsSIzJ49O6eddlrmz5+fE088sdOzCr8BAAAAAGqpVOo9wevOKaeckiVLlmTGjBkZNmzYC577+c9/nltuuSUf+chHctppp7Vfb25uzne+853MnDkzBx54YPv1iRMn5sknn8z3v//97L333kmSww47LMcff3wmT56clpaWbLrppp2aVec3AAAAAAAv6tZbb80f//jHfOxjH8uwYcOyevXqLFmypObZGTNmJEnGjx/f4fqRRx6ZxsbGTJ8+vf1aW1tbZs2alREjRrQH32uMHz8+q1atytVXX93peYXfAAAAAAC8qN/+9rdJkpEjR+ZTn/pUdtxxx7z1rW/NHnvskfPOOy+rV69OklSr1dx9993ZZJNN0tzc3OEZjY2NefOb35y77767/drf/va3LFu2LDvttNNav3PMmDFpaGjIXXfd1el51Z4AAAAAALyBjBs3bp33Z8+eXfP63//+9yTJl7/85YwYMSLf/OY3kySXXnppzjnnnDz++OP5xje+kYULF6atrS1bb711zecMGTIkt99+exYvXpx+/fpl3rx5SZKhQ4eudbZXr14ZOHBg5s+f/5K/3xrCbwAAAAAAXtSaipOmpqb89Kc/Ta9evZIk++67b/bbb79cccUVGT9+fJqampKk/f4/6927d5Ln6k769euXtra2Fz2/5kxnCL8BAAAAAGqpVOs9wavihTa7X0xjY2OS5IMf/GCHoLpXr1754Ac/mHPPPTd/+tOf8v73vz9JsmLFiprPWb58eZK0h+Rrfq7r/MCBAzs9r85vAAAAAABe1JpaksGDB691b821Z555JgMGDEhTU1N7nck/mz9/fvr165d+/fp1eG6t8ytWrMiCBQsyZMiQTs8r/AYAAAAA4EWteSHl448/vta9NcH1xhtvnIaGhmy//fZ54okn0tra2uHcsmXL8pe//CU77LBD+7VRo0ald+/eueOOO9Z67h133JFqtZrRo0d3el7hNwAAAAAAL2rcuHHZYIMNMmPGjCxevLj9+pIlS/Lzn/8866+/fvbYY48kSUtLS5Jk6tSpHZ5x6aWXZtmyZe33k+dqT/bee+/MnTs3119/fYfzU6ZMSc+ePbP//vt3el6d3wCdNP3aX+W00yeu88zb37ZjLvjet5Mk515wcSZP+ekLnp115YVpHrbuv7rz9TO/n59N/0WS5Dczf5pBG2/UyakBAN5Yjjzy4OzxzrfnrW/dITtsv2169+6dY479TP7zJz+reb5//375t389OQcdtG+GDh2cxx9/ItOmXZOvf3NilixZWvMz/fr1zcmfPT4HHbRvttj8TVmxYkUeePCRXH31rHzjm2e/ml8PgNdKtVLvCV5X+vfvny9/+cv5whe+kEMOOSSHHHJIGhoaMm3atMyfPz+f+cxnMmzYsCTJwQcfnOnTp+cnP/lJFi1alJ133jl//etfc8kll2TXXXfNAQcc0OHZn/3sZ3PzzTfnlFNOyT333JMRI0Zk9uzZ+fWvf51PfvKTGTlyZKfnFX4DdNK2W2+RTxzzf2re+9Wvf5f7H3w473z729a61/KB92Z4jZC7f7++6/x9f7jltvxs+i/S1NSYtrZlL29oAIA3mK9/9ZRsttmmefLJf+Txx5/IZptt+oJn+/Rpyg2zp2XMTtvn+ut/k8t/Nj077bh9Tj75E3n3u9+RPcd+qP3FXGtsuunw/GrWz7LFFm/K7Nk35brrZqdXr97ZasvNctBB+wq/ASjWgQcemIEDB+bHP/5xzj333FQqlYwaNSoTJ07Mfvvt135uvfXWy49+9KOce+65ue6663Lttddm8ODBGT9+fE444YSst956HZ47fPjwXHbZZTn77LNz2WWXZenSpdlss83y9a9/PYcffvjLmlX4DdBJ247aMtuO2nKt6ytXrsyl065Oz/XWywEfeO9a91v2fV92fWvn+qkWLV6Sfz3j7Oy91x55euEz+fPtd7/suQEA3kj+7/Gfz333P5BHHmnNKZ8/Iad/60svePbzn/tkxuy0fb575g/ypS+f0X799G99Mad8/sRM+PRx+c53f9B+fb311svPLv9xhg8fmr33OTy/+e0fOjzvn/9lHgBK8573vCfvec97XvRc3759c8opp+SUU055Sc/ddNNNM3Hiuv+2fWfo/AZ4hcy+8eYsfObZvPudu2bQRgNfkWd+e9L5Wb58Rb588gmvyPMAAN4oZt9wUx55pPXFDyY5ZvwRWbRocb75rUkdrn/zW5OyaNHiHDP+iA7XP/Sh/bLLzjtl4tnnrxV8J8nq1atf9twAwCvH5jfAK2Ta1b9Mknzog++vef/WO+7O3f9zbxoaeuRNmw7PbjuPSZ8+TS/4vN/87o+Zcd1/5btf/UI2Hjjg1RgZAOANb+utt0hz87DMmvXrLF3a1uHe0qVt+cMf5mSfffbKiBHDM3fuY0mSww59rqP0ymnXZMSI4dn3A+MyYMAG+fsDD+eXv7zhBTvCAeiGKtV6T0AXvOzwe/78+bn77rszb968tLW1pampKUOHDs0OO+yQIUPW/eI2gNI8Nm9+/nTrnRmyyaDsUaPvO3nuxZfPt0H/fvnCp/9vWmpUpCx85tn823e+l7Hv3i37vm/PV2NkAACSbL3V5kmS++5/sOb9++5/MPvss1e23mrz9vD7rWOeq7J71x5vz5nf/UoaGxvbzz/xxFM54sjj89sbb36VJwcAXkynw+/77rsv3/rWt/KnP/0pSVKt/u9//WhoaEiSvP3tb8+XvvSljBo16hUaE+D17efX/iqVSiUHfuB9a3U8brPV5vnGlz6TXcaMzuCNN8pTTy/Ib3//p/zgP36S0741MRv065e93vWODp/5xlnnZuXKlfnK5058Lb8GAMAbzgYb9k+SPPvsopr3Fy1a3OFckmyyycZJkrMnfj0Tzz4/5553YZYtW5YPH35gvvudf820Ky/I9qP3zLx5T7zK0wMA69Kp8Pu+++7Lhz/84ecCngMPzJgxYzJkyJD07t07y5cvz/z583P77bfnl7/8ZY444ohceumlAnCgeJVKJdOv/VUaGhpy0P57r3X/ve95Z4c/Nw8bkiMPOSBbbLZpjpvw5Xzvxxd1CL+v+6/fZtYNN+b0f/1cBm280as+PwAAndOjx3Ovz7r2F//V4QWZ5543NSNGDMvnP3dCjhl/RE4/45x6jQgApJPh98SJE7Phhhvmpz/9aYYNG1bzzGGHHZZPfepTOeqoozJp0qScd955r8igAK9XN8+5PY/PfyLv2HmnjBg+9CV/7h07j8mmzcNy398fyuIlS9Kvb9888+yifGvieXn37rvmgPePexWnBgAgSZ595rmN7w026F/zfv/+/TqcS5JnnlmUwYM3ztVXX7/W+auvvj6f/9wJedvbRr8K0wLwWqtWKvUegS7oVPh966235hOf+MQLBt9rDB8+PEcddVTOP//8Lg0H0B1cdc2sJMnBH9yn058duOEGeWTuY1m2bHn69e2bx+c9kYXPPJsb/3BLtn/nB2p+Zs8D/k+S5MqpP8i2o7Z8+YMDANDe9b2m+/uf1eoE/9vf/p7BgzfOwmeeXev8mmtNz+sBBwDqo1Ph98qVK9OrV6+XdLZ3795ZuXLlyxoKoLtY+MyzueGmm7PhBv3z3nfv3qnPLm1blvsffDhNTY0ZsOGGSZINN9wgB+9fO0S/8eZb8tQ/FmS/9+2Z3r17Z8MNN+jy/AAAb3T33fdAWlsfz+6775I+fZqydGlb+70+fZqy++675IEHHm5/2WWS/Po3v88737lr3vLmUZk+/boOz3vzm5+r/nzo4bmvzRcAAF5Qp8LvUaNG5fLLL89BBx2UPn36vOC5JUuW5LLLLtP3DRTv6l/OzsqVq7L/gWNr/sfBJUuW5sl/PJ3NRo7ocH3Z8uX56nfOyZKlbTlwv/elZ8/nXpI5bMjgfP2LE2r+rn858ZQ89Y8F+fynjtMFDgDwCpoy9dL862mfzWlfntChw/u0L09I//798u3vfL/D+QsvujyfO/kTOeGT43PhRZfnscfmJUn69eubU7/wqSTJlVde/dp9AQCgpk6F38cee2xOOumk7L///jnkkEPaX3jZq1evrFixov2Fl1dccUXmzZuXc87xcg+gbFdd81zP44deoPJk4bOL8sEjP57t3zwqW7xp0wzaeGD+8fTC3Pzn2zP/iaey9Zab5XMnfOy1HBkA4A3hmPFH5J3v3DVJsv322yZJjj3miLznPbslSX7/+1syZeqlSZIzzzovH/zgPjnl8ydmpx23z+133J0xO+2QvffeM3Pm3J5zvvcfHZ790EOP5gunfjPnTPpmbvvzrzJ9xnVZvnxF9v3AuGy++cj88Ec/yQ2//t1r+G0BgFo6FX7vvffe+cY3vpHvfOc7+d73vpeGhoa1zlSr1fTt2zdf/epXs/fee79igwK83tz9P3/NfQ88lB3esk1GbVm7I3LDDfrlwwftn7v/8tfcdPOcPLtocXr37pUtNhuZ/3NIS4485INp7N37NZ4cAKB873znrjn6o4etdW1NIJ6kPfxeurQtY8d9KP/2ryfnoIP2zZ577p7HH38iEyeen69/c2KWLVu21vPPPW9qHn54bk7+7PE5/LCW9Oy5Xu75n7/ljG9/r/25ABSgUq33BHRBQ7Va7fT/BZ999tnMnj07d955Z+bNm5dly5alsbExQ4cOzejRo/Pe9743G2zwynXRrnzqgVfsWQAAvPKahr+r3iMAALAOq1a01nuEbmnJtz5a7xFeFX2//J/1HuE10anN7zU22GCDHHTQQTnooINe6XkAAAAAAKDLetR7AAAAAAAAeKW9rM1vAAAAAIDiVSv1noAusPkNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHF0fgMAAAAA1FKp1nsCusDmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBLpVLvCegCm98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEfnNwAAAABALZVqvSegC2x+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXxwksAAAAAgFqqlXpPQBfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqVTrPQFdYPMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKFaqdR7BLrA5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQS6Va7wnoApvfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH5zcAAAAAQC06v7s1m98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcbzwEgAAAACglmql3hPQBTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgFoq1XpPQBfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqOr87tZsfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHJ3fAAAAAAC16Pzu1mx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAALVUKvWegC6w+Q0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcYTfAAAAAAAUxwsvAQAAAABqqVTrPQFdYPMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqEXnd7dm8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACooVrV+d2d2fwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDg6vwEAAAAAaqno/O7ObH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxfHCSwAAAACAWrzwsluz+Q0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcXR+AwAAAADUUNX53a3Z/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABq0fndrdn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqp1HsAusLmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBDtVKt9wh0gc1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDheeAkAAAAAUIsXXnZrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAWir1HoCusPkNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHF0fgMAAAAA1FCtVOs9Al1g8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACopVLvAegKm98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEfnNwAAAABADdVKtd4j0AU2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojvAbAAAAAIDieOElAAAAAEAtlXoPQFfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqOr87tZsfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHJ3fAAAAAAC16Pzu1mx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAADVUdX53aza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgFp0fndrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4njhJQAAAABADVUvvFzLNtts84L3rr766owaNar9z6tWrcqUKVMybdq0tLa2ZsCAARk3blwmTJiQgQMHrvX5BQsWZNKkSZk9e3YWLlyY5ubmHHLIIRk/fnx69ux8lC38BgAAAADgJdt5551z2GGHrXV92LBhHf78xS9+MTNnzsxee+2VY489NnPnzs1FF12U2267LZdffnn69OnTfnbx4sU56qij8uCDD+bII4/MNttskzlz5uSss87KAw88kDPOOKPTcwq/AQAAAAB4yTbddNO0tLSs88zNN9+cmTNnZuzYsZk8eXL79e222y4nnXRSpkyZkhNPPLH9+gUXXJD7778/p556asaPH58kOfTQQ9O/f/9cfPHFOfjgg7PLLrt0ak6d3wAAAAAAdMrKlSuzePHiF7w/Y8aMJGkPstfYZ5990tzc3H7/+eebmppyxBFHdLi+5vPTp0/v9Iw2vwEAAAAAaii183vcuHHrvD979ux13p81a1ZmzpyZ1atXp3///tlzzz0zYcKEjBgxov3MnXfemR49emSnnXZa6/NjxozJNddck4ULF2bAgAF56qmn0tramjFjxqSxsbHD2REjRmTw4MG56667XvoX/P+E3wAAAAAAvCTbb7999tlnn2y22WZZsWJFbr311lxxxRW56aabcskll2TLLbdMksybNy8DBw5Mr1691nrGkCFD2s8MGDAg8+bNS5IMHTq05u8cOnRoHnnkkU7PKvwGAAAAAHgDebHN7nWZNm1ahz/vv//+2XPPPfPxj388p59+ei644IIkybJly7LhhhvWfEbv3r3bzzz/Z62gfM35tra2Ts+q8xsAAAAAgJftPe95T3bcccf88Y9/zPLly5MkjY2NWbFiRc3zzz/z/J/rOt/U1NTpuYTfAAAAAAA1VCtl/vNqGDFiRFatWpWFCxcmea6qZMGCBTUD7fnz57efef7PNfUn/2zevHntVSmdIfwGAAAAAKBLHnrooay//voZOHBgkmT06NGpVCq588471zp7++23Z+TIkRkwYECSZNCgQRk+fHjuvffe9gqUNVpbW/Pkk09m9OjRnZ5J+A0AAAAAwItasGBBzevXXHNN7rnnnuyxxx7tvd0tLS1JkilTpnQ4e/3116e1tbX9/hoHHHBA2tracumll3a4PnXq1A7P6wwvvAQAAAAA4EVNnjw5t912W97xjndk2LBhWblyZW677bZcf/31GTx4cL785S+3n919992z//7755prrsnxxx+fcePGZe7cubnwwguz1VZbZfz48R2efdxxx2XWrFk588wz09ramm222SZz5szJjBkz0tLSkl133bXT8zZUq9Vql7/1q2zlUw/UewQAANahafi76j0CAADrsGpFa71H6Jbm77lnvUd4VQz5zW9e1udmz56dSy+9NPfdd18WLFiQarWa5ubm7LnnnjnuuOOy8cYbdzi/cuXKTJkyJVdddVVaW1szYMCAjB07NhMmTMhGG2201vOffvrpTJo0KTfccEMWLlyY5ubmfOhDH8oxxxyTnj07v8ct/AYAoMuE3wAAr2/C75dH+N296fwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDidbwkHAAAAAHgDqFbqPQFdYfMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKFaaaj3CHSBzW8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiqPzGwAAAACghmql3hPQFTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBqq1YZ6j0AX2PwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDg6vwEAAAAAaqhW6j0BXWHzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOF14CAAAAANRQrTTUewS6wOY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUEO1Wu8J6Aqb3wAAAAAAFEf4DQAAAABAcYTfAAAAAAAUR+c3AAAAAEAN1UpDvUegC2x+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAADXo/O7ebH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVSr9Z6ArrD5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABTHCy8BAAAAAGqoVhrqPQJdYPMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKFa1fndndn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqoVuo9AV1h8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACooVJtqPcIdIHNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKo/MbAAAAAKCGqs7vbs3mNwAAAAAAxRF+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcL7wEAAAAAKihWvHCy+7M5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQQ7Va7wnoCpvfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH5zcAAAAAQA3VSkO9R6ALbH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVSqOr+7M5vfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHG88BIAAAAAoIaqF152aza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBqq1XpPQFfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqFQb6j0CXWDzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDi6PwGAAAAAKihqvO7W7P5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRQrdZ7ArrC5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHC+8BAAAAACooVJtqPcIdIHNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACK0y06v9+947H1HgEAgHVYdOVn6j0CAAC84qo6v7s1m98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFKdbdH4DAAAAALzWKjq/uzWb3wAAAAAAFEf4DQAAAABAcYTfAAAAAAAUR+c3AAAAAEAN1XoPQJfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqFQb6j0CXWDzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOF14CAAAAANRQ9cLLbs3mNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBDpd4D0CU2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaqmmo9wh0gc1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIqj8xsAAAAAoIZKtd4T0BU2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaKmmo9wh0gc1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDheeAkAAAAAUEPVCy+7NZvfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH5zcAAAAAQA2Veg9Al9j8BgAAAACgOMJvAAAAAABelkqlksMOOyzbbLNN/uVf/mWt+21tbTnrrLMyduzYbL/99hk7dmz+/d//PW1tbTWf19rampNPPjnveMc7Mnr06LS0tOSKK654WbOpPQEAAAAA4GW56KKLct9999W8t3r16nz84x/PLbfckpaWluyyyy659957c8EFF+Suu+7K1KlT06PH/+5nz5s3L4cffngWLVqUo48+OiNGjMjs2bNz2mmnZf78+TnxxBM7NZvwGwAAAACghmoa6j3C69qjjz6ac845J5/5zGdy+umnr3X/5z//eW655ZZ85CMfyWmnndZ+vbm5Od/5zncyc+bMHHjgge3XJ06cmCeffDLf//73s/feeydJDjvssBx//PGZPHlyWlpasummm77k+dSeAAAAAADQaaeddlq22mqrfOQjH6l5f8aMGUmS8ePHd7h+5JFHprGxMdOnT2+/1tbWllmzZmXEiBHtwfca48ePz6pVq3L11Vd3aj6b3wAAAAAAdMrPfvaz/PnPf860adM6VJesUa1Wc/fdd2eTTTZJc3Nzh3uNjY1585vfnLvvvrv92t/+9rcsW7YsO+2001rPGjNmTBoaGnLXXXd1akbhNwAAAADAG8i4cePWeX/27NnrvD9//vx897vfzfjx47PtttvWPLNw4cK0tbVl6623rnl/yJAhuf3227N48eL069cv8+bNS5IMHTp0rbO9evXKwIEDM3/+/HXO9c+E3wAAAAAANVTqPcDr1Fe/+tUMHDhwnS+gXLZsWZLngutaevfuneS5upN+/fqlra3tRc+vOfNSCb8BAAAAAN5AXmyze12uvfba3HDDDZk6dWoaGxtf8NyaeytWrKh5f/ny5UmSpqamDj/XdX7gwIGdmlX4DQAAAADAi1qxYkW++c1vZo899khzc3MefvjhDveXLVuWhx9+OH379s3GG2+cpqam9jqTfzZ//vz069cv/fr1S/K/dSe1zq9YsSILFizIjjvu2Kl5hd8AAAAAALyoZcuW5emnn87vfve77L333mvdv/3227P33ntn3333zdlnn53tt98+c+bMSWtra4eXXi5btix/+ctfMmbMmPZro0aNSu/evXPHHXes9dw77rgj1Wo1o0eP7tS8wm8AAAAAAF5UU1NTzjnnnJr3Pv3pT2fUqFE54YQTMmzYsCRJS0tL5syZk6lTp+a0005rP3vppZdm2bJlaWlp6fDsvffeO1dffXWuv/76DuH6lClT0rNnz+y///6dmlf4DQAAAABQgxdedrT++uvn/e9//wve33jjjTvcP/jggzN9+vT85Cc/yaJFi7Lzzjvnr3/9ay655JLsuuuuOeCAAzp8/rOf/WxuvvnmnHLKKbnnnnsyYsSIzJ49O7/+9a/zyU9+MiNHjuzUvMJvAAAAAABeceutt15+9KMf5dxzz811112Xa6+9NoMHD8748eNzwgknZL311utwfvjw4bnsssty9tln57LLLsvSpUuz2Wab5etf/3oOP/zwTv/+hmq1Wn2lvsyrZbfmveo9AgAA63DD5P3qPQIAAOvQdMDn6j1Ct/SLIR+u9wivin3nX1bvEV4TPeo9AAAAAAAAvNLUngAAAAAA1FBNQ71HoAtsfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHJ3fAAAAAAA1VFR+d2s2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaKlH63Z3Z/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqNZ7ALrE5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHC+8BAAAAACooVLvAegSm98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEfnNwAAAABADZWGhnqPQBfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqNZ7ALrE5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQQ6XeA9AlNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGioN9Z6ArrD5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABTHCy8BAAAAAGqoxBsvuzOb3wAAAAAAFEf4DQAAAABAcYTfAAAAAAAUR+c3AAAAAEAN1XoPQJfY/AYAAAAAoDjCbwAAAAAAiiP8BgAAAACgODq/AQAAAABqqDTUewK6wuY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUEOl3gPQJTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOJ44SUAAAAAQA3Veg9Al9j8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqoNNR7ArrC5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQQ6XeA9AlNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGnR+d282vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaqg31noCusPkNAAAAAEBxhN8AAAAAABRH+A0AAAAAQHGE3wAAAAAAFMcLLwEAAAAAaqjUewC6xOY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUIPO7+7N5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQQ7XeA9AlNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGioN9Z6ArrD5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRQqfcAdInNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4XngJAAAAAFCDF152bza/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBqq9R6ALrH5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRQaaj3BHSFzW8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiqPzGwAAAACghkq9B6BLbH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVTrPQBdYvMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI4XXgIAAAAA1FDxystuzeY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUEOl3gPQJTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBqq9R6ALrH5DQAAAABAcYTfAAAAAAAUR/gNAAAAAEBxdH4DAAAAANRQqfcAdInNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4XngJAAAAAFBDpaHeE9AVNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGiqp1nsEusDmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFCDxu/uzeY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUEOl3gPQJTa/AQAAAAAojvAbAAAAAIDiCL8BAAAAACiOzm8AAAAAgBoqqdZ7hNeVp59+OmeeeWbuueeezJ8/P0uXLs3gwYOz44475mMf+1i22267DudXrVqVKVOmZNq0aWltbc2AAQMybty4TJgwIQMHDlzr+QsWLMikSZMye/bsLFy4MM3NzTnkkEMyfvz49OzZ+Shb+A0AAAAAwItatGhRHnzwwey+++4ZPnx4mpqa0tramp///Oc57LDDcv755+dd73pX+/kvfvGLmTlzZvbaa68ce+yxmTt3bi666KLcdtttufzyy9OnT5/2s4sXL85RRx2VBx98MEceeWS22WabzJkzJ2eddVYeeOCBnHHGGZ2eV/gNAAAAAMCLetOb3pTLLrtsretHHHFE9tprr/z4xz9uD79vvvnmzJw5M2PHjs3kyZPbz2633XY56aSTMmXKlJx44ont1y+44ILcf//9OfXUUzN+/PgkyaGHHpr+/fvn4osvzsEHH5xddtmlU/Pq/AYAAAAA4GUbNGhQevfunUWLFrVfmzFjRpK0B9lr7LPPPmlubm6///zzTU1NOeKIIzpcX/P56dOnd3ou4TcAAAAAAC/ZypUr8/TTT+fJJ5/MXXfdlZNPPjlLly7Nnnvu2X7mzjvvTI8ePbLTTjut9fkxY8bkkUceycKFC5MkTz31VFpbW7PtttumsbGxw9kRI0Zk8ODBueuuuzo9p9oTAAAAAIAaSn3d5bhx49Z5f/bs2eu8f9ttt+WjH/1o+5/79++f4447LieccEL7tXnz5mXgwIHp1avXWp8fMmRI+5kBAwZk3rx5SZKhQ4fW/H1Dhw7NI488ss6ZahF+AwAAAADwkm277baZOnVqVqxYkYceeigzZszIkiVLsmLFivTs+VzkvGzZsmy44YY1P9+7d+/2M8//WSsoX3O+ra2t03MKvwEAAAAA3kBebLP7xWy44YbZfffd2/980EEHpaWlJY8++mj+4z/+I0nS2NiYFStW1Pz88uXL2888/+e6zjc1NXV6Tp3fAAAAAAC8bBtuuGHGjh2bm266KXPnzk3yXFXJggULagba8+fPbz/z/J9r6k/+2bx589qrUjpD+A0AAAAAUEOl0H9eDWuqS5599tkkyejRo1OpVHLnnXeudfb222/PyJEjM2DAgCTJoEGDMnz48Nx7773tz1mjtbU1Tz75ZEaPHt3pmYTfAAAAAAC8qKeeeqrm9blz52b27Nnp379/ttxyyyRJS0tLkmTKlCkdzl5//fVpbW1tv7/GAQcckLa2tlx66aUdrk+dOrXD8zpD5zcAAAAAAC/qhz/8Yf7whz/k3e9+d0aMGJEkeeCBBzJ9+vQsXbo03/72t9tfZrn77rtn//33zzXXXJPjjz8+48aNy9y5c3PhhRdmq622yvjx4zs8+7jjjsusWbNy5plnprW1Ndtss03mzJmTGTNmpKWlJbvuumun5xV+AwAAAADwovbaa6/Mnz8/s2bNytNPP51Vq1Zlk002yZ577pmjjz56rWqSb3/72xk1alSuuuqqfO1rX8uAAQPS0tKSCRMmpG/fvh3O9uvXL5dcckkmTZqUX/7yl7nsssvS3Nyck08+Occcc8zLmrehWq1WX/a3fY3s1rxXvUcAAGAdbpi8X71HAABgHZoO+Fy9R+iWPrvZh+s9wqti4kOX1XuE14TObwAAAAAAiiP8BgAAAACgOMJvAAAAAACK44WXAAAAAAA1vO5flsg62fwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDg6vwEAAAAAaqjUewC6xOY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUBwvvAQAAAAAqKGaar1HoAtsfgMAAAAAUBzhNwAAAAAAxRF+AwAAAABQHJ3fAAAAAAA1VOo9AF1i8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACooZJqvUegC2x+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAADVo/O7ebH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVS0fndrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4njhJQAAAABADZV6D0CX2PwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDg6vwE6afDQQRm7/3uy29i3501bjczGgzfKswsX5a45/52LJ1+W/7n9L2t9pk+/PvnYyf+SPfd9VzYevFH+8cTTueGa3+SCiRelbemydf6+nuv3zAXXTs6o7bbKw/c/kg+/5+hX66sBABTnhrsfzOU3/yX3tj6VthWrMqh/U0a/aUgm7Ldrhg7olySZfP2t+eGvbnvBZ1z7xQ+neaP+a12vVKqZ+ee/Zfqcv+b+eQuyavXqbLJh3+y02dB8oWW39G3s9ap9LwBeG9VU6z0CXSD8BuikQ8YflI+eeGTmPtiaW3775yz8x8KM2GJE3r3PHnn3+9+ZfzvxW5k989ft5xubGjN52qSM2n7r/PE3c/Kr6Tdk1PZb5/984sMZ844d84kPfTorlq98wd937GePzojNml+LrwYAUIxqtZpvTvtdpv3p3my68QbZZ8ct07dx/Tz5zJLc+sC8PL5gcXv4vcYH37Z1htcIufs3rR1ir1i1Op/7z//KjX95JKOGbZQDdh6VXj17ZN7CJfndvY/mhH3eJvwGgDoTfgN00v/ccW8++aEJuf2Pd3a4vuOuO+T7l/97Tjl9Qm785e+ycsVzgfZRn/xwRm2/df7zB5dk8hk/bj//iS8el4+eeGQ+fNyh+c8fXFLzd71lp21z1CePyKSv/CCfO/3Tr96XAgAozCW/uyfT/nRvDtvtLfnCgbtlvR4dWz9Xra6s9ZkDdhmVXbYc/pKef84vbsmNf3kkn953l4zfa6cO9yoVW4IA8Hqg8xugk3573U1rBd9Jcuctd+fWP9yRDQZukC233bz9+gFH7Jsli5dm6qSfdDg/ddJPsmTx0nzwiH1r/p5evdfPv046NXfdcnemXTT9Ff0OAAAlW7ZyVX74q9syYqP+OaVl7eA7SXqu9/L/dXj+M0ty+e/vyVs3H7pW8J0kPXo0pEePhpf9fADglWHzG+AVtHrVqud+rl6dJNl0ixEZPGxw/vjrW7KsrWO397K2Zbl7zn/nHXvtmk2GD84Tjz3Z4f7xpx6XIc2b5HNHf+m1GR4AoBA3/3Vunm1bnpZdRqVSrWb23Q/m4SefSf+mXnn71s0ZOWjDmp+77YF5+e9HnkhDQ0NGDtow79i6OX16r7/Wuf+668GsqlTzvtGbZ8myFfnN/zySeQsXZ+N+TdltmxEZsmHfV/srAvAaWfvvCdGdCL8BXiFDhm+Snfd4W56c91T+/pcHkySbbj4iSfLog3NrfubRB+fmHXvtmk03H9Eh/N7p7aNz2LEH53tfm5zWhx979YcHACjIX1qfSpL0aGjIoROn5eEnn2m/16OhIUe9a/t89oPvWOtzk6+/tcOf+zf1yikH7JYP7jyq4/PnPvf/ty1qW5EDz7wiTz67tP3e+uv1yEn77pqPvHuHV+z7AAAvj/Ab4BWwXs/18m/f+1J6N/bKeaf/KJXKc/9tuF//57Z+Fi9aUvNzSxYv7XAuee4FmV+eeEr++9b/yRVTrnqVJwcAKM/Ti9uSJBffdHe2bR6Ui086MFtsMiD3tj6Vb0z7Xf7zxrszYuMNctjub0mSbDNso3z1sHdnly2HZVD/PnlqUVtu+ssjOW/WrfnKz36b/k29s+d2b3re85/7G30//K/b8o6tm3P+x/fN0A375tYH5+UbV96Uf7/6j9l8kwHZY9tNX/svDwC0e1U7vydPnpy3vOUtr+avAKi7hoaG/OvZp2bMbjtm+sXX5JfTftWl533qK8dn8JBB+dbJ30216mVJAACdteZ9k+uv1yNnH/2+bL/p4PTpvX7eusWwnPmRcenR0JCf3Hh3+/mxO2yeA3fZJs0bbZDe6/dM80b98+F3bpczPzIuSXLuL//8T89/7hds1K8pZ330fdlyyMD0beyVd795ZP7t0HcnSX7y27teg28KAKzLq775LbgBStbQ0JAvTzwl+xz83lw37fp899SJHe6v2fh+/mb38/Xt16fDuTG77ZiDP9qS7399ch59oHZVCgAA69avsVeS5C0jBmeTf+rf3mroRmneuH8eferZPNu2PBs09X7B57x96+aM2HiD3Dfv6SxetqL9uf2anvv59q2Gp6lXx3+t3n3UiPTquV7umfvUK/mVAKiTamSb3ZnaE4CXqaGhIaed/YXse+g+uf7ns/PNCd9Z6z/4ren6XtP9/c/+uRN81HZbJUk+9ZVP5FNf+cRa59+01cjc3PrrLHpmcfZ+ywdfse8CAFCSzQY/90LL/v8/pP5n/f9/iL185eqkad3PGtinMY/m2Sxbsao9/P7f568dnPfo0ZC+vdfP4mUrXu74AMArpNPh9/bbb/+Sz9r6Bkr1/OD7VzNuyNdOOr295/v5Hn1gbp58/MnssMv2aWxqzLK2Ze33Gpsas8Mu26f14cfaX3b5wF8fzMxLrq35Ow84cr8semZxfn3tb7Osbfmr88UAAAqwy5bDkyQPPLFwrXsrV1fy6D+eTVOvnhnYt3Gdz2lbsTJ/n78gTb16ZsDzzu661fD8x+w78sD8BWt95unFbVmwZFne9P8DcgCgfjodfq9evTobb7xxNt988xc9+9hjj+Wxxx57WYMBvF6tqTrZ99B9Mvvq3+Rrn/pWzeB7jZmX/iLHfvbojJ/wkUw+48ft18dP+Ej69uuT//z+T9uvzbnptsy56baazzngyP3y9JNP54zPn/WKfRcAgBJtOmiD7DaqOTf/rTVX/eneHPz2bdvvTb3hjixqW5H93rpVeq7XI0uWrchTi5bmTYMHdHjGspWr8rUrbsqS5SvTssuo9Fzvf1+Z9bYthmWLTQbkT/c/lpv/Nje7jXrub/NVq9V8/7o5SZK9R2/x6n9RAGCdOh1+jxw5MsOGDcuFF174omcnT56c733vey9nLoDXrWM+89Hsd9j7s2Tx0jzywKP5l09/ZK0zN876Xe675+9JkovPuyzv2ued+eiJR2bU9lvnb3f/LaN2GJV37LlL/uf2v+Ty/7jytf4KAADF+9JBe+Toc2fm61felF/f81A2Hzwg9z72j9xy/2MZNrBfPrPf25MkC5cuz4FnXpHtRgzO5psMyKAN+uQfi9ryp/taM/+ZJdl66EbtZ9dYr0ePfO3w9+S486/Npy74ZcbusHmGbNg3tz84L//96JN5c/OgHDN2x3p8bQDgeTodfr/lLW/JH/7wh1djFoBuYdimQ5M897LK8TWC7yR5fO689vB7WduyfPJDE/Kxk4/Onvu+O2/bfac89cQ/csn5l+eCiRdluT5IAIBX3KaDNsglnz4w5826NX/466O5+W+tGdS/KYfv/pb83/e9NRv1e67se8M+vXPYbm/Jfz/6ZH5376NZ1LY8vdfvmc03GZAj9tguH37ndmlcf+1/dd5h5Ca5+KSWnH/9bbnlvtYsXr4ywwb0yzFjd8rHxu6Upl7rv9ZfGYBXwQv/PW+6g4ZqJ4u5f/SjH2XixIn51a9+lU033XSdZ2fMmJErr7wyP/nJT7o05G7Ne3Xp8wAAvLpumLxfvUcAAGAdmg74XL1H6JaO3uxD9R7hVXHRQ9PqPcJrotPhdz0IvwEAXt+E3wAAr2/C75dH+N299XjxIwAAAAAA0L10uvMbAAAAAOCNoPL6L81gHWx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAADVo/O7ebH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVS0fndrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGqo6v7s1m98AAAAAABRH+A0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcbzwEgAAAACghkq9B6BLbH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVRSrfcIdIHNbwAAAAAAiiP8BgAAAACgOMJvAAAAAACKo/MbAAAAAKCGqs7vbs3mNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBDpd4D0CU2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaqtVqvUegC2x+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXxwksAAAAAgBoq8cLL7szmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBDpd4D0CU2vwEAAAAAKI7wGwAAAACA4gi/AQAAAAAojs5vAAAAAIAaqqnWewS6wOY3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXR+Q0AAAAAUENF53e3ZvMbAAAAAIDiCL8BAAAAACiO8BsAAAAAgOLo/AYAAAAAqKFa1fndndn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4wm8AAAAAAIrjhZcAAAAAADVU6j0AXWLzGwAAAACA4gi/AQAAAAAojvAbAAAAAIDi6PwGAAAAAKihmmq9R6ALbH4DAAAAAFAc4TcAAAAAAMURfgMAAAAAUByd3wAAAAAANVR0fndrNr8BAAAAACiO8BsAAAAAgOIIvwEAAAAAKI7ObwAAAACAGqpVnd/dmc1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDheeAkAAAAAUEMlXnjZndn8BgAAAACgOMJvAAAAAACKI/wGAAAAAKA4Or8BAAAAAGqo6vzu1mx+AwAAAABQHOE3AAAAAADFEX4DAAAAAFAcnd8AAAAAADVUqjq/uzOb3wAAAAAAFEf4DQAAAABAcYTfAAAAAAAUR+c3AAAAAEANGr+7N5vfAAAAAAAUR/gNAAAAAEBxhN8AAAAAABRH5zcAAAAAQA0Vrd/dms1vAAAAAACKI/wGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDheeAkAAAAAUIMXXnZvNr8BAAAAACiOzW8AAAAAAF7UQw89lKuvvjq///3v8+ijj2bJkiUZPnx4dt9993z84x/PJpts0uH8qlWrMmXKlEybNi2tra0ZMGBAxo0blwkTJmTgwIFrPX/BggWZNGlSZs+enYULF6a5uTmHHHJIxo8fn549Ox9lC78BAAAAAHhRV155ZX76059mr732ygc+8IE0NjbmjjvuyCWXXJKZM2fm0ksvzZZbbtl+/otf/GJmzpyZvfbaK8cee2zmzp2biy66KLfddlsuv/zy9OnTp/3s4sWLc9RRR+XBBx/MkUcemW222SZz5szJWWedlQceeCBnnHFGp+cVfgMAAAAA1FCt6vx+vn322Scf//jHs8EGG7RfO/zww7PTTjvlK1/5Sr73ve/lnHPOSZLcfPPNmTlzZsaOHZvJkye3n99uu+1y0kknZcqUKTnxxBPbr19wwQW5//77c+qpp2b8+PFJkkMPPTT9+/fPxRdfnIMPPji77LJLp+bV+Q0AAAAAwIvaYYcdOgTfa+y3335Jkr/+9a/t12bMmJEk7UH2Gvvss0+am5vb7z//fFNTU4444ogO19d8fvr06Z2e1+Y3AAAAAMAbyLhx49Z5f/bs2Z163vz585MkgwYNar925513pkePHtlpp53WOj9mzJhcc801WbhwYQYMGJCnnnoqra2tGTNmTBobGzucHTFiRAYPHpy77rqrUzMlNr8BAAAAAOiCNVUnBx98cPu1efPmZeDAgenVq9da54cMGdJ+5vk/hw4dWvP5Q4cObQ/YO8PmNwAAAABADZWU2fnd2c3udTn//PMza9asvPe9781BBx3Ufn3ZsmXZcMMNa36md+/e7Wee/7NWUL7mfFtbW6dns/kNAAAAAECnXXTRRTn77LOz66675qyzzkpDQ0P7vcbGxqxYsaLm55YvX95+5vk/13W+qamp0/MJvwEAAAAA6JSpU6fm9NNPz2677ZYf/ehHa4XTQ4cOzYIFC2oG2msqTNbUnKz5uab+5J/NmzevvSqlM4TfAAAAAAC8ZD/60Y/y7W9/O+9617vywx/+sOZW9ujRo1OpVHLnnXeude/222/PyJEjM2DAgCTPvShz+PDhuffee9srUNZobW3Nk08+mdGjR3d6TuE3AAAAAEAN1UL/1xXnn39+/v3f/z177bVXzjvvvPb+7n/W0tKSJJkyZUqH69dff31aW1vb769xwAEHpK2tLZdeemmH61OnTu3wvM7wwksAAAAAAF7UT3/605x99tkZNGhQ3ve+9+W6667rcL9v375573vfmyTZfffds//+++eaa67J8ccfn3HjxmXu3Lm58MILs9VWW2X8+PEdPnvcccdl1qxZOfPMM9Pa2pptttkmc+bMyYwZM9LS0pJdd9210/M2VKvV1/0rS3dr3qveIwAAsA43TN6v3iMAALAOTQd8rt4jdEu7DH93vUd4Vcx57MaX9blTTz01P//5z1/wfnNzc2644Yb2P69cuTJTpkzJVVddldbW1gwYMCBjx47NhAkTstFGG631+aeffjqTJk3KDTfckIULF6a5uTkf+tCHcswxx6Rnz87vcQu/AQDoMuE3AMDrm/D75RF+d29qTwAAAAAAaugGe8OsgxdeAgAAAABQHOE3AAAAAADFEX4DAAAAAFAc4TcAAAAAAMXxwksAAAAAgBoq8cLL7szmNwAAAAAAxRF+AwAAAABQHOE3AAAAAADF0fkNAAAAAFBDtarzuzuz+Q0AAAAAQHGE3wAAAAAAFEf4DQAAAABAcXR+AwAAAADUUInO7+7M5jcAAAAAAMURfgMAAAAAUBzhNwAAAAAAxdH5DQAAAABQQ1Xnd7dm8xsAAAAAgOIIvwEAAAAAKI7wGwAAAACA4uj8BgAAAACooVLV+d2d2fwGAAAAAKA4wm8AAAAAAIoj/AYAAAAAoDjCbwAAAAAAiuOFlwAAAADA/2vv7lWjaqMwDC/FiGLQ0p/CMtEigigegAgW1hIscgJWYmGj4FlYiDYSsEhjIwTBA1AQFWHAFBYBR22EQEBNMa9VbLK/Rj/2sJ+5rnLvZtU3i/XSoZUHL4fM5jcAAAAAAHHEbwAAAAAA4ojfAAAAAADEcfMbAAAAAKDDpLn5PWQ2vwEAAAAAiCN+AwAAAAAQR/wGAAAAACCOm98AAAAAAB1aufk9ZDa/AQAAAACII34DAAAAABBH/AYAAAAAII6b3wAAAAAAHSbNze8hs/kNAAAAAEAc8RsAAAAAgDjiNwAAAAAAccRvAAAAAADiePASAAAAAKBDKw9eDpnNbwAAAAAA4ojfAAAAAADEEb8BAAAAAIjj5jcAAAAAQIdJc/N7yGx+AwAAAAAQR/wGAAAAACCO+A0AAAAAQBw3vwEAAAAAOrRy83vIbH4DAAAAABBH/AYAAAAAII74DQAAAABAHDe/AQAAAAA6tDaZ9gj8A5vfAAAAAADEEb8BAAAAAIgjfgMAAAAAEMfNbwAAAACADpNq0x6Bf2DzGwAAAACAOOI3AAAAAABxxG8AAAAAAOKI3wAAAAAAxPHgJQAAAABAh9Y8eDlkNr8BAAAAAIgjfgMAAAAAEEf8BgAAAAAgjpvfAAAAAAAdJuXm95DZ/AYAAAAAII74DQAAAABAHPEbAAAAAIA4bn4DAAAAAHRozc3vIbP5DQAAAABAHPEbAAAAAIA44jcAAAAAAHHc/AYAAAAA6DBx83vQbH4DAAAAABBH/AYAAAAAII74DQAAAABAHDe/AQAAAAA6tHLze8hsfgMAAAAAEEf8BgAAAAAgjvgNAAAAAEAc8RsAAAAAgDgevAQAAAAA6NCaBy+HzOY3AAAAAABxxG8AAAAAAOKI3wAAAAAAxHHzGwAAAACgw6Tc/B4ym98AAAAAAMQRvwEAAAAAiCN+AwAAAAAQx81vAAAAAIAOrbn5PWQ2vwEAAAAAiCN+AwAAAAAQR/wGAAAAACCOm98AAAAAAB0mbn4Pms1vAAAAAADiiN8AAAAAAMQRvwEAAAAAiOPmNwAAAABAh+bm96DZ/AYAAAAAII74DQAAAABAHPEbAAAAAIA44jcAAAAAAHE8eAkAAAAA0GFSHrwcMpvfAAAAAADEEb8BAAAAAIgjfgMAAAAAEMfNbwAAAACADq25+T1kNr8BAAAAAIgjfgMAAAAAEEf8BgAAAAAgjpvfAAAAAAAdJm5+D5rNbwAAAAAA4ojfAAAAAADEEb8BAAAAAIjj5jcAAAAAQIdWbn4Pmc1vAAAAAADiiN8AAAAAAMQRvwEAAAAAiCN+AwAAAAAQx4OXAAAAAAAdJs2Dl0Nm8xsAAAAAgDjiNwAAAAAAccRvAAAAAADiuPkNAAAAANChufk9aDa/AQAAAACII34DAAAAABBH/AYAAAAAII6b3wAAAAAAHVq5+T1kNr8BAAAAAIgjfgMAAAAAEEf8BgAAAAAgjpvfAAAAAAAdWnPze8hsfgMAAAAAEEf8BgAAAAAgjvgNAAAAAEAcN78BAAAAADq4+T1sNr8BAAAAAIgjfgMAAAAAEEf8BgAAAAAgjvgNAAAAAEAcD14CAAAAAHTw3OWw2fwGAAAAACCO+A0AAAAAQBzxGwAAAACAOPtaa07XAAAAAAAQxeY3AAAAAABxxG8AAAAAAOKI3wAAAAAAxBG/AQAAAACII34DAAAAABBH/AYAAAAAII74DQAAAABAHPEbAAAAAIA44jcAAAAAAHHEbwAAAAAA4ojfAAAAAADEEb8BAAAAAIgjfgMAAAAAEOfAtAcAmCUvXryoR48e1cbGRs3NzdWFCxfq9u3btbCwMO3RAABm2sOHD2s0GtVoNKrNzc3av39/jUajaY8FAPyDfa21Nu0hAGbB2tpa3bt3rxYWFmp5ebl+/fpVq6urtbW1VU+fPq3FxcVpjwgAMLMWFxfr6NGjdfbs2fr06VN9//5d/AaAgRO/AXqwtbVVly9frvn5+Xr+/HnNz89XVdV4PK5r167V0tJSPXnyZMpTAgDMrs3NzTp9+nRVVa2srNSbN2/EbwAYODe/AXrw8uXL2t7eruvXr/8J31VVp06dqqtXr9arV6/qy5cvU5wQAGC27YZvACCH+A3Qg/fv31dV1fnz5/f82/324cOHXmcCAAAASCZ+A/Tg27dvVVV14sSJPf92v339+rXXmQAAAACSid8APfjx40dVVR08eHDPv91vP3/+7HUmAAAAgGTiN0APDh8+XFVVOzs7e/7tfjt06FCvMwEAAAAkE78BenD8+PGq6j5tsvut6yQKAAAAAH9H/Abowblz56qq6u3bt3v+vXv3rqqqlpaW+hwJAAAAIJr4DdCDK1eu1JEjR2ptba22t7f/fB+Px7W+vl6XLl2qkydPTnFCAAAAgCwHpj0AwCw4duxY3blzp+7fv183btyo5eXl2tnZqdXV1aqqunv37pQnBACYbc+ePavxeFxVVZ8/f67WWj148ODP/5s3b05rNADgL+1rrbVpDwEwK9bX1+vx48e1sbFRc3NzdfHixbp161adOXNm2qMBAMy0lZWVev369X/+//jxY4/TAAD/B/EbAAAAAIA4bn4DAAAAABBH/AYAAAAAII74DQAAAABAHPEbAAAAAIA44jcAAAAAAHHEbwAAAAAA4ojfAAAAAADEEb8BAAAAAIgjfgMAAAAAEEf8BgAAAAAgjvgNAAAAAEAc8RsAAAAAgDjiNwAAAAAAcX4DW2nL7LH9tEoAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "plt.figure(figsize=(20,14))\n",
        "sn.set(font_scale=1.2) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g') # for num predict size\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaO24aHTozF1"
      },
      "outputs": [],
      "source": [
        "label_dict = output_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc3pCLvlpMZa"
      },
      "outputs": [],
      "source": [
        "label = [key for key, value in label_dict.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjrVgHOCpPtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6478f86c-a74c-4453-874b-fcb1c9aaaedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.7871    0.8767    0.8295       860\n",
            "         pos     0.8609    0.7628    0.8089       860\n",
            "\n",
            "    accuracy                         0.8198      1720\n",
            "   macro avg     0.8240    0.8198    0.8192      1720\n",
            "weighted avg     0.8240    0.8198    0.8192      1720\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true, predicted_classes, target_names=label, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSASqSFIqFy6"
      },
      "source": [
        "# **CNN Model for Sentiment Analysis using Word Embedding from Gensim ‚¨á**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORiJQClCpfiG"
      },
      "outputs": [],
      "source": [
        "sentences = [st.split() for st in cleaned_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuVt80zhqV1g"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sentences, min_count=1, vector_size=DIMENSION, workers=6, sg=1, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL9A4VGBqfcN"
      },
      "outputs": [],
      "source": [
        "w2v_model.save('w2v_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL5WeHaVr3uX"
      },
      "outputs": [],
      "source": [
        "new_model = Word2Vec.load('w2v_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bu6rqRur7zJ"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, DIMENSION))\n",
        "\n",
        "for word, i in train_word_tokenizer.word_index.items():\n",
        "    if word in new_model.wv.index_to_key:\n",
        "        embedding_vector = new_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpJeVyWKtwPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc87e66-21c3-4011-f00b-2f33e7ed336b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13051, 256), 256, 13051)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "embedding_matrix.shape, DIMENSION, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuo9_tl1sHa1"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_w2v_model(length, vocab_size, embedding_matrix):\n",
        "    # channel 1\n",
        "    inputs1 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding1 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs1)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = tf.keras.layers.Flatten()(pool1)\n",
        "    # channel 2\n",
        "    inputs2 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding2 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs2)\n",
        "    conv2 = tf.keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "    drop2 = tf.keras.layers.Dropout(0.5)(conv2)\n",
        "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
        "    flat2 = tf.keras.layers.Flatten()(pool2)\n",
        "    # channel 3\n",
        "    inputs3 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding3 = tf.keras.layers.Embedding(vocab_size, DIMENSION, trainable = False, weights=[embedding_matrix])(inputs3)\n",
        "    conv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "    drop3 = tf.keras.layers.Dropout(0.5)(conv3)\n",
        "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
        "    flat3 = tf.keras.layers.Flatten()(pool3)\n",
        "    # merge\n",
        "    merged = tf.keras.layers.concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEvAd_SNsgdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b378003c-ba52-497e-8780-b709eae68986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 506, 256)     3341056     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 506, 256)     3341056     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 506, 256)     3341056     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 503, 32)      32800       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 501, 32)      49184       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 499, 32)      65568       ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 503, 32)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 501, 32)      0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 499, 32)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 251, 32)     0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 250, 32)     0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 249, 32)     0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 8032)         0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 8000)         0           ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 7968)         0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 24000)        0           ['flatten_3[0][0]',              \n",
            "                                                                  'flatten_4[0][0]',              \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           240010      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            22          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,410,752\n",
            "Trainable params: 387,584\n",
            "Non-trainable params: 10,023,168\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model2 = define_w2v_model(max_length, vocab_size, embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3y1ZlaCsos4"
      },
      "outputs": [],
      "source": [
        "filename = 'model2.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvlzLUuts9rb"
      },
      "outputs": [],
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 3, verbose=1,factor=0.1, min_lr=0.000001)\n",
        "callbacks_list = [checkpoint, learning_rate_reduction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ56evZwsd0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74480dc-0028-4cb7-f87d-0549bb4134d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5133\n",
            "Epoch 1: val_loss improved from inf to 0.68092, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 11ms/step - loss: 0.6871 - accuracy: 0.5116 - val_loss: 0.6809 - val_accuracy: 0.5128 - lr: 1.0000e-06\n",
            "Epoch 2/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5354\n",
            "Epoch 2: val_loss improved from 0.68092 to 0.67674, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6787 - accuracy: 0.5372 - val_loss: 0.6767 - val_accuracy: 0.5209 - lr: 1.0000e-06\n",
            "Epoch 3/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6758 - accuracy: 0.5406\n",
            "Epoch 3: val_loss improved from 0.67674 to 0.67383, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6756 - accuracy: 0.5411 - val_loss: 0.6738 - val_accuracy: 0.5233 - lr: 1.0000e-06\n",
            "Epoch 4/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6733 - accuracy: 0.5492\n",
            "Epoch 4: val_loss improved from 0.67383 to 0.67158, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6729 - accuracy: 0.5506 - val_loss: 0.6716 - val_accuracy: 0.5279 - lr: 1.0000e-06\n",
            "Epoch 5/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6701 - accuracy: 0.5456\n",
            "Epoch 5: val_loss improved from 0.67158 to 0.66982, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6697 - accuracy: 0.5465 - val_loss: 0.6698 - val_accuracy: 0.5337 - lr: 1.0000e-06\n",
            "Epoch 6/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6677 - accuracy: 0.5576\n",
            "Epoch 6: val_loss improved from 0.66982 to 0.66812, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.6679 - accuracy: 0.5564 - val_loss: 0.6681 - val_accuracy: 0.5471 - lr: 1.0000e-06\n",
            "Epoch 7/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.6666 - accuracy: 0.5625\n",
            "Epoch 7: val_loss improved from 0.66812 to 0.66663, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6668 - accuracy: 0.5618 - val_loss: 0.6666 - val_accuracy: 0.5570 - lr: 1.0000e-06\n",
            "Epoch 8/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6657 - accuracy: 0.5751\n",
            "Epoch 8: val_loss improved from 0.66663 to 0.66537, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6656 - accuracy: 0.5749 - val_loss: 0.6654 - val_accuracy: 0.5733 - lr: 1.0000e-06\n",
            "Epoch 9/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6634 - accuracy: 0.5878\n",
            "Epoch 9: val_loss improved from 0.66537 to 0.66409, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6634 - accuracy: 0.5878 - val_loss: 0.6641 - val_accuracy: 0.5860 - lr: 1.0000e-06\n",
            "Epoch 10/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.5929\n",
            "Epoch 10: val_loss improved from 0.66409 to 0.66286, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6622 - accuracy: 0.5933 - val_loss: 0.6629 - val_accuracy: 0.5971 - lr: 1.0000e-06\n",
            "Epoch 11/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6611 - accuracy: 0.6119\n",
            "Epoch 11: val_loss improved from 0.66286 to 0.66151, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.6613 - accuracy: 0.6115 - val_loss: 0.6615 - val_accuracy: 0.6203 - lr: 1.0000e-06\n",
            "Epoch 12/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.6145\n",
            "Epoch 12: val_loss improved from 0.66151 to 0.66015, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6602 - val_accuracy: 0.6407 - lr: 1.0000e-06\n",
            "Epoch 13/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6569 - accuracy: 0.6350\n",
            "Epoch 13: val_loss improved from 0.66015 to 0.65873, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6571 - accuracy: 0.6353 - val_loss: 0.6587 - val_accuracy: 0.6581 - lr: 1.0000e-06\n",
            "Epoch 14/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6568 - accuracy: 0.6387\n",
            "Epoch 14: val_loss improved from 0.65873 to 0.65728, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6567 - accuracy: 0.6395 - val_loss: 0.6573 - val_accuracy: 0.6692 - lr: 1.0000e-06\n",
            "Epoch 15/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6538 - accuracy: 0.6498\n",
            "Epoch 15: val_loss improved from 0.65728 to 0.65562, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6538 - accuracy: 0.6504 - val_loss: 0.6556 - val_accuracy: 0.6814 - lr: 1.0000e-06\n",
            "Epoch 16/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6533 - accuracy: 0.6570\n",
            "Epoch 16: val_loss improved from 0.65562 to 0.65391, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6532 - accuracy: 0.6570 - val_loss: 0.6539 - val_accuracy: 0.6872 - lr: 1.0000e-06\n",
            "Epoch 17/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6504 - accuracy: 0.6691\n",
            "Epoch 17: val_loss improved from 0.65391 to 0.65214, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6499 - accuracy: 0.6695 - val_loss: 0.6521 - val_accuracy: 0.6895 - lr: 1.0000e-06\n",
            "Epoch 18/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6482 - accuracy: 0.6746\n",
            "Epoch 18: val_loss improved from 0.65214 to 0.65044, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.6482 - accuracy: 0.6743 - val_loss: 0.6504 - val_accuracy: 0.6901 - lr: 1.0000e-06\n",
            "Epoch 19/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6467 - accuracy: 0.6789\n",
            "Epoch 19: val_loss improved from 0.65044 to 0.64863, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6467 - accuracy: 0.6786 - val_loss: 0.6486 - val_accuracy: 0.6907 - lr: 1.0000e-06\n",
            "Epoch 20/500\n",
            "208/215 [============================>.] - ETA: 0s - loss: 0.6450 - accuracy: 0.6797\n",
            "Epoch 20: val_loss improved from 0.64863 to 0.64687, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6444 - accuracy: 0.6795 - val_loss: 0.6469 - val_accuracy: 0.6942 - lr: 1.0000e-06\n",
            "Epoch 21/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.6831\n",
            "Epoch 21: val_loss improved from 0.64687 to 0.64518, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.6435 - accuracy: 0.6834 - val_loss: 0.6452 - val_accuracy: 0.6930 - lr: 1.0000e-06\n",
            "Epoch 22/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6410 - accuracy: 0.6814\n",
            "Epoch 22: val_loss improved from 0.64518 to 0.64348, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6410 - accuracy: 0.6810 - val_loss: 0.6435 - val_accuracy: 0.6924 - lr: 1.0000e-06\n",
            "Epoch 23/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6382 - accuracy: 0.6874\n",
            "Epoch 23: val_loss improved from 0.64348 to 0.64180, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6382 - accuracy: 0.6871 - val_loss: 0.6418 - val_accuracy: 0.6866 - lr: 1.0000e-06\n",
            "Epoch 24/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.6794\n",
            "Epoch 24: val_loss improved from 0.64180 to 0.64030, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6364 - accuracy: 0.6810 - val_loss: 0.6403 - val_accuracy: 0.6843 - lr: 1.0000e-06\n",
            "Epoch 25/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6352 - accuracy: 0.6783\n",
            "Epoch 25: val_loss improved from 0.64030 to 0.63882, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.6354 - accuracy: 0.6781 - val_loss: 0.6388 - val_accuracy: 0.6860 - lr: 1.0000e-06\n",
            "Epoch 26/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6329 - accuracy: 0.6859\n",
            "Epoch 26: val_loss improved from 0.63882 to 0.63737, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.6332 - accuracy: 0.6846 - val_loss: 0.6374 - val_accuracy: 0.6837 - lr: 1.0000e-06\n",
            "Epoch 27/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6314 - accuracy: 0.6798\n",
            "Epoch 27: val_loss improved from 0.63737 to 0.63598, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6315 - accuracy: 0.6797 - val_loss: 0.6360 - val_accuracy: 0.6820 - lr: 1.0000e-06\n",
            "Epoch 28/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.6817\n",
            "Epoch 28: val_loss improved from 0.63598 to 0.63460, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6302 - accuracy: 0.6817 - val_loss: 0.6346 - val_accuracy: 0.6797 - lr: 1.0000e-06\n",
            "Epoch 29/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6296 - accuracy: 0.6770\n",
            "Epoch 29: val_loss improved from 0.63460 to 0.63322, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6295 - accuracy: 0.6773 - val_loss: 0.6332 - val_accuracy: 0.6814 - lr: 1.0000e-06\n",
            "Epoch 30/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6261 - accuracy: 0.6811\n",
            "Epoch 30: val_loss improved from 0.63322 to 0.63191, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.6267 - accuracy: 0.6805 - val_loss: 0.6319 - val_accuracy: 0.6814 - lr: 1.0000e-06\n",
            "Epoch 31/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6256 - accuracy: 0.6812\n",
            "Epoch 31: val_loss improved from 0.63191 to 0.63066, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.6256 - accuracy: 0.6814 - val_loss: 0.6307 - val_accuracy: 0.6797 - lr: 1.0000e-06\n",
            "Epoch 32/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.6767\n",
            "Epoch 32: val_loss improved from 0.63066 to 0.62948, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.6250 - accuracy: 0.6769 - val_loss: 0.6295 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 33/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6217 - accuracy: 0.6813\n",
            "Epoch 33: val_loss improved from 0.62948 to 0.62828, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6214 - accuracy: 0.6814 - val_loss: 0.6283 - val_accuracy: 0.6791 - lr: 1.0000e-06\n",
            "Epoch 34/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6204 - accuracy: 0.6795\n",
            "Epoch 34: val_loss improved from 0.62828 to 0.62715, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6211 - accuracy: 0.6791 - val_loss: 0.6272 - val_accuracy: 0.6797 - lr: 1.0000e-06\n",
            "Epoch 35/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6203 - accuracy: 0.6809\n",
            "Epoch 35: val_loss improved from 0.62715 to 0.62602, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.6196 - accuracy: 0.6820 - val_loss: 0.6260 - val_accuracy: 0.6808 - lr: 1.0000e-06\n",
            "Epoch 36/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.6182 - accuracy: 0.6794\n",
            "Epoch 36: val_loss improved from 0.62602 to 0.62488, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6179 - accuracy: 0.6792 - val_loss: 0.6249 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 37/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.6865\n",
            "Epoch 37: val_loss improved from 0.62488 to 0.62378, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6159 - accuracy: 0.6849 - val_loss: 0.6238 - val_accuracy: 0.6820 - lr: 1.0000e-06\n",
            "Epoch 38/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.6151 - accuracy: 0.6819\n",
            "Epoch 38: val_loss improved from 0.62378 to 0.62277, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6149 - accuracy: 0.6821 - val_loss: 0.6228 - val_accuracy: 0.6820 - lr: 1.0000e-06\n",
            "Epoch 39/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.6128 - accuracy: 0.6854\n",
            "Epoch 39: val_loss improved from 0.62277 to 0.62170, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6128 - accuracy: 0.6849 - val_loss: 0.6217 - val_accuracy: 0.6797 - lr: 1.0000e-06\n",
            "Epoch 40/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.6859\n",
            "Epoch 40: val_loss improved from 0.62170 to 0.62063, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6117 - accuracy: 0.6853 - val_loss: 0.6206 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 41/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.6844\n",
            "Epoch 41: val_loss improved from 0.62063 to 0.61959, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6098 - accuracy: 0.6839 - val_loss: 0.6196 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 42/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6076 - accuracy: 0.6838\n",
            "Epoch 42: val_loss improved from 0.61959 to 0.61853, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6069 - accuracy: 0.6850 - val_loss: 0.6185 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 43/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6072 - accuracy: 0.6887\n",
            "Epoch 43: val_loss improved from 0.61853 to 0.61750, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6081 - accuracy: 0.6874 - val_loss: 0.6175 - val_accuracy: 0.6802 - lr: 1.0000e-06\n",
            "Epoch 44/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.6853\n",
            "Epoch 44: val_loss improved from 0.61750 to 0.61651, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6085 - accuracy: 0.6853 - val_loss: 0.6165 - val_accuracy: 0.6808 - lr: 1.0000e-06\n",
            "Epoch 45/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.6043 - accuracy: 0.6888\n",
            "Epoch 45: val_loss improved from 0.61651 to 0.61550, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6055 - accuracy: 0.6876 - val_loss: 0.6155 - val_accuracy: 0.6814 - lr: 1.0000e-06\n",
            "Epoch 46/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.6876\n",
            "Epoch 46: val_loss improved from 0.61550 to 0.61445, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6038 - accuracy: 0.6866 - val_loss: 0.6145 - val_accuracy: 0.6826 - lr: 1.0000e-06\n",
            "Epoch 47/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.6860\n",
            "Epoch 47: val_loss improved from 0.61445 to 0.61347, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6040 - accuracy: 0.6858 - val_loss: 0.6135 - val_accuracy: 0.6837 - lr: 1.0000e-06\n",
            "Epoch 48/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.6901\n",
            "Epoch 48: val_loss improved from 0.61347 to 0.61244, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.6022 - accuracy: 0.6901 - val_loss: 0.6124 - val_accuracy: 0.6826 - lr: 1.0000e-06\n",
            "Epoch 49/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.6863\n",
            "Epoch 49: val_loss improved from 0.61244 to 0.61147, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.6011 - accuracy: 0.6863 - val_loss: 0.6115 - val_accuracy: 0.6831 - lr: 1.0000e-06\n",
            "Epoch 50/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.6832\n",
            "Epoch 50: val_loss improved from 0.61147 to 0.61050, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.5990 - accuracy: 0.6842 - val_loss: 0.6105 - val_accuracy: 0.6837 - lr: 1.0000e-06\n",
            "Epoch 51/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5982 - accuracy: 0.6910\n",
            "Epoch 51: val_loss improved from 0.61050 to 0.60962, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5985 - accuracy: 0.6903 - val_loss: 0.6096 - val_accuracy: 0.6860 - lr: 1.0000e-06\n",
            "Epoch 52/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5963 - accuracy: 0.6869\n",
            "Epoch 52: val_loss improved from 0.60962 to 0.60865, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5959 - accuracy: 0.6882 - val_loss: 0.6086 - val_accuracy: 0.6855 - lr: 1.0000e-06\n",
            "Epoch 53/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5958 - accuracy: 0.6876\n",
            "Epoch 53: val_loss improved from 0.60865 to 0.60765, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5958 - accuracy: 0.6876 - val_loss: 0.6076 - val_accuracy: 0.6872 - lr: 1.0000e-06\n",
            "Epoch 54/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5950 - accuracy: 0.6888\n",
            "Epoch 54: val_loss improved from 0.60765 to 0.60676, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5951 - accuracy: 0.6876 - val_loss: 0.6068 - val_accuracy: 0.6890 - lr: 1.0000e-06\n",
            "Epoch 55/500\n",
            "208/215 [============================>.] - ETA: 0s - loss: 0.5930 - accuracy: 0.6873\n",
            "Epoch 55: val_loss improved from 0.60676 to 0.60582, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5931 - accuracy: 0.6879 - val_loss: 0.6058 - val_accuracy: 0.6890 - lr: 1.0000e-06\n",
            "Epoch 56/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5929 - accuracy: 0.6897\n",
            "Epoch 56: val_loss improved from 0.60582 to 0.60490, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.5929 - accuracy: 0.6897 - val_loss: 0.6049 - val_accuracy: 0.6895 - lr: 1.0000e-06\n",
            "Epoch 57/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.6897\n",
            "Epoch 57: val_loss improved from 0.60490 to 0.60401, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5921 - accuracy: 0.6885 - val_loss: 0.6040 - val_accuracy: 0.6890 - lr: 1.0000e-06\n",
            "Epoch 58/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.6891\n",
            "Epoch 58: val_loss improved from 0.60401 to 0.60301, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5895 - accuracy: 0.6891 - val_loss: 0.6030 - val_accuracy: 0.6901 - lr: 1.0000e-06\n",
            "Epoch 59/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.6903\n",
            "Epoch 59: val_loss improved from 0.60301 to 0.60213, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5897 - accuracy: 0.6903 - val_loss: 0.6021 - val_accuracy: 0.6913 - lr: 1.0000e-06\n",
            "Epoch 60/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5883 - accuracy: 0.6945\n",
            "Epoch 60: val_loss improved from 0.60213 to 0.60126, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.5883 - accuracy: 0.6946 - val_loss: 0.6013 - val_accuracy: 0.6919 - lr: 1.0000e-06\n",
            "Epoch 61/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5873 - accuracy: 0.6926\n",
            "Epoch 61: val_loss improved from 0.60126 to 0.60035, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5871 - accuracy: 0.6929 - val_loss: 0.6003 - val_accuracy: 0.6913 - lr: 1.0000e-06\n",
            "Epoch 62/500\n",
            "208/215 [============================>.] - ETA: 0s - loss: 0.5844 - accuracy: 0.6917\n",
            "Epoch 62: val_loss improved from 0.60035 to 0.59945, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5849 - accuracy: 0.6923 - val_loss: 0.5994 - val_accuracy: 0.6919 - lr: 1.0000e-06\n",
            "Epoch 63/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5846 - accuracy: 0.6937\n",
            "Epoch 63: val_loss improved from 0.59945 to 0.59848, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5846 - accuracy: 0.6942 - val_loss: 0.5985 - val_accuracy: 0.6930 - lr: 1.0000e-06\n",
            "Epoch 64/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5807 - accuracy: 0.6996\n",
            "Epoch 64: val_loss improved from 0.59848 to 0.59751, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5813 - accuracy: 0.6988 - val_loss: 0.5975 - val_accuracy: 0.6942 - lr: 1.0000e-06\n",
            "Epoch 65/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5806 - accuracy: 0.7019\n",
            "Epoch 65: val_loss improved from 0.59751 to 0.59654, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5812 - accuracy: 0.7016 - val_loss: 0.5965 - val_accuracy: 0.6942 - lr: 1.0000e-06\n",
            "Epoch 66/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5799 - accuracy: 0.6985\n",
            "Epoch 66: val_loss improved from 0.59654 to 0.59557, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5801 - accuracy: 0.6981 - val_loss: 0.5956 - val_accuracy: 0.6948 - lr: 1.0000e-06\n",
            "Epoch 67/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.7017\n",
            "Epoch 67: val_loss improved from 0.59557 to 0.59466, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.5780 - accuracy: 0.7017 - val_loss: 0.5947 - val_accuracy: 0.6965 - lr: 1.0000e-06\n",
            "Epoch 68/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5766 - accuracy: 0.7010\n",
            "Epoch 68: val_loss improved from 0.59466 to 0.59366, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5770 - accuracy: 0.7006 - val_loss: 0.5937 - val_accuracy: 0.6971 - lr: 1.0000e-06\n",
            "Epoch 69/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5745 - accuracy: 0.7041\n",
            "Epoch 69: val_loss improved from 0.59366 to 0.59273, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5756 - accuracy: 0.7028 - val_loss: 0.5927 - val_accuracy: 0.6983 - lr: 1.0000e-06\n",
            "Epoch 70/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5756 - accuracy: 0.7051\n",
            "Epoch 70: val_loss improved from 0.59273 to 0.59179, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5757 - accuracy: 0.7051 - val_loss: 0.5918 - val_accuracy: 0.7000 - lr: 1.0000e-06\n",
            "Epoch 71/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5719 - accuracy: 0.7022\n",
            "Epoch 71: val_loss improved from 0.59179 to 0.59084, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5722 - accuracy: 0.7016 - val_loss: 0.5908 - val_accuracy: 0.7012 - lr: 1.0000e-06\n",
            "Epoch 72/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5726 - accuracy: 0.7061\n",
            "Epoch 72: val_loss improved from 0.59084 to 0.58998, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5716 - accuracy: 0.7076 - val_loss: 0.5900 - val_accuracy: 0.7023 - lr: 1.0000e-06\n",
            "Epoch 73/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5733 - accuracy: 0.7015\n",
            "Epoch 73: val_loss improved from 0.58998 to 0.58907, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5724 - accuracy: 0.7022 - val_loss: 0.5891 - val_accuracy: 0.7017 - lr: 1.0000e-06\n",
            "Epoch 74/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5703 - accuracy: 0.7044\n",
            "Epoch 74: val_loss improved from 0.58907 to 0.58816, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5705 - accuracy: 0.7044 - val_loss: 0.5882 - val_accuracy: 0.7023 - lr: 1.0000e-06\n",
            "Epoch 75/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5685 - accuracy: 0.7062\n",
            "Epoch 75: val_loss improved from 0.58816 to 0.58719, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.5677 - accuracy: 0.7073 - val_loss: 0.5872 - val_accuracy: 0.7041 - lr: 1.0000e-06\n",
            "Epoch 76/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7065\n",
            "Epoch 76: val_loss improved from 0.58719 to 0.58618, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.5681 - accuracy: 0.7064 - val_loss: 0.5862 - val_accuracy: 0.7058 - lr: 1.0000e-06\n",
            "Epoch 77/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5672 - accuracy: 0.7067\n",
            "Epoch 77: val_loss improved from 0.58618 to 0.58522, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.5671 - accuracy: 0.7068 - val_loss: 0.5852 - val_accuracy: 0.7070 - lr: 1.0000e-06\n",
            "Epoch 78/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7131\n",
            "Epoch 78: val_loss improved from 0.58522 to 0.58433, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5634 - accuracy: 0.7126 - val_loss: 0.5843 - val_accuracy: 0.7052 - lr: 1.0000e-06\n",
            "Epoch 79/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5642 - accuracy: 0.7100\n",
            "Epoch 79: val_loss improved from 0.58433 to 0.58332, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5641 - accuracy: 0.7103 - val_loss: 0.5833 - val_accuracy: 0.7064 - lr: 1.0000e-06\n",
            "Epoch 80/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5612 - accuracy: 0.7110\n",
            "Epoch 80: val_loss improved from 0.58332 to 0.58234, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5608 - accuracy: 0.7113 - val_loss: 0.5823 - val_accuracy: 0.7052 - lr: 1.0000e-06\n",
            "Epoch 81/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5591 - accuracy: 0.7152\n",
            "Epoch 81: val_loss improved from 0.58234 to 0.58134, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5598 - accuracy: 0.7144 - val_loss: 0.5813 - val_accuracy: 0.7064 - lr: 1.0000e-06\n",
            "Epoch 82/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7108\n",
            "Epoch 82: val_loss improved from 0.58134 to 0.58029, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.5610 - accuracy: 0.7108 - val_loss: 0.5803 - val_accuracy: 0.7076 - lr: 1.0000e-06\n",
            "Epoch 83/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7183\n",
            "Epoch 83: val_loss improved from 0.58029 to 0.57941, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5563 - accuracy: 0.7183 - val_loss: 0.5794 - val_accuracy: 0.7076 - lr: 1.0000e-06\n",
            "Epoch 84/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5587 - accuracy: 0.7151\n",
            "Epoch 84: val_loss improved from 0.57941 to 0.57833, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5582 - accuracy: 0.7151 - val_loss: 0.5783 - val_accuracy: 0.7093 - lr: 1.0000e-06\n",
            "Epoch 85/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5567 - accuracy: 0.7211\n",
            "Epoch 85: val_loss improved from 0.57833 to 0.57739, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.5565 - accuracy: 0.7225 - val_loss: 0.5774 - val_accuracy: 0.7093 - lr: 1.0000e-06\n",
            "Epoch 86/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5543 - accuracy: 0.7192\n",
            "Epoch 86: val_loss improved from 0.57739 to 0.57648, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5541 - accuracy: 0.7195 - val_loss: 0.5765 - val_accuracy: 0.7093 - lr: 1.0000e-06\n",
            "Epoch 87/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5531 - accuracy: 0.7188\n",
            "Epoch 87: val_loss improved from 0.57648 to 0.57558, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5535 - accuracy: 0.7185 - val_loss: 0.5756 - val_accuracy: 0.7093 - lr: 1.0000e-06\n",
            "Epoch 88/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7193\n",
            "Epoch 88: val_loss improved from 0.57558 to 0.57459, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5514 - accuracy: 0.7198 - val_loss: 0.5746 - val_accuracy: 0.7122 - lr: 1.0000e-06\n",
            "Epoch 89/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7224\n",
            "Epoch 89: val_loss improved from 0.57459 to 0.57348, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5508 - accuracy: 0.7221 - val_loss: 0.5735 - val_accuracy: 0.7128 - lr: 1.0000e-06\n",
            "Epoch 90/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.7263\n",
            "Epoch 90: val_loss improved from 0.57348 to 0.57250, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.5486 - accuracy: 0.7263 - val_loss: 0.5725 - val_accuracy: 0.7140 - lr: 1.0000e-06\n",
            "Epoch 91/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5494 - accuracy: 0.7238\n",
            "Epoch 91: val_loss improved from 0.57250 to 0.57166, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5485 - accuracy: 0.7253 - val_loss: 0.5717 - val_accuracy: 0.7128 - lr: 1.0000e-06\n",
            "Epoch 92/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5469 - accuracy: 0.7229\n",
            "Epoch 92: val_loss improved from 0.57166 to 0.57066, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5466 - accuracy: 0.7227 - val_loss: 0.5707 - val_accuracy: 0.7151 - lr: 1.0000e-06\n",
            "Epoch 93/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.7298\n",
            "Epoch 93: val_loss improved from 0.57066 to 0.56976, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5453 - accuracy: 0.7298 - val_loss: 0.5698 - val_accuracy: 0.7157 - lr: 1.0000e-06\n",
            "Epoch 94/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7302\n",
            "Epoch 94: val_loss improved from 0.56976 to 0.56886, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5450 - accuracy: 0.7304 - val_loss: 0.5689 - val_accuracy: 0.7163 - lr: 1.0000e-06\n",
            "Epoch 95/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7277\n",
            "Epoch 95: val_loss improved from 0.56886 to 0.56782, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5428 - accuracy: 0.7273 - val_loss: 0.5678 - val_accuracy: 0.7174 - lr: 1.0000e-06\n",
            "Epoch 96/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5423 - accuracy: 0.7302\n",
            "Epoch 96: val_loss improved from 0.56782 to 0.56677, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.5427 - accuracy: 0.7294 - val_loss: 0.5668 - val_accuracy: 0.7186 - lr: 1.0000e-06\n",
            "Epoch 97/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7325\n",
            "Epoch 97: val_loss improved from 0.56677 to 0.56600, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5405 - accuracy: 0.7317 - val_loss: 0.5660 - val_accuracy: 0.7192 - lr: 1.0000e-06\n",
            "Epoch 98/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.7362\n",
            "Epoch 98: val_loss improved from 0.56600 to 0.56518, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5401 - accuracy: 0.7362 - val_loss: 0.5652 - val_accuracy: 0.7192 - lr: 1.0000e-06\n",
            "Epoch 99/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7321\n",
            "Epoch 99: val_loss improved from 0.56518 to 0.56422, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5381 - accuracy: 0.7317 - val_loss: 0.5642 - val_accuracy: 0.7198 - lr: 1.0000e-06\n",
            "Epoch 100/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.7388\n",
            "Epoch 100: val_loss improved from 0.56422 to 0.56333, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5367 - accuracy: 0.7390 - val_loss: 0.5633 - val_accuracy: 0.7198 - lr: 1.0000e-06\n",
            "Epoch 101/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5371 - accuracy: 0.7380\n",
            "Epoch 101: val_loss improved from 0.56333 to 0.56241, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.5378 - accuracy: 0.7369 - val_loss: 0.5624 - val_accuracy: 0.7209 - lr: 1.0000e-06\n",
            "Epoch 102/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5343 - accuracy: 0.7403\n",
            "Epoch 102: val_loss improved from 0.56241 to 0.56146, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.5335 - accuracy: 0.7406 - val_loss: 0.5615 - val_accuracy: 0.7186 - lr: 1.0000e-06\n",
            "Epoch 103/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5345 - accuracy: 0.7384\n",
            "Epoch 103: val_loss improved from 0.56146 to 0.56066, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5347 - accuracy: 0.7379 - val_loss: 0.5607 - val_accuracy: 0.7203 - lr: 1.0000e-06\n",
            "Epoch 104/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5325 - accuracy: 0.7403\n",
            "Epoch 104: val_loss improved from 0.56066 to 0.55967, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5327 - accuracy: 0.7406 - val_loss: 0.5597 - val_accuracy: 0.7215 - lr: 1.0000e-06\n",
            "Epoch 105/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5331 - accuracy: 0.7335\n",
            "Epoch 105: val_loss improved from 0.55967 to 0.55866, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.5326 - accuracy: 0.7340 - val_loss: 0.5587 - val_accuracy: 0.7209 - lr: 1.0000e-06\n",
            "Epoch 106/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5311 - accuracy: 0.7424\n",
            "Epoch 106: val_loss improved from 0.55866 to 0.55783, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5311 - accuracy: 0.7424 - val_loss: 0.5578 - val_accuracy: 0.7215 - lr: 1.0000e-06\n",
            "Epoch 107/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.7420\n",
            "Epoch 107: val_loss improved from 0.55783 to 0.55699, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5275 - accuracy: 0.7426 - val_loss: 0.5570 - val_accuracy: 0.7215 - lr: 1.0000e-06\n",
            "Epoch 108/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5287 - accuracy: 0.7456\n",
            "Epoch 108: val_loss improved from 0.55699 to 0.55601, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.5287 - accuracy: 0.7456 - val_loss: 0.5560 - val_accuracy: 0.7221 - lr: 1.0000e-06\n",
            "Epoch 109/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7433\n",
            "Epoch 109: val_loss improved from 0.55601 to 0.55522, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5258 - accuracy: 0.7424 - val_loss: 0.5552 - val_accuracy: 0.7233 - lr: 1.0000e-06\n",
            "Epoch 110/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5259 - accuracy: 0.7456\n",
            "Epoch 110: val_loss improved from 0.55522 to 0.55436, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5255 - accuracy: 0.7459 - val_loss: 0.5544 - val_accuracy: 0.7221 - lr: 1.0000e-06\n",
            "Epoch 111/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5237 - accuracy: 0.7484\n",
            "Epoch 111: val_loss improved from 0.55436 to 0.55346, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5237 - accuracy: 0.7485 - val_loss: 0.5535 - val_accuracy: 0.7233 - lr: 1.0000e-06\n",
            "Epoch 112/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.7469\n",
            "Epoch 112: val_loss improved from 0.55346 to 0.55272, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5234 - accuracy: 0.7469 - val_loss: 0.5527 - val_accuracy: 0.7233 - lr: 1.0000e-06\n",
            "Epoch 113/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5229 - accuracy: 0.7460\n",
            "Epoch 113: val_loss improved from 0.55272 to 0.55177, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5226 - accuracy: 0.7462 - val_loss: 0.5518 - val_accuracy: 0.7244 - lr: 1.0000e-06\n",
            "Epoch 114/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5214 - accuracy: 0.7506\n",
            "Epoch 114: val_loss improved from 0.55177 to 0.55117, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5214 - accuracy: 0.7506 - val_loss: 0.5512 - val_accuracy: 0.7238 - lr: 1.0000e-06\n",
            "Epoch 115/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5185 - accuracy: 0.7564\n",
            "Epoch 115: val_loss improved from 0.55117 to 0.55026, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5187 - accuracy: 0.7560 - val_loss: 0.5503 - val_accuracy: 0.7273 - lr: 1.0000e-06\n",
            "Epoch 116/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5180 - accuracy: 0.7533\n",
            "Epoch 116: val_loss improved from 0.55026 to 0.54941, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5183 - accuracy: 0.7526 - val_loss: 0.5494 - val_accuracy: 0.7285 - lr: 1.0000e-06\n",
            "Epoch 117/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5195 - accuracy: 0.7475\n",
            "Epoch 117: val_loss improved from 0.54941 to 0.54847, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.5193 - accuracy: 0.7478 - val_loss: 0.5485 - val_accuracy: 0.7291 - lr: 1.0000e-06\n",
            "Epoch 118/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5173 - accuracy: 0.7519\n",
            "Epoch 118: val_loss improved from 0.54847 to 0.54778, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5179 - accuracy: 0.7519 - val_loss: 0.5478 - val_accuracy: 0.7291 - lr: 1.0000e-06\n",
            "Epoch 119/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5149 - accuracy: 0.7533\n",
            "Epoch 119: val_loss improved from 0.54778 to 0.54702, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5150 - accuracy: 0.7522 - val_loss: 0.5470 - val_accuracy: 0.7314 - lr: 1.0000e-06\n",
            "Epoch 120/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5151 - accuracy: 0.7561\n",
            "Epoch 120: val_loss improved from 0.54702 to 0.54625, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.5150 - accuracy: 0.7552 - val_loss: 0.5462 - val_accuracy: 0.7320 - lr: 1.0000e-06\n",
            "Epoch 121/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7545\n",
            "Epoch 121: val_loss improved from 0.54625 to 0.54546, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5140 - accuracy: 0.7563 - val_loss: 0.5455 - val_accuracy: 0.7314 - lr: 1.0000e-06\n",
            "Epoch 122/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5134 - accuracy: 0.7547\n",
            "Epoch 122: val_loss improved from 0.54546 to 0.54472, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5133 - accuracy: 0.7549 - val_loss: 0.5447 - val_accuracy: 0.7326 - lr: 1.0000e-06\n",
            "Epoch 123/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7584\n",
            "Epoch 123: val_loss improved from 0.54472 to 0.54389, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.5122 - accuracy: 0.7577 - val_loss: 0.5439 - val_accuracy: 0.7331 - lr: 1.0000e-06\n",
            "Epoch 124/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7569\n",
            "Epoch 124: val_loss improved from 0.54389 to 0.54320, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5113 - accuracy: 0.7567 - val_loss: 0.5432 - val_accuracy: 0.7331 - lr: 1.0000e-06\n",
            "Epoch 125/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5109 - accuracy: 0.7534\n",
            "Epoch 125: val_loss improved from 0.54320 to 0.54254, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5110 - accuracy: 0.7532 - val_loss: 0.5425 - val_accuracy: 0.7343 - lr: 1.0000e-06\n",
            "Epoch 126/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5117 - accuracy: 0.7588\n",
            "Epoch 126: val_loss improved from 0.54254 to 0.54147, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5100 - accuracy: 0.7602 - val_loss: 0.5415 - val_accuracy: 0.7337 - lr: 1.0000e-06\n",
            "Epoch 127/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.7581\n",
            "Epoch 127: val_loss improved from 0.54147 to 0.54090, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5066 - accuracy: 0.7581 - val_loss: 0.5409 - val_accuracy: 0.7349 - lr: 1.0000e-06\n",
            "Epoch 128/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7669\n",
            "Epoch 128: val_loss improved from 0.54090 to 0.54016, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5042 - accuracy: 0.7664 - val_loss: 0.5402 - val_accuracy: 0.7355 - lr: 1.0000e-06\n",
            "Epoch 129/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5056 - accuracy: 0.7649\n",
            "Epoch 129: val_loss improved from 0.54016 to 0.53952, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5049 - accuracy: 0.7644 - val_loss: 0.5395 - val_accuracy: 0.7366 - lr: 1.0000e-06\n",
            "Epoch 130/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5081 - accuracy: 0.7614\n",
            "Epoch 130: val_loss improved from 0.53952 to 0.53874, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.5079 - accuracy: 0.7613 - val_loss: 0.5387 - val_accuracy: 0.7372 - lr: 1.0000e-06\n",
            "Epoch 131/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7645\n",
            "Epoch 131: val_loss improved from 0.53874 to 0.53814, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5033 - accuracy: 0.7660 - val_loss: 0.5381 - val_accuracy: 0.7384 - lr: 1.0000e-06\n",
            "Epoch 132/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.5018 - accuracy: 0.7641\n",
            "Epoch 132: val_loss improved from 0.53814 to 0.53746, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5012 - accuracy: 0.7641 - val_loss: 0.5375 - val_accuracy: 0.7384 - lr: 1.0000e-06\n",
            "Epoch 133/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.7621\n",
            "Epoch 133: val_loss improved from 0.53746 to 0.53672, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.5035 - accuracy: 0.7621 - val_loss: 0.5367 - val_accuracy: 0.7384 - lr: 1.0000e-06\n",
            "Epoch 134/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5019 - accuracy: 0.7673\n",
            "Epoch 134: val_loss improved from 0.53672 to 0.53595, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.5030 - accuracy: 0.7658 - val_loss: 0.5359 - val_accuracy: 0.7390 - lr: 1.0000e-06\n",
            "Epoch 135/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4998 - accuracy: 0.7647\n",
            "Epoch 135: val_loss improved from 0.53595 to 0.53520, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4994 - accuracy: 0.7645 - val_loss: 0.5352 - val_accuracy: 0.7401 - lr: 1.0000e-06\n",
            "Epoch 136/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.7667\n",
            "Epoch 136: val_loss improved from 0.53520 to 0.53451, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.5013 - accuracy: 0.7664 - val_loss: 0.5345 - val_accuracy: 0.7419 - lr: 1.0000e-06\n",
            "Epoch 137/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.5004 - accuracy: 0.7622\n",
            "Epoch 137: val_loss improved from 0.53451 to 0.53394, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.5001 - accuracy: 0.7625 - val_loss: 0.5339 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 138/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.7673\n",
            "Epoch 138: val_loss improved from 0.53394 to 0.53333, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4979 - accuracy: 0.7673 - val_loss: 0.5333 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 139/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4984 - accuracy: 0.7696\n",
            "Epoch 139: val_loss improved from 0.53333 to 0.53277, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4978 - accuracy: 0.7703 - val_loss: 0.5328 - val_accuracy: 0.7442 - lr: 1.0000e-06\n",
            "Epoch 140/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4961 - accuracy: 0.7687\n",
            "Epoch 140: val_loss improved from 0.53277 to 0.53182, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4963 - accuracy: 0.7680 - val_loss: 0.5318 - val_accuracy: 0.7448 - lr: 1.0000e-06\n",
            "Epoch 141/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4980 - accuracy: 0.7680\n",
            "Epoch 141: val_loss improved from 0.53182 to 0.53128, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4975 - accuracy: 0.7683 - val_loss: 0.5313 - val_accuracy: 0.7448 - lr: 1.0000e-06\n",
            "Epoch 142/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4953 - accuracy: 0.7690\n",
            "Epoch 142: val_loss improved from 0.53128 to 0.53057, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4951 - accuracy: 0.7680 - val_loss: 0.5306 - val_accuracy: 0.7442 - lr: 1.0000e-06\n",
            "Epoch 143/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4943 - accuracy: 0.7715\n",
            "Epoch 143: val_loss improved from 0.53057 to 0.53010, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4942 - accuracy: 0.7718 - val_loss: 0.5301 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 144/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.7703\n",
            "Epoch 144: val_loss improved from 0.53010 to 0.52936, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4921 - accuracy: 0.7703 - val_loss: 0.5294 - val_accuracy: 0.7430 - lr: 1.0000e-06\n",
            "Epoch 145/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7720\n",
            "Epoch 145: val_loss improved from 0.52936 to 0.52868, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4919 - accuracy: 0.7703 - val_loss: 0.5287 - val_accuracy: 0.7424 - lr: 1.0000e-06\n",
            "Epoch 146/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4932 - accuracy: 0.7698\n",
            "Epoch 146: val_loss improved from 0.52868 to 0.52813, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4929 - accuracy: 0.7705 - val_loss: 0.5281 - val_accuracy: 0.7442 - lr: 1.0000e-06\n",
            "Epoch 147/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4923 - accuracy: 0.7693\n",
            "Epoch 147: val_loss improved from 0.52813 to 0.52771, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4923 - accuracy: 0.7693 - val_loss: 0.5277 - val_accuracy: 0.7453 - lr: 1.0000e-06\n",
            "Epoch 148/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7723\n",
            "Epoch 148: val_loss improved from 0.52771 to 0.52695, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4909 - accuracy: 0.7721 - val_loss: 0.5269 - val_accuracy: 0.7453 - lr: 1.0000e-06\n",
            "Epoch 149/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4903 - accuracy: 0.7709\n",
            "Epoch 149: val_loss improved from 0.52695 to 0.52610, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4898 - accuracy: 0.7712 - val_loss: 0.5261 - val_accuracy: 0.7459 - lr: 1.0000e-06\n",
            "Epoch 150/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.7727\n",
            "Epoch 150: val_loss improved from 0.52610 to 0.52566, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4885 - accuracy: 0.7727 - val_loss: 0.5257 - val_accuracy: 0.7465 - lr: 1.0000e-06\n",
            "Epoch 151/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.7741\n",
            "Epoch 151: val_loss improved from 0.52566 to 0.52495, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4866 - accuracy: 0.7728 - val_loss: 0.5249 - val_accuracy: 0.7471 - lr: 1.0000e-06\n",
            "Epoch 152/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.7739\n",
            "Epoch 152: val_loss improved from 0.52495 to 0.52438, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4885 - accuracy: 0.7741 - val_loss: 0.5244 - val_accuracy: 0.7483 - lr: 1.0000e-06\n",
            "Epoch 153/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4869 - accuracy: 0.7737\n",
            "Epoch 153: val_loss improved from 0.52438 to 0.52409, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4870 - accuracy: 0.7737 - val_loss: 0.5241 - val_accuracy: 0.7494 - lr: 1.0000e-06\n",
            "Epoch 154/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.7743\n",
            "Epoch 154: val_loss improved from 0.52409 to 0.52353, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4854 - accuracy: 0.7743 - val_loss: 0.5235 - val_accuracy: 0.7494 - lr: 1.0000e-06\n",
            "Epoch 155/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7728\n",
            "Epoch 155: val_loss improved from 0.52353 to 0.52311, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4858 - accuracy: 0.7734 - val_loss: 0.5231 - val_accuracy: 0.7494 - lr: 1.0000e-06\n",
            "Epoch 156/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7729\n",
            "Epoch 156: val_loss improved from 0.52311 to 0.52253, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4847 - accuracy: 0.7734 - val_loss: 0.5225 - val_accuracy: 0.7494 - lr: 1.0000e-06\n",
            "Epoch 157/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4832 - accuracy: 0.7782\n",
            "Epoch 157: val_loss improved from 0.52253 to 0.52184, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4828 - accuracy: 0.7783 - val_loss: 0.5218 - val_accuracy: 0.7506 - lr: 1.0000e-06\n",
            "Epoch 158/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.7750\n",
            "Epoch 158: val_loss improved from 0.52184 to 0.52118, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4815 - accuracy: 0.7750 - val_loss: 0.5212 - val_accuracy: 0.7535 - lr: 1.0000e-06\n",
            "Epoch 159/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.7758\n",
            "Epoch 159: val_loss improved from 0.52118 to 0.52096, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4809 - accuracy: 0.7756 - val_loss: 0.5210 - val_accuracy: 0.7523 - lr: 1.0000e-06\n",
            "Epoch 160/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4809 - accuracy: 0.7747\n",
            "Epoch 160: val_loss improved from 0.52096 to 0.52054, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4809 - accuracy: 0.7747 - val_loss: 0.5205 - val_accuracy: 0.7529 - lr: 1.0000e-06\n",
            "Epoch 161/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4821 - accuracy: 0.7808\n",
            "Epoch 161: val_loss improved from 0.52054 to 0.51946, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4814 - accuracy: 0.7808 - val_loss: 0.5195 - val_accuracy: 0.7570 - lr: 1.0000e-06\n",
            "Epoch 162/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.7821\n",
            "Epoch 162: val_loss did not improve from 0.51946\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4786 - accuracy: 0.7807 - val_loss: 0.5195 - val_accuracy: 0.7535 - lr: 1.0000e-06\n",
            "Epoch 163/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4781 - accuracy: 0.7778\n",
            "Epoch 163: val_loss improved from 0.51946 to 0.51868, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4784 - accuracy: 0.7778 - val_loss: 0.5187 - val_accuracy: 0.7558 - lr: 1.0000e-06\n",
            "Epoch 164/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4772 - accuracy: 0.7827\n",
            "Epoch 164: val_loss improved from 0.51868 to 0.51816, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4772 - accuracy: 0.7827 - val_loss: 0.5182 - val_accuracy: 0.7558 - lr: 1.0000e-06\n",
            "Epoch 165/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4783 - accuracy: 0.7779\n",
            "Epoch 165: val_loss improved from 0.51816 to 0.51749, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4784 - accuracy: 0.7776 - val_loss: 0.5175 - val_accuracy: 0.7570 - lr: 1.0000e-06\n",
            "Epoch 166/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4759 - accuracy: 0.7796\n",
            "Epoch 166: val_loss improved from 0.51749 to 0.51731, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4756 - accuracy: 0.7795 - val_loss: 0.5173 - val_accuracy: 0.7564 - lr: 1.0000e-06\n",
            "Epoch 167/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4772 - accuracy: 0.7766\n",
            "Epoch 167: val_loss improved from 0.51731 to 0.51670, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4767 - accuracy: 0.7769 - val_loss: 0.5167 - val_accuracy: 0.7570 - lr: 1.0000e-06\n",
            "Epoch 168/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.7856\n",
            "Epoch 168: val_loss improved from 0.51670 to 0.51632, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4758 - accuracy: 0.7856 - val_loss: 0.5163 - val_accuracy: 0.7570 - lr: 1.0000e-06\n",
            "Epoch 169/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.7804\n",
            "Epoch 169: val_loss improved from 0.51632 to 0.51574, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4740 - accuracy: 0.7801 - val_loss: 0.5157 - val_accuracy: 0.7576 - lr: 1.0000e-06\n",
            "Epoch 170/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4706 - accuracy: 0.7868\n",
            "Epoch 170: val_loss improved from 0.51574 to 0.51512, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4711 - accuracy: 0.7862 - val_loss: 0.5151 - val_accuracy: 0.7576 - lr: 1.0000e-06\n",
            "Epoch 171/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4730 - accuracy: 0.7836\n",
            "Epoch 171: val_loss improved from 0.51512 to 0.51467, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4727 - accuracy: 0.7836 - val_loss: 0.5147 - val_accuracy: 0.7587 - lr: 1.0000e-06\n",
            "Epoch 172/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.7796\n",
            "Epoch 172: val_loss improved from 0.51467 to 0.51436, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4729 - accuracy: 0.7795 - val_loss: 0.5144 - val_accuracy: 0.7581 - lr: 1.0000e-06\n",
            "Epoch 173/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4747 - accuracy: 0.7795\n",
            "Epoch 173: val_loss improved from 0.51436 to 0.51356, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4745 - accuracy: 0.7795 - val_loss: 0.5136 - val_accuracy: 0.7610 - lr: 1.0000e-06\n",
            "Epoch 174/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.7797\n",
            "Epoch 174: val_loss improved from 0.51356 to 0.51313, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4715 - accuracy: 0.7797 - val_loss: 0.5131 - val_accuracy: 0.7622 - lr: 1.0000e-06\n",
            "Epoch 175/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.7811\n",
            "Epoch 175: val_loss improved from 0.51313 to 0.51276, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4697 - accuracy: 0.7811 - val_loss: 0.5128 - val_accuracy: 0.7628 - lr: 1.0000e-06\n",
            "Epoch 176/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4715 - accuracy: 0.7811\n",
            "Epoch 176: val_loss improved from 0.51276 to 0.51230, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4711 - accuracy: 0.7815 - val_loss: 0.5123 - val_accuracy: 0.7634 - lr: 1.0000e-06\n",
            "Epoch 177/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4692 - accuracy: 0.7821\n",
            "Epoch 177: val_loss improved from 0.51230 to 0.51212, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4697 - accuracy: 0.7820 - val_loss: 0.5121 - val_accuracy: 0.7616 - lr: 1.0000e-06\n",
            "Epoch 178/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.7812\n",
            "Epoch 178: val_loss improved from 0.51212 to 0.51138, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4705 - accuracy: 0.7812 - val_loss: 0.5114 - val_accuracy: 0.7628 - lr: 1.0000e-06\n",
            "Epoch 179/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4717 - accuracy: 0.7827\n",
            "Epoch 179: val_loss improved from 0.51138 to 0.51078, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4715 - accuracy: 0.7827 - val_loss: 0.5108 - val_accuracy: 0.7640 - lr: 1.0000e-06\n",
            "Epoch 180/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.7824\n",
            "Epoch 180: val_loss improved from 0.51078 to 0.51063, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4707 - accuracy: 0.7843 - val_loss: 0.5106 - val_accuracy: 0.7628 - lr: 1.0000e-06\n",
            "Epoch 181/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.7852\n",
            "Epoch 181: val_loss improved from 0.51063 to 0.51055, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4675 - accuracy: 0.7852 - val_loss: 0.5105 - val_accuracy: 0.7634 - lr: 1.0000e-06\n",
            "Epoch 182/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.7850\n",
            "Epoch 182: val_loss improved from 0.51055 to 0.50993, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4661 - accuracy: 0.7860 - val_loss: 0.5099 - val_accuracy: 0.7651 - lr: 1.0000e-06\n",
            "Epoch 183/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.7847\n",
            "Epoch 183: val_loss improved from 0.50993 to 0.50953, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4689 - accuracy: 0.7859 - val_loss: 0.5095 - val_accuracy: 0.7657 - lr: 1.0000e-06\n",
            "Epoch 184/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4678 - accuracy: 0.7818\n",
            "Epoch 184: val_loss improved from 0.50953 to 0.50938, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4686 - accuracy: 0.7814 - val_loss: 0.5094 - val_accuracy: 0.7651 - lr: 1.0000e-06\n",
            "Epoch 185/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4643 - accuracy: 0.7868\n",
            "Epoch 185: val_loss improved from 0.50938 to 0.50868, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4648 - accuracy: 0.7863 - val_loss: 0.5087 - val_accuracy: 0.7663 - lr: 1.0000e-06\n",
            "Epoch 186/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4631 - accuracy: 0.7878\n",
            "Epoch 186: val_loss improved from 0.50868 to 0.50832, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4644 - accuracy: 0.7874 - val_loss: 0.5083 - val_accuracy: 0.7674 - lr: 1.0000e-06\n",
            "Epoch 187/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.7858\n",
            "Epoch 187: val_loss improved from 0.50832 to 0.50769, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4643 - accuracy: 0.7858 - val_loss: 0.5077 - val_accuracy: 0.7680 - lr: 1.0000e-06\n",
            "Epoch 188/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4649 - accuracy: 0.7861\n",
            "Epoch 188: val_loss improved from 0.50769 to 0.50733, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4647 - accuracy: 0.7865 - val_loss: 0.5073 - val_accuracy: 0.7692 - lr: 1.0000e-06\n",
            "Epoch 189/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4623 - accuracy: 0.7862\n",
            "Epoch 189: val_loss improved from 0.50733 to 0.50711, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4622 - accuracy: 0.7865 - val_loss: 0.5071 - val_accuracy: 0.7692 - lr: 1.0000e-06\n",
            "Epoch 190/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4628 - accuracy: 0.7869\n",
            "Epoch 190: val_loss improved from 0.50711 to 0.50682, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4626 - accuracy: 0.7869 - val_loss: 0.5068 - val_accuracy: 0.7698 - lr: 1.0000e-06\n",
            "Epoch 191/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.7801\n",
            "Epoch 191: val_loss improved from 0.50682 to 0.50668, saving model to model2.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.4627 - accuracy: 0.7811 - val_loss: 0.5067 - val_accuracy: 0.7703 - lr: 1.0000e-06\n",
            "Epoch 192/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.7871\n",
            "Epoch 192: val_loss improved from 0.50668 to 0.50595, saving model to model2.h5\n",
            "215/215 [==============================] - 4s 19ms/step - loss: 0.4598 - accuracy: 0.7874 - val_loss: 0.5060 - val_accuracy: 0.7703 - lr: 1.0000e-06\n",
            "Epoch 193/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4608 - accuracy: 0.7910\n",
            "Epoch 193: val_loss improved from 0.50595 to 0.50584, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4605 - accuracy: 0.7914 - val_loss: 0.5058 - val_accuracy: 0.7698 - lr: 1.0000e-06\n",
            "Epoch 194/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.7865\n",
            "Epoch 194: val_loss improved from 0.50584 to 0.50549, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4614 - accuracy: 0.7875 - val_loss: 0.5055 - val_accuracy: 0.7698 - lr: 1.0000e-06\n",
            "Epoch 195/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.7918\n",
            "Epoch 195: val_loss improved from 0.50549 to 0.50520, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4588 - accuracy: 0.7917 - val_loss: 0.5052 - val_accuracy: 0.7698 - lr: 1.0000e-06\n",
            "Epoch 196/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7879\n",
            "Epoch 196: val_loss improved from 0.50520 to 0.50486, saving model to model2.h5\n",
            "215/215 [==============================] - 4s 18ms/step - loss: 0.4589 - accuracy: 0.7875 - val_loss: 0.5049 - val_accuracy: 0.7703 - lr: 1.0000e-06\n",
            "Epoch 197/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.7891\n",
            "Epoch 197: val_loss improved from 0.50486 to 0.50411, saving model to model2.h5\n",
            "215/215 [==============================] - 4s 20ms/step - loss: 0.4565 - accuracy: 0.7891 - val_loss: 0.5041 - val_accuracy: 0.7715 - lr: 1.0000e-06\n",
            "Epoch 198/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.7925\n",
            "Epoch 198: val_loss improved from 0.50411 to 0.50364, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4560 - accuracy: 0.7927 - val_loss: 0.5036 - val_accuracy: 0.7727 - lr: 1.0000e-06\n",
            "Epoch 199/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4571 - accuracy: 0.7909\n",
            "Epoch 199: val_loss improved from 0.50364 to 0.50282, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4566 - accuracy: 0.7911 - val_loss: 0.5028 - val_accuracy: 0.7733 - lr: 1.0000e-06\n",
            "Epoch 200/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.7887\n",
            "Epoch 200: val_loss did not improve from 0.50282\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4566 - accuracy: 0.7887 - val_loss: 0.5029 - val_accuracy: 0.7727 - lr: 1.0000e-06\n",
            "Epoch 201/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4560 - accuracy: 0.7878\n",
            "Epoch 201: val_loss improved from 0.50282 to 0.50270, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4565 - accuracy: 0.7875 - val_loss: 0.5027 - val_accuracy: 0.7727 - lr: 1.0000e-06\n",
            "Epoch 202/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4542 - accuracy: 0.7931\n",
            "Epoch 202: val_loss improved from 0.50270 to 0.50201, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4547 - accuracy: 0.7927 - val_loss: 0.5020 - val_accuracy: 0.7721 - lr: 1.0000e-06\n",
            "Epoch 203/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.7874\n",
            "Epoch 203: val_loss improved from 0.50201 to 0.50170, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4549 - accuracy: 0.7875 - val_loss: 0.5017 - val_accuracy: 0.7727 - lr: 1.0000e-06\n",
            "Epoch 204/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4544 - accuracy: 0.7934\n",
            "Epoch 204: val_loss improved from 0.50170 to 0.50139, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4545 - accuracy: 0.7930 - val_loss: 0.5014 - val_accuracy: 0.7733 - lr: 1.0000e-06\n",
            "Epoch 205/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4567 - accuracy: 0.7915\n",
            "Epoch 205: val_loss did not improve from 0.50139\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4567 - accuracy: 0.7917 - val_loss: 0.5014 - val_accuracy: 0.7738 - lr: 1.0000e-06\n",
            "Epoch 206/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.7921\n",
            "Epoch 206: val_loss improved from 0.50139 to 0.50067, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4532 - accuracy: 0.7916 - val_loss: 0.5007 - val_accuracy: 0.7733 - lr: 1.0000e-06\n",
            "Epoch 207/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4512 - accuracy: 0.7934\n",
            "Epoch 207: val_loss improved from 0.50067 to 0.50005, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.4509 - accuracy: 0.7937 - val_loss: 0.5000 - val_accuracy: 0.7756 - lr: 1.0000e-06\n",
            "Epoch 208/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4571 - accuracy: 0.7895\n",
            "Epoch 208: val_loss improved from 0.50005 to 0.49939, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4554 - accuracy: 0.7910 - val_loss: 0.4994 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 209/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4511 - accuracy: 0.7938\n",
            "Epoch 209: val_loss improved from 0.49939 to 0.49901, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4522 - accuracy: 0.7932 - val_loss: 0.4990 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 210/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.7926\n",
            "Epoch 210: val_loss improved from 0.49901 to 0.49882, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4523 - accuracy: 0.7943 - val_loss: 0.4988 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 211/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4506 - accuracy: 0.7939\n",
            "Epoch 211: val_loss did not improve from 0.49882\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4514 - accuracy: 0.7936 - val_loss: 0.4989 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 212/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4517 - accuracy: 0.7968\n",
            "Epoch 212: val_loss improved from 0.49882 to 0.49868, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4517 - accuracy: 0.7969 - val_loss: 0.4987 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 213/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4518 - accuracy: 0.7923\n",
            "Epoch 213: val_loss improved from 0.49868 to 0.49821, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4515 - accuracy: 0.7927 - val_loss: 0.4982 - val_accuracy: 0.7756 - lr: 1.0000e-06\n",
            "Epoch 214/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4496 - accuracy: 0.7950\n",
            "Epoch 214: val_loss improved from 0.49821 to 0.49804, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4500 - accuracy: 0.7945 - val_loss: 0.4980 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 215/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4477 - accuracy: 0.7973\n",
            "Epoch 215: val_loss improved from 0.49804 to 0.49776, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4480 - accuracy: 0.7972 - val_loss: 0.4978 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 216/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.7918\n",
            "Epoch 216: val_loss improved from 0.49776 to 0.49709, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4503 - accuracy: 0.7922 - val_loss: 0.4971 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 217/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.7957\n",
            "Epoch 217: val_loss improved from 0.49709 to 0.49648, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4487 - accuracy: 0.7956 - val_loss: 0.4965 - val_accuracy: 0.7750 - lr: 1.0000e-06\n",
            "Epoch 218/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4485 - accuracy: 0.7932\n",
            "Epoch 218: val_loss did not improve from 0.49648\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4493 - accuracy: 0.7926 - val_loss: 0.4968 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 219/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.7952\n",
            "Epoch 219: val_loss improved from 0.49648 to 0.49632, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4458 - accuracy: 0.7965 - val_loss: 0.4963 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 220/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.7978\n",
            "Epoch 220: val_loss improved from 0.49632 to 0.49627, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4462 - accuracy: 0.7969 - val_loss: 0.4963 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 221/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4454 - accuracy: 0.7971\n",
            "Epoch 221: val_loss improved from 0.49627 to 0.49624, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4452 - accuracy: 0.7974 - val_loss: 0.4962 - val_accuracy: 0.7762 - lr: 1.0000e-06\n",
            "Epoch 222/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4453 - accuracy: 0.7955\n",
            "Epoch 222: val_loss improved from 0.49624 to 0.49562, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4452 - accuracy: 0.7959 - val_loss: 0.4956 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 223/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4446 - accuracy: 0.7996\n",
            "Epoch 223: val_loss improved from 0.49562 to 0.49547, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4456 - accuracy: 0.7984 - val_loss: 0.4955 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 224/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.7999\n",
            "Epoch 224: val_loss improved from 0.49547 to 0.49528, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4446 - accuracy: 0.7990 - val_loss: 0.4953 - val_accuracy: 0.7767 - lr: 1.0000e-06\n",
            "Epoch 225/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4458 - accuracy: 0.7986\n",
            "Epoch 225: val_loss improved from 0.49528 to 0.49438, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4452 - accuracy: 0.7991 - val_loss: 0.4944 - val_accuracy: 0.7779 - lr: 1.0000e-06\n",
            "Epoch 226/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4450 - accuracy: 0.7974\n",
            "Epoch 226: val_loss did not improve from 0.49438\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4449 - accuracy: 0.7977 - val_loss: 0.4944 - val_accuracy: 0.7773 - lr: 1.0000e-06\n",
            "Epoch 227/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4439 - accuracy: 0.8010\n",
            "Epoch 227: val_loss improved from 0.49438 to 0.49424, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4438 - accuracy: 0.8009 - val_loss: 0.4942 - val_accuracy: 0.7785 - lr: 1.0000e-06\n",
            "Epoch 228/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8007\n",
            "Epoch 228: val_loss improved from 0.49424 to 0.49392, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4435 - accuracy: 0.7996 - val_loss: 0.4939 - val_accuracy: 0.7791 - lr: 1.0000e-06\n",
            "Epoch 229/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8019\n",
            "Epoch 229: val_loss improved from 0.49392 to 0.49325, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4411 - accuracy: 0.8020 - val_loss: 0.4933 - val_accuracy: 0.7797 - lr: 1.0000e-06\n",
            "Epoch 230/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4422 - accuracy: 0.7977\n",
            "Epoch 230: val_loss improved from 0.49325 to 0.49310, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4424 - accuracy: 0.7983 - val_loss: 0.4931 - val_accuracy: 0.7802 - lr: 1.0000e-06\n",
            "Epoch 231/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4410 - accuracy: 0.7997\n",
            "Epoch 231: val_loss improved from 0.49310 to 0.49286, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4411 - accuracy: 0.7994 - val_loss: 0.4929 - val_accuracy: 0.7802 - lr: 1.0000e-06\n",
            "Epoch 232/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.8001\n",
            "Epoch 232: val_loss improved from 0.49286 to 0.49221, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4396 - accuracy: 0.8000 - val_loss: 0.4922 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 233/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8001\n",
            "Epoch 233: val_loss improved from 0.49221 to 0.49213, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4408 - accuracy: 0.7997 - val_loss: 0.4921 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 234/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4397 - accuracy: 0.7992\n",
            "Epoch 234: val_loss did not improve from 0.49213\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4404 - accuracy: 0.7988 - val_loss: 0.4924 - val_accuracy: 0.7785 - lr: 1.0000e-06\n",
            "Epoch 235/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4402 - accuracy: 0.7998\n",
            "Epoch 235: val_loss improved from 0.49213 to 0.49167, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4409 - accuracy: 0.7993 - val_loss: 0.4917 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 236/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4366 - accuracy: 0.8014\n",
            "Epoch 236: val_loss did not improve from 0.49167\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4374 - accuracy: 0.8006 - val_loss: 0.4917 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 237/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4419 - accuracy: 0.7976\n",
            "Epoch 237: val_loss improved from 0.49167 to 0.49107, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4421 - accuracy: 0.7977 - val_loss: 0.4911 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 238/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4384 - accuracy: 0.8030\n",
            "Epoch 238: val_loss improved from 0.49107 to 0.49095, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4381 - accuracy: 0.8033 - val_loss: 0.4910 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 239/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4381 - accuracy: 0.8008\n",
            "Epoch 239: val_loss improved from 0.49095 to 0.49045, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4380 - accuracy: 0.8012 - val_loss: 0.4905 - val_accuracy: 0.7831 - lr: 1.0000e-06\n",
            "Epoch 240/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4423 - accuracy: 0.7993\n",
            "Epoch 240: val_loss improved from 0.49045 to 0.49034, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4420 - accuracy: 0.7996 - val_loss: 0.4903 - val_accuracy: 0.7826 - lr: 1.0000e-06\n",
            "Epoch 241/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.8019\n",
            "Epoch 241: val_loss did not improve from 0.49034\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4367 - accuracy: 0.8016 - val_loss: 0.4905 - val_accuracy: 0.7808 - lr: 1.0000e-06\n",
            "Epoch 242/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4348 - accuracy: 0.8039\n",
            "Epoch 242: val_loss improved from 0.49034 to 0.48983, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4358 - accuracy: 0.8036 - val_loss: 0.4898 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 243/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.8041\n",
            "Epoch 243: val_loss improved from 0.48983 to 0.48934, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4363 - accuracy: 0.8041 - val_loss: 0.4893 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 244/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4392 - accuracy: 0.8009\n",
            "Epoch 244: val_loss improved from 0.48934 to 0.48918, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4379 - accuracy: 0.8022 - val_loss: 0.4892 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 245/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.8031\n",
            "Epoch 245: val_loss improved from 0.48918 to 0.48867, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4356 - accuracy: 0.8035 - val_loss: 0.4887 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 246/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4352 - accuracy: 0.8020\n",
            "Epoch 246: val_loss improved from 0.48867 to 0.48845, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4354 - accuracy: 0.8022 - val_loss: 0.4885 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 247/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.8053\n",
            "Epoch 247: val_loss improved from 0.48845 to 0.48828, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4345 - accuracy: 0.8047 - val_loss: 0.4883 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 248/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.8014\n",
            "Epoch 248: val_loss improved from 0.48828 to 0.48777, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4340 - accuracy: 0.8017 - val_loss: 0.4878 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 249/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4318 - accuracy: 0.8083\n",
            "Epoch 249: val_loss did not improve from 0.48777\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4324 - accuracy: 0.8076 - val_loss: 0.4879 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 250/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.8041\n",
            "Epoch 250: val_loss did not improve from 0.48777\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4320 - accuracy: 0.8035 - val_loss: 0.4881 - val_accuracy: 0.7820 - lr: 1.0000e-06\n",
            "Epoch 251/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4377 - accuracy: 0.8046\n",
            "Epoch 251: val_loss did not improve from 0.48777\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4376 - accuracy: 0.8048 - val_loss: 0.4880 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 252/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.8026\n",
            "Epoch 252: val_loss improved from 0.48777 to 0.48776, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4343 - accuracy: 0.8025 - val_loss: 0.4878 - val_accuracy: 0.7814 - lr: 1.0000e-06\n",
            "Epoch 253/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4323 - accuracy: 0.8026\n",
            "Epoch 253: val_loss improved from 0.48776 to 0.48747, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4326 - accuracy: 0.8023 - val_loss: 0.4875 - val_accuracy: 0.7843 - lr: 1.0000e-06\n",
            "Epoch 254/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4336 - accuracy: 0.8004\n",
            "Epoch 254: val_loss improved from 0.48747 to 0.48718, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4338 - accuracy: 0.8003 - val_loss: 0.4872 - val_accuracy: 0.7826 - lr: 1.0000e-06\n",
            "Epoch 255/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4312 - accuracy: 0.8039\n",
            "Epoch 255: val_loss improved from 0.48718 to 0.48650, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4312 - accuracy: 0.8042 - val_loss: 0.4865 - val_accuracy: 0.7843 - lr: 1.0000e-06\n",
            "Epoch 256/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.8080\n",
            "Epoch 256: val_loss did not improve from 0.48650\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4280 - accuracy: 0.8077 - val_loss: 0.4866 - val_accuracy: 0.7831 - lr: 1.0000e-06\n",
            "Epoch 257/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4310 - accuracy: 0.8046\n",
            "Epoch 257: val_loss did not improve from 0.48650\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4308 - accuracy: 0.8054 - val_loss: 0.4865 - val_accuracy: 0.7826 - lr: 1.0000e-06\n",
            "Epoch 258/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4311 - accuracy: 0.8069\n",
            "Epoch 258: val_loss improved from 0.48650 to 0.48603, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4315 - accuracy: 0.8065 - val_loss: 0.4860 - val_accuracy: 0.7843 - lr: 1.0000e-06\n",
            "Epoch 259/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.8077\n",
            "Epoch 259: val_loss improved from 0.48603 to 0.48570, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4290 - accuracy: 0.8074 - val_loss: 0.4857 - val_accuracy: 0.7843 - lr: 1.0000e-06\n",
            "Epoch 260/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4302 - accuracy: 0.8059\n",
            "Epoch 260: val_loss improved from 0.48570 to 0.48556, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4297 - accuracy: 0.8062 - val_loss: 0.4856 - val_accuracy: 0.7837 - lr: 1.0000e-06\n",
            "Epoch 261/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8032\n",
            "Epoch 261: val_loss did not improve from 0.48556\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4298 - accuracy: 0.8032 - val_loss: 0.4857 - val_accuracy: 0.7831 - lr: 1.0000e-06\n",
            "Epoch 262/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.8060\n",
            "Epoch 262: val_loss improved from 0.48556 to 0.48490, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4295 - accuracy: 0.8054 - val_loss: 0.4849 - val_accuracy: 0.7855 - lr: 1.0000e-06\n",
            "Epoch 263/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.8048\n",
            "Epoch 263: val_loss improved from 0.48490 to 0.48480, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4284 - accuracy: 0.8048 - val_loss: 0.4848 - val_accuracy: 0.7855 - lr: 1.0000e-06\n",
            "Epoch 264/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4290 - accuracy: 0.8103\n",
            "Epoch 264: val_loss improved from 0.48480 to 0.48457, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4281 - accuracy: 0.8105 - val_loss: 0.4846 - val_accuracy: 0.7866 - lr: 1.0000e-06\n",
            "Epoch 265/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8110\n",
            "Epoch 265: val_loss improved from 0.48457 to 0.48452, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4249 - accuracy: 0.8115 - val_loss: 0.4845 - val_accuracy: 0.7866 - lr: 1.0000e-06\n",
            "Epoch 266/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.8070\n",
            "Epoch 266: val_loss improved from 0.48452 to 0.48386, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4276 - accuracy: 0.8070 - val_loss: 0.4839 - val_accuracy: 0.7878 - lr: 1.0000e-06\n",
            "Epoch 267/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.8058\n",
            "Epoch 267: val_loss improved from 0.48386 to 0.48347, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4259 - accuracy: 0.8065 - val_loss: 0.4835 - val_accuracy: 0.7872 - lr: 1.0000e-06\n",
            "Epoch 268/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8088\n",
            "Epoch 268: val_loss did not improve from 0.48347\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4272 - accuracy: 0.8086 - val_loss: 0.4837 - val_accuracy: 0.7878 - lr: 1.0000e-06\n",
            "Epoch 269/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4281 - accuracy: 0.8065\n",
            "Epoch 269: val_loss improved from 0.48347 to 0.48314, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4286 - accuracy: 0.8062 - val_loss: 0.4831 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 270/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8066\n",
            "Epoch 270: val_loss did not improve from 0.48314\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4240 - accuracy: 0.8074 - val_loss: 0.4833 - val_accuracy: 0.7872 - lr: 1.0000e-06\n",
            "Epoch 271/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4237 - accuracy: 0.8083\n",
            "Epoch 271: val_loss improved from 0.48314 to 0.48269, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.4243 - accuracy: 0.8086 - val_loss: 0.4827 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 272/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.8067\n",
            "Epoch 272: val_loss improved from 0.48269 to 0.48223, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4273 - accuracy: 0.8062 - val_loss: 0.4822 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 273/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4272 - accuracy: 0.8087\n",
            "Epoch 273: val_loss improved from 0.48223 to 0.48189, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.4266 - accuracy: 0.8093 - val_loss: 0.4819 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 274/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8100\n",
            "Epoch 274: val_loss did not improve from 0.48189\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4234 - accuracy: 0.8106 - val_loss: 0.4824 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 275/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4263 - accuracy: 0.8074\n",
            "Epoch 275: val_loss did not improve from 0.48189\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4263 - accuracy: 0.8074 - val_loss: 0.4820 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 276/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8073\n",
            "Epoch 276: val_loss did not improve from 0.48189\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4235 - accuracy: 0.8073 - val_loss: 0.4824 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 277/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8138\n",
            "Epoch 277: val_loss did not improve from 0.48189\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4223 - accuracy: 0.8135 - val_loss: 0.4824 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 278/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4248 - accuracy: 0.8094\n",
            "Epoch 278: val_loss improved from 0.48189 to 0.48120, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4244 - accuracy: 0.8094 - val_loss: 0.4812 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 279/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4258 - accuracy: 0.8059\n",
            "Epoch 279: val_loss did not improve from 0.48120\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.4258 - accuracy: 0.8057 - val_loss: 0.4820 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 280/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8112\n",
            "Epoch 280: val_loss did not improve from 0.48120\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4237 - accuracy: 0.8112 - val_loss: 0.4813 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 281/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8129\n",
            "Epoch 281: val_loss improved from 0.48120 to 0.48092, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4217 - accuracy: 0.8126 - val_loss: 0.4809 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 282/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4237 - accuracy: 0.8102\n",
            "Epoch 282: val_loss did not improve from 0.48092\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4230 - accuracy: 0.8108 - val_loss: 0.4809 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 283/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8083\n",
            "Epoch 283: val_loss improved from 0.48092 to 0.48067, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4228 - accuracy: 0.8086 - val_loss: 0.4807 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 284/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8087\n",
            "Epoch 284: val_loss did not improve from 0.48067\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4216 - accuracy: 0.8087 - val_loss: 0.4808 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 285/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8081\n",
            "Epoch 285: val_loss did not improve from 0.48067\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4204 - accuracy: 0.8081 - val_loss: 0.4807 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 286/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8089\n",
            "Epoch 286: val_loss improved from 0.48067 to 0.47984, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4217 - accuracy: 0.8092 - val_loss: 0.4798 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 287/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8067\n",
            "Epoch 287: val_loss did not improve from 0.47984\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4215 - accuracy: 0.8070 - val_loss: 0.4803 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 288/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8125\n",
            "Epoch 288: val_loss did not improve from 0.47984\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4186 - accuracy: 0.8118 - val_loss: 0.4800 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 289/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4169 - accuracy: 0.8126\n",
            "Epoch 289: val_loss did not improve from 0.47984\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4173 - accuracy: 0.8125 - val_loss: 0.4801 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 290/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8171\n",
            "Epoch 290: val_loss improved from 0.47984 to 0.47949, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4179 - accuracy: 0.8169 - val_loss: 0.4795 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 291/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8173\n",
            "Epoch 291: val_loss improved from 0.47949 to 0.47900, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4167 - accuracy: 0.8176 - val_loss: 0.4790 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 292/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8150\n",
            "Epoch 292: val_loss improved from 0.47900 to 0.47854, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4159 - accuracy: 0.8150 - val_loss: 0.4785 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 293/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4192 - accuracy: 0.8112\n",
            "Epoch 293: val_loss did not improve from 0.47854\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4189 - accuracy: 0.8113 - val_loss: 0.4791 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 294/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8134\n",
            "Epoch 294: val_loss did not improve from 0.47854\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4167 - accuracy: 0.8141 - val_loss: 0.4787 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 295/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.8128\n",
            "Epoch 295: val_loss improved from 0.47854 to 0.47842, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4154 - accuracy: 0.8131 - val_loss: 0.4784 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 296/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4135 - accuracy: 0.8157\n",
            "Epoch 296: val_loss improved from 0.47842 to 0.47791, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4126 - accuracy: 0.8163 - val_loss: 0.4779 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 297/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8108\n",
            "Epoch 297: val_loss did not improve from 0.47791\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4156 - accuracy: 0.8108 - val_loss: 0.4779 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 298/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8121\n",
            "Epoch 298: val_loss did not improve from 0.47791\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4142 - accuracy: 0.8122 - val_loss: 0.4783 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 299/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4125 - accuracy: 0.8173\n",
            "Epoch 299: val_loss improved from 0.47791 to 0.47772, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4125 - accuracy: 0.8173 - val_loss: 0.4777 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 300/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8161\n",
            "Epoch 300: val_loss did not improve from 0.47772\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4144 - accuracy: 0.8156 - val_loss: 0.4778 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 301/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8196\n",
            "Epoch 301: val_loss improved from 0.47772 to 0.47753, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4130 - accuracy: 0.8198 - val_loss: 0.4775 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 302/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4112 - accuracy: 0.8185\n",
            "Epoch 302: val_loss improved from 0.47753 to 0.47725, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4118 - accuracy: 0.8180 - val_loss: 0.4773 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 303/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4145 - accuracy: 0.8142\n",
            "Epoch 303: val_loss improved from 0.47725 to 0.47653, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4145 - accuracy: 0.8142 - val_loss: 0.4765 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 304/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.8119\n",
            "Epoch 304: val_loss did not improve from 0.47653\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4153 - accuracy: 0.8119 - val_loss: 0.4769 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 305/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4110 - accuracy: 0.8174\n",
            "Epoch 305: val_loss did not improve from 0.47653\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4102 - accuracy: 0.8176 - val_loss: 0.4774 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 306/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4146 - accuracy: 0.8144\n",
            "Epoch 306: val_loss did not improve from 0.47653\n",
            "215/215 [==============================] - 5s 22ms/step - loss: 0.4146 - accuracy: 0.8144 - val_loss: 0.4766 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 307/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8203\n",
            "Epoch 307: val_loss did not improve from 0.47653\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.4098 - accuracy: 0.8201 - val_loss: 0.4770 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 308/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4124 - accuracy: 0.8141\n",
            "Epoch 308: val_loss improved from 0.47653 to 0.47640, saving model to model2.h5\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.4124 - accuracy: 0.8141 - val_loss: 0.4764 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 309/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8170\n",
            "Epoch 309: val_loss improved from 0.47640 to 0.47547, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4120 - accuracy: 0.8170 - val_loss: 0.4755 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 310/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4159 - accuracy: 0.8112\n",
            "Epoch 310: val_loss did not improve from 0.47547\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4138 - accuracy: 0.8128 - val_loss: 0.4759 - val_accuracy: 0.7895 - lr: 1.0000e-06\n",
            "Epoch 311/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8143\n",
            "Epoch 311: val_loss did not improve from 0.47547\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4120 - accuracy: 0.8142 - val_loss: 0.4762 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 312/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8185\n",
            "Epoch 312: val_loss did not improve from 0.47547\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4118 - accuracy: 0.8185 - val_loss: 0.4765 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 313/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8152\n",
            "Epoch 313: val_loss did not improve from 0.47547\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4095 - accuracy: 0.8148 - val_loss: 0.4761 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 314/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.8173\n",
            "Epoch 314: val_loss improved from 0.47547 to 0.47455, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4104 - accuracy: 0.8185 - val_loss: 0.4746 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 315/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.8162\n",
            "Epoch 315: val_loss did not improve from 0.47455\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4105 - accuracy: 0.8164 - val_loss: 0.4756 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 316/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.4089 - accuracy: 0.8189\n",
            "Epoch 316: val_loss did not improve from 0.47455\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4088 - accuracy: 0.8192 - val_loss: 0.4752 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
            "Epoch 317/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4051 - accuracy: 0.8187\n",
            "Epoch 317: val_loss improved from 0.47455 to 0.47452, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4053 - accuracy: 0.8185 - val_loss: 0.4745 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 318/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.8161\n",
            "Epoch 318: val_loss did not improve from 0.47452\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4109 - accuracy: 0.8161 - val_loss: 0.4746 - val_accuracy: 0.7890 - lr: 1.0000e-06\n",
            "Epoch 319/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4104 - accuracy: 0.8141\n",
            "Epoch 319: val_loss improved from 0.47452 to 0.47362, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4093 - accuracy: 0.8144 - val_loss: 0.4736 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 320/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8179\n",
            "Epoch 320: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4069 - accuracy: 0.8182 - val_loss: 0.4739 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 321/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4081 - accuracy: 0.8209\n",
            "Epoch 321: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4081 - accuracy: 0.8209 - val_loss: 0.4741 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 322/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4086 - accuracy: 0.8146\n",
            "Epoch 322: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4078 - accuracy: 0.8157 - val_loss: 0.4738 - val_accuracy: 0.7901 - lr: 1.0000e-06\n",
            "Epoch 323/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4064 - accuracy: 0.8207\n",
            "Epoch 323: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4063 - accuracy: 0.8209 - val_loss: 0.4738 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 324/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.8213\n",
            "Epoch 324: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4070 - accuracy: 0.8212 - val_loss: 0.4738 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 325/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.8164\n",
            "Epoch 325: val_loss did not improve from 0.47362\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4094 - accuracy: 0.8164 - val_loss: 0.4746 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 326/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4037 - accuracy: 0.8183\n",
            "Epoch 326: val_loss improved from 0.47362 to 0.47313, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4037 - accuracy: 0.8183 - val_loss: 0.4731 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 327/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8201\n",
            "Epoch 327: val_loss did not improve from 0.47313\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4031 - accuracy: 0.8202 - val_loss: 0.4731 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 328/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8185\n",
            "Epoch 328: val_loss improved from 0.47313 to 0.47241, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4049 - accuracy: 0.8186 - val_loss: 0.4724 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 329/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8188\n",
            "Epoch 329: val_loss improved from 0.47241 to 0.47223, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4013 - accuracy: 0.8195 - val_loss: 0.4722 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 330/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.8183\n",
            "Epoch 330: val_loss improved from 0.47223 to 0.47176, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4058 - accuracy: 0.8183 - val_loss: 0.4718 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 331/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4034 - accuracy: 0.8212\n",
            "Epoch 331: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4034 - accuracy: 0.8209 - val_loss: 0.4720 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 332/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4029 - accuracy: 0.8217\n",
            "Epoch 332: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4029 - accuracy: 0.8217 - val_loss: 0.4722 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 333/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3993 - accuracy: 0.8228\n",
            "Epoch 333: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3991 - accuracy: 0.8233 - val_loss: 0.4724 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 334/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8199\n",
            "Epoch 334: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4048 - accuracy: 0.8195 - val_loss: 0.4725 - val_accuracy: 0.7907 - lr: 1.0000e-06\n",
            "Epoch 335/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4028 - accuracy: 0.8168\n",
            "Epoch 335: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4048 - accuracy: 0.8157 - val_loss: 0.4726 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 336/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4017 - accuracy: 0.8232\n",
            "Epoch 336: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4019 - accuracy: 0.8227 - val_loss: 0.4720 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 337/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.4018 - accuracy: 0.8213\n",
            "Epoch 337: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4014 - accuracy: 0.8215 - val_loss: 0.4718 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 338/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4033 - accuracy: 0.8197\n",
            "Epoch 338: val_loss did not improve from 0.47176\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4032 - accuracy: 0.8199 - val_loss: 0.4719 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 339/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8202\n",
            "Epoch 339: val_loss improved from 0.47176 to 0.47163, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4029 - accuracy: 0.8202 - val_loss: 0.4716 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 340/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8212\n",
            "Epoch 340: val_loss improved from 0.47163 to 0.47098, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4021 - accuracy: 0.8212 - val_loss: 0.4710 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 341/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8188\n",
            "Epoch 341: val_loss improved from 0.47098 to 0.47062, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.4020 - accuracy: 0.8192 - val_loss: 0.4706 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 342/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.4003 - accuracy: 0.8209\n",
            "Epoch 342: val_loss improved from 0.47062 to 0.47047, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3991 - accuracy: 0.8214 - val_loss: 0.4705 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 343/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8202\n",
            "Epoch 343: val_loss did not improve from 0.47047\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3994 - accuracy: 0.8206 - val_loss: 0.4705 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 344/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.8231\n",
            "Epoch 344: val_loss did not improve from 0.47047\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3990 - accuracy: 0.8231 - val_loss: 0.4713 - val_accuracy: 0.7913 - lr: 1.0000e-06\n",
            "Epoch 345/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.4001 - accuracy: 0.8208\n",
            "Epoch 345: val_loss did not improve from 0.47047\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.4000 - accuracy: 0.8205 - val_loss: 0.4711 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 346/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.4000 - accuracy: 0.8223\n",
            "Epoch 346: val_loss improved from 0.47047 to 0.47024, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3995 - accuracy: 0.8227 - val_loss: 0.4702 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 347/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8243\n",
            "Epoch 347: val_loss did not improve from 0.47024\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3980 - accuracy: 0.8241 - val_loss: 0.4704 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 348/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8220\n",
            "Epoch 348: val_loss improved from 0.47024 to 0.47015, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3993 - accuracy: 0.8222 - val_loss: 0.4702 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 349/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3973 - accuracy: 0.8256\n",
            "Epoch 349: val_loss improved from 0.47015 to 0.46928, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3971 - accuracy: 0.8257 - val_loss: 0.4693 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 350/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.4018 - accuracy: 0.8247\n",
            "Epoch 350: val_loss did not improve from 0.46928\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4018 - accuracy: 0.8247 - val_loss: 0.4695 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 351/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3998 - accuracy: 0.8233\n",
            "Epoch 351: val_loss improved from 0.46928 to 0.46915, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3998 - accuracy: 0.8231 - val_loss: 0.4692 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 352/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3986 - accuracy: 0.8199\n",
            "Epoch 352: val_loss did not improve from 0.46915\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3973 - accuracy: 0.8208 - val_loss: 0.4694 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 353/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8266\n",
            "Epoch 353: val_loss improved from 0.46915 to 0.46909, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3941 - accuracy: 0.8266 - val_loss: 0.4691 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 354/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3971 - accuracy: 0.8243\n",
            "Epoch 354: val_loss did not improve from 0.46909\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3970 - accuracy: 0.8244 - val_loss: 0.4696 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 355/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8227\n",
            "Epoch 355: val_loss did not improve from 0.46909\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3984 - accuracy: 0.8225 - val_loss: 0.4695 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 356/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3941 - accuracy: 0.8259\n",
            "Epoch 356: val_loss improved from 0.46909 to 0.46888, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3946 - accuracy: 0.8256 - val_loss: 0.4689 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 357/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3977 - accuracy: 0.8252\n",
            "Epoch 357: val_loss improved from 0.46888 to 0.46876, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 13ms/step - loss: 0.3977 - accuracy: 0.8251 - val_loss: 0.4688 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 358/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.8263\n",
            "Epoch 358: val_loss improved from 0.46876 to 0.46830, saving model to model2.h5\n",
            "215/215 [==============================] - 5s 21ms/step - loss: 0.3951 - accuracy: 0.8263 - val_loss: 0.4683 - val_accuracy: 0.7942 - lr: 1.0000e-06\n",
            "Epoch 359/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3957 - accuracy: 0.8238\n",
            "Epoch 359: val_loss did not improve from 0.46830\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3955 - accuracy: 0.8238 - val_loss: 0.4686 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 360/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3943 - accuracy: 0.8279\n",
            "Epoch 360: val_loss did not improve from 0.46830\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3941 - accuracy: 0.8276 - val_loss: 0.4688 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 361/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8269\n",
            "Epoch 361: val_loss did not improve from 0.46830\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3970 - accuracy: 0.8272 - val_loss: 0.4684 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 362/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3965 - accuracy: 0.8241\n",
            "Epoch 362: val_loss did not improve from 0.46830\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3964 - accuracy: 0.8240 - val_loss: 0.4684 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 363/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3953 - accuracy: 0.8238\n",
            "Epoch 363: val_loss did not improve from 0.46830\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3956 - accuracy: 0.8238 - val_loss: 0.4683 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 364/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8234\n",
            "Epoch 364: val_loss improved from 0.46830 to 0.46777, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3945 - accuracy: 0.8234 - val_loss: 0.4678 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 365/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3951 - accuracy: 0.8230\n",
            "Epoch 365: val_loss did not improve from 0.46777\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3950 - accuracy: 0.8230 - val_loss: 0.4681 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 366/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.8250\n",
            "Epoch 366: val_loss did not improve from 0.46777\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3946 - accuracy: 0.8250 - val_loss: 0.4679 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 367/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8239\n",
            "Epoch 367: val_loss improved from 0.46777 to 0.46775, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3929 - accuracy: 0.8243 - val_loss: 0.4678 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 368/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3900 - accuracy: 0.8270\n",
            "Epoch 368: val_loss improved from 0.46775 to 0.46763, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3914 - accuracy: 0.8260 - val_loss: 0.4676 - val_accuracy: 0.7919 - lr: 1.0000e-06\n",
            "Epoch 369/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8225\n",
            "Epoch 369: val_loss did not improve from 0.46763\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3956 - accuracy: 0.8230 - val_loss: 0.4678 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 370/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8279\n",
            "Epoch 370: val_loss improved from 0.46763 to 0.46726, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3928 - accuracy: 0.8266 - val_loss: 0.4673 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 371/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3933 - accuracy: 0.8245\n",
            "Epoch 371: val_loss improved from 0.46726 to 0.46719, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3934 - accuracy: 0.8243 - val_loss: 0.4672 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 372/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8285\n",
            "Epoch 372: val_loss improved from 0.46719 to 0.46685, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3894 - accuracy: 0.8285 - val_loss: 0.4669 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 373/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3902 - accuracy: 0.8249\n",
            "Epoch 373: val_loss improved from 0.46685 to 0.46668, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3902 - accuracy: 0.8249 - val_loss: 0.4667 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 374/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8281\n",
            "Epoch 374: val_loss did not improve from 0.46668\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3929 - accuracy: 0.8281 - val_loss: 0.4674 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 375/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3931 - accuracy: 0.8249\n",
            "Epoch 375: val_loss did not improve from 0.46668\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3928 - accuracy: 0.8249 - val_loss: 0.4668 - val_accuracy: 0.7924 - lr: 1.0000e-06\n",
            "Epoch 376/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8245\n",
            "Epoch 376: val_loss did not improve from 0.46668\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3909 - accuracy: 0.8243 - val_loss: 0.4667 - val_accuracy: 0.7930 - lr: 1.0000e-06\n",
            "Epoch 377/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8269\n",
            "Epoch 377: val_loss improved from 0.46668 to 0.46642, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3871 - accuracy: 0.8269 - val_loss: 0.4664 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 378/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8276\n",
            "Epoch 378: val_loss improved from 0.46642 to 0.46608, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3881 - accuracy: 0.8276 - val_loss: 0.4661 - val_accuracy: 0.7936 - lr: 1.0000e-06\n",
            "Epoch 379/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8307\n",
            "Epoch 379: val_loss improved from 0.46608 to 0.46599, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3907 - accuracy: 0.8307 - val_loss: 0.4660 - val_accuracy: 0.7942 - lr: 1.0000e-06\n",
            "Epoch 380/500\n",
            "208/215 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8242\n",
            "Epoch 380: val_loss did not improve from 0.46599\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3903 - accuracy: 0.8243 - val_loss: 0.4662 - val_accuracy: 0.7948 - lr: 1.0000e-06\n",
            "Epoch 381/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8269\n",
            "Epoch 381: val_loss improved from 0.46599 to 0.46552, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3898 - accuracy: 0.8269 - val_loss: 0.4655 - val_accuracy: 0.7959 - lr: 1.0000e-06\n",
            "Epoch 382/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.8286\n",
            "Epoch 382: val_loss did not improve from 0.46552\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3884 - accuracy: 0.8297 - val_loss: 0.4657 - val_accuracy: 0.7959 - lr: 1.0000e-06\n",
            "Epoch 383/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3894 - accuracy: 0.8267\n",
            "Epoch 383: val_loss improved from 0.46552 to 0.46477, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3892 - accuracy: 0.8267 - val_loss: 0.4648 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 384/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8249\n",
            "Epoch 384: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3878 - accuracy: 0.8250 - val_loss: 0.4659 - val_accuracy: 0.7953 - lr: 1.0000e-06\n",
            "Epoch 385/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.8271\n",
            "Epoch 385: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3885 - accuracy: 0.8267 - val_loss: 0.4658 - val_accuracy: 0.7953 - lr: 1.0000e-06\n",
            "Epoch 386/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8284\n",
            "Epoch 386: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3892 - accuracy: 0.8283 - val_loss: 0.4655 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 387/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8262\n",
            "Epoch 387: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3897 - accuracy: 0.8262 - val_loss: 0.4653 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 388/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3865 - accuracy: 0.8283\n",
            "Epoch 388: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3869 - accuracy: 0.8285 - val_loss: 0.4654 - val_accuracy: 0.7959 - lr: 1.0000e-06\n",
            "Epoch 389/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8292\n",
            "Epoch 389: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3853 - accuracy: 0.8283 - val_loss: 0.4649 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 390/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3883 - accuracy: 0.8275\n",
            "Epoch 390: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3889 - accuracy: 0.8270 - val_loss: 0.4661 - val_accuracy: 0.7953 - lr: 1.0000e-06\n",
            "Epoch 391/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3873 - accuracy: 0.8303\n",
            "Epoch 391: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3879 - accuracy: 0.8305 - val_loss: 0.4653 - val_accuracy: 0.7953 - lr: 1.0000e-06\n",
            "Epoch 392/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8286\n",
            "Epoch 392: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3871 - accuracy: 0.8285 - val_loss: 0.4657 - val_accuracy: 0.7959 - lr: 1.0000e-06\n",
            "Epoch 393/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3871 - accuracy: 0.8273\n",
            "Epoch 393: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3861 - accuracy: 0.8282 - val_loss: 0.4655 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 394/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8306\n",
            "Epoch 394: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3853 - accuracy: 0.8310 - val_loss: 0.4651 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 395/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8314\n",
            "Epoch 395: val_loss did not improve from 0.46477\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3840 - accuracy: 0.8301 - val_loss: 0.4650 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 396/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3869 - accuracy: 0.8276\n",
            "Epoch 396: val_loss improved from 0.46477 to 0.46448, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3865 - accuracy: 0.8282 - val_loss: 0.4645 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 397/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8329\n",
            "Epoch 397: val_loss did not improve from 0.46448\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3837 - accuracy: 0.8328 - val_loss: 0.4648 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 398/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3846 - accuracy: 0.8342\n",
            "Epoch 398: val_loss improved from 0.46448 to 0.46436, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3848 - accuracy: 0.8342 - val_loss: 0.4644 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 399/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3872 - accuracy: 0.8269\n",
            "Epoch 399: val_loss improved from 0.46436 to 0.46379, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3866 - accuracy: 0.8272 - val_loss: 0.4638 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 400/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.8326\n",
            "Epoch 400: val_loss did not improve from 0.46379\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3870 - accuracy: 0.8324 - val_loss: 0.4640 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 401/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8317\n",
            "Epoch 401: val_loss improved from 0.46379 to 0.46378, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.3831 - accuracy: 0.8304 - val_loss: 0.4638 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 402/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3843 - accuracy: 0.8323\n",
            "Epoch 402: val_loss did not improve from 0.46378\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3852 - accuracy: 0.8315 - val_loss: 0.4644 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 403/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3832 - accuracy: 0.8321\n",
            "Epoch 403: val_loss improved from 0.46378 to 0.46371, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3835 - accuracy: 0.8314 - val_loss: 0.4637 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 404/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8289\n",
            "Epoch 404: val_loss did not improve from 0.46371\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3842 - accuracy: 0.8294 - val_loss: 0.4644 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 405/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.8323\n",
            "Epoch 405: val_loss improved from 0.46371 to 0.46345, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3833 - accuracy: 0.8323 - val_loss: 0.4634 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 406/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8292\n",
            "Epoch 406: val_loss improved from 0.46345 to 0.46312, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3851 - accuracy: 0.8294 - val_loss: 0.4631 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 407/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.8307\n",
            "Epoch 407: val_loss did not improve from 0.46312\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3818 - accuracy: 0.8307 - val_loss: 0.4631 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 408/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3858 - accuracy: 0.8303\n",
            "Epoch 408: val_loss did not improve from 0.46312\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3866 - accuracy: 0.8294 - val_loss: 0.4632 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 409/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8321\n",
            "Epoch 409: val_loss improved from 0.46312 to 0.46312, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3810 - accuracy: 0.8320 - val_loss: 0.4631 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 410/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.8327\n",
            "Epoch 410: val_loss improved from 0.46312 to 0.46262, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3793 - accuracy: 0.8327 - val_loss: 0.4626 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 411/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3815 - accuracy: 0.8309\n",
            "Epoch 411: val_loss did not improve from 0.46262\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3813 - accuracy: 0.8311 - val_loss: 0.4630 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 412/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8372\n",
            "Epoch 412: val_loss did not improve from 0.46262\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3780 - accuracy: 0.8372 - val_loss: 0.4636 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 413/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.8343\n",
            "Epoch 413: val_loss did not improve from 0.46262\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3774 - accuracy: 0.8352 - val_loss: 0.4629 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 414/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8307\n",
            "Epoch 414: val_loss improved from 0.46262 to 0.46219, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3842 - accuracy: 0.8315 - val_loss: 0.4622 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 415/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3770 - accuracy: 0.8325\n",
            "Epoch 415: val_loss did not improve from 0.46219\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3782 - accuracy: 0.8313 - val_loss: 0.4626 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 416/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3764 - accuracy: 0.8381\n",
            "Epoch 416: val_loss did not improve from 0.46219\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3776 - accuracy: 0.8369 - val_loss: 0.4629 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 417/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3781 - accuracy: 0.8311\n",
            "Epoch 417: val_loss improved from 0.46219 to 0.46198, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3780 - accuracy: 0.8315 - val_loss: 0.4620 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 418/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8323\n",
            "Epoch 418: val_loss did not improve from 0.46198\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3810 - accuracy: 0.8313 - val_loss: 0.4621 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 419/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8319\n",
            "Epoch 419: val_loss did not improve from 0.46198\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3752 - accuracy: 0.8323 - val_loss: 0.4622 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 420/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.8329\n",
            "Epoch 420: val_loss did not improve from 0.46198\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3782 - accuracy: 0.8334 - val_loss: 0.4620 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 421/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3755 - accuracy: 0.8369\n",
            "Epoch 421: val_loss improved from 0.46198 to 0.46187, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3754 - accuracy: 0.8369 - val_loss: 0.4619 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 422/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8328\n",
            "Epoch 422: val_loss improved from 0.46187 to 0.46186, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3778 - accuracy: 0.8318 - val_loss: 0.4619 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 423/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.8320\n",
            "Epoch 423: val_loss improved from 0.46186 to 0.46166, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3781 - accuracy: 0.8321 - val_loss: 0.4617 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 424/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.8323\n",
            "Epoch 424: val_loss improved from 0.46166 to 0.46134, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3780 - accuracy: 0.8323 - val_loss: 0.4613 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 425/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8332\n",
            "Epoch 425: val_loss did not improve from 0.46134\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3748 - accuracy: 0.8328 - val_loss: 0.4621 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 426/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3768 - accuracy: 0.8340\n",
            "Epoch 426: val_loss did not improve from 0.46134\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3767 - accuracy: 0.8340 - val_loss: 0.4617 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 427/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8353\n",
            "Epoch 427: val_loss improved from 0.46134 to 0.46088, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3747 - accuracy: 0.8362 - val_loss: 0.4609 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 428/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3776 - accuracy: 0.8335\n",
            "Epoch 428: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3792 - accuracy: 0.8323 - val_loss: 0.4618 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 429/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.8302\n",
            "Epoch 429: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3800 - accuracy: 0.8304 - val_loss: 0.4615 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 430/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8272\n",
            "Epoch 430: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3786 - accuracy: 0.8273 - val_loss: 0.4618 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 431/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8310\n",
            "Epoch 431: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3747 - accuracy: 0.8305 - val_loss: 0.4617 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 432/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8371\n",
            "Epoch 432: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3735 - accuracy: 0.8371 - val_loss: 0.4621 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 433/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3759 - accuracy: 0.8335\n",
            "Epoch 433: val_loss did not improve from 0.46088\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3769 - accuracy: 0.8327 - val_loss: 0.4613 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 434/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3762 - accuracy: 0.8323\n",
            "Epoch 434: val_loss improved from 0.46088 to 0.46063, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3772 - accuracy: 0.8313 - val_loss: 0.4606 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 435/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8341\n",
            "Epoch 435: val_loss did not improve from 0.46063\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3776 - accuracy: 0.8343 - val_loss: 0.4609 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 436/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8303\n",
            "Epoch 436: val_loss did not improve from 0.46063\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3752 - accuracy: 0.8310 - val_loss: 0.4607 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 437/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3780 - accuracy: 0.8309\n",
            "Epoch 437: val_loss did not improve from 0.46063\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3775 - accuracy: 0.8315 - val_loss: 0.4612 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 438/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.8321\n",
            "Epoch 438: val_loss did not improve from 0.46063\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3751 - accuracy: 0.8326 - val_loss: 0.4609 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 439/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8348\n",
            "Epoch 439: val_loss improved from 0.46063 to 0.46025, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3747 - accuracy: 0.8339 - val_loss: 0.4602 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 440/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8363\n",
            "Epoch 440: val_loss improved from 0.46025 to 0.46011, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3720 - accuracy: 0.8363 - val_loss: 0.4601 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 441/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8331\n",
            "Epoch 441: val_loss did not improve from 0.46011\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3730 - accuracy: 0.8337 - val_loss: 0.4605 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 442/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.8382\n",
            "Epoch 442: val_loss did not improve from 0.46011\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3694 - accuracy: 0.8382 - val_loss: 0.4606 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 443/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.8347\n",
            "Epoch 443: val_loss did not improve from 0.46011\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3713 - accuracy: 0.8347 - val_loss: 0.4609 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 444/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3719 - accuracy: 0.8339\n",
            "Epoch 444: val_loss did not improve from 0.46011\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3714 - accuracy: 0.8343 - val_loss: 0.4604 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 445/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3733 - accuracy: 0.8306\n",
            "Epoch 445: val_loss improved from 0.46011 to 0.45964, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3727 - accuracy: 0.8311 - val_loss: 0.4596 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 446/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8353\n",
            "Epoch 446: val_loss did not improve from 0.45964\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3703 - accuracy: 0.8355 - val_loss: 0.4604 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 447/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8334\n",
            "Epoch 447: val_loss improved from 0.45964 to 0.45926, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3742 - accuracy: 0.8344 - val_loss: 0.4593 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 448/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.8334\n",
            "Epoch 448: val_loss did not improve from 0.45926\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3733 - accuracy: 0.8355 - val_loss: 0.4599 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 449/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.8365\n",
            "Epoch 449: val_loss did not improve from 0.45926\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3713 - accuracy: 0.8368 - val_loss: 0.4601 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 450/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8381\n",
            "Epoch 450: val_loss did not improve from 0.45926\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3682 - accuracy: 0.8379 - val_loss: 0.4603 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 451/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8389\n",
            "Epoch 451: val_loss improved from 0.45926 to 0.45902, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.3694 - accuracy: 0.8385 - val_loss: 0.4590 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 452/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3678 - accuracy: 0.8382\n",
            "Epoch 452: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3681 - accuracy: 0.8381 - val_loss: 0.4591 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 453/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8366\n",
            "Epoch 453: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3721 - accuracy: 0.8359 - val_loss: 0.4595 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 454/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3702 - accuracy: 0.8373\n",
            "Epoch 454: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3697 - accuracy: 0.8375 - val_loss: 0.4591 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 455/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8404\n",
            "Epoch 455: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3642 - accuracy: 0.8407 - val_loss: 0.4601 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 456/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8357\n",
            "Epoch 456: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3724 - accuracy: 0.8360 - val_loss: 0.4599 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 457/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8362\n",
            "Epoch 457: val_loss did not improve from 0.45902\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3701 - accuracy: 0.8355 - val_loss: 0.4594 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 458/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8381\n",
            "Epoch 458: val_loss improved from 0.45902 to 0.45889, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3698 - accuracy: 0.8368 - val_loss: 0.4589 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 459/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8291\n",
            "Epoch 459: val_loss did not improve from 0.45889\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3714 - accuracy: 0.8291 - val_loss: 0.4597 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 460/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8384\n",
            "Epoch 460: val_loss did not improve from 0.45889\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3641 - accuracy: 0.8384 - val_loss: 0.4599 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 461/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3670 - accuracy: 0.8352\n",
            "Epoch 461: val_loss improved from 0.45889 to 0.45851, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3683 - accuracy: 0.8347 - val_loss: 0.4585 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 462/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8381\n",
            "Epoch 462: val_loss did not improve from 0.45851\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3711 - accuracy: 0.8384 - val_loss: 0.4589 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 463/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8376\n",
            "Epoch 463: val_loss improved from 0.45851 to 0.45837, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3655 - accuracy: 0.8379 - val_loss: 0.4584 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 464/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8386\n",
            "Epoch 464: val_loss improved from 0.45837 to 0.45789, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3650 - accuracy: 0.8384 - val_loss: 0.4579 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 465/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8408\n",
            "Epoch 465: val_loss did not improve from 0.45789\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3649 - accuracy: 0.8408 - val_loss: 0.4591 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 466/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3673 - accuracy: 0.8364\n",
            "Epoch 466: val_loss did not improve from 0.45789\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3677 - accuracy: 0.8360 - val_loss: 0.4585 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 467/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8393\n",
            "Epoch 467: val_loss did not improve from 0.45789\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3671 - accuracy: 0.8382 - val_loss: 0.4581 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 468/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8330\n",
            "Epoch 468: val_loss did not improve from 0.45789\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3674 - accuracy: 0.8330 - val_loss: 0.4579 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 469/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3637 - accuracy: 0.8385\n",
            "Epoch 469: val_loss improved from 0.45789 to 0.45767, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3644 - accuracy: 0.8382 - val_loss: 0.4577 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 470/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3667 - accuracy: 0.8351\n",
            "Epoch 470: val_loss did not improve from 0.45767\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3661 - accuracy: 0.8352 - val_loss: 0.4587 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 471/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3660 - accuracy: 0.8374\n",
            "Epoch 471: val_loss did not improve from 0.45767\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3660 - accuracy: 0.8376 - val_loss: 0.4589 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 472/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8402\n",
            "Epoch 472: val_loss did not improve from 0.45767\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3656 - accuracy: 0.8401 - val_loss: 0.4580 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 473/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3647 - accuracy: 0.8424\n",
            "Epoch 473: val_loss did not improve from 0.45767\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3649 - accuracy: 0.8423 - val_loss: 0.4581 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 474/500\n",
            "208/215 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8427\n",
            "Epoch 474: val_loss did not improve from 0.45767\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3631 - accuracy: 0.8432 - val_loss: 0.4578 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 475/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.8367\n",
            "Epoch 475: val_loss improved from 0.45767 to 0.45754, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3658 - accuracy: 0.8371 - val_loss: 0.4575 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 476/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8368\n",
            "Epoch 476: val_loss did not improve from 0.45754\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3650 - accuracy: 0.8378 - val_loss: 0.4577 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 477/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3626 - accuracy: 0.8407\n",
            "Epoch 477: val_loss did not improve from 0.45754\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3629 - accuracy: 0.8404 - val_loss: 0.4583 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 478/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8426\n",
            "Epoch 478: val_loss improved from 0.45754 to 0.45753, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3604 - accuracy: 0.8430 - val_loss: 0.4575 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 479/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3615 - accuracy: 0.8433\n",
            "Epoch 479: val_loss did not improve from 0.45753\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3608 - accuracy: 0.8438 - val_loss: 0.4576 - val_accuracy: 0.7994 - lr: 1.0000e-06\n",
            "Epoch 480/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3649 - accuracy: 0.8355\n",
            "Epoch 480: val_loss did not improve from 0.45753\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3649 - accuracy: 0.8355 - val_loss: 0.4578 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 481/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3625 - accuracy: 0.8419\n",
            "Epoch 481: val_loss did not improve from 0.45753\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3617 - accuracy: 0.8424 - val_loss: 0.4579 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 482/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3614 - accuracy: 0.8448\n",
            "Epoch 482: val_loss improved from 0.45753 to 0.45671, saving model to model2.h5\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3618 - accuracy: 0.8446 - val_loss: 0.4567 - val_accuracy: 0.8006 - lr: 1.0000e-06\n",
            "Epoch 483/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3611 - accuracy: 0.8402\n",
            "Epoch 483: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3625 - accuracy: 0.8387 - val_loss: 0.4577 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 484/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.8401\n",
            "Epoch 484: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3621 - accuracy: 0.8401 - val_loss: 0.4575 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 485/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3607 - accuracy: 0.8384\n",
            "Epoch 485: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3613 - accuracy: 0.8384 - val_loss: 0.4584 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 486/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3616 - accuracy: 0.8387\n",
            "Epoch 486: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3616 - accuracy: 0.8387 - val_loss: 0.4587 - val_accuracy: 0.7965 - lr: 1.0000e-06\n",
            "Epoch 487/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8420\n",
            "Epoch 487: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3613 - accuracy: 0.8426 - val_loss: 0.4578 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 488/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3610 - accuracy: 0.8411\n",
            "Epoch 488: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3616 - accuracy: 0.8404 - val_loss: 0.4579 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 489/500\n",
            "211/215 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8430\n",
            "Epoch 489: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3582 - accuracy: 0.8438 - val_loss: 0.4570 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 490/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3643 - accuracy: 0.8384\n",
            "Epoch 490: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3647 - accuracy: 0.8382 - val_loss: 0.4570 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 491/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8421\n",
            "Epoch 491: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3598 - accuracy: 0.8411 - val_loss: 0.4575 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 492/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.8442\n",
            "Epoch 492: val_loss did not improve from 0.45671\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3601 - accuracy: 0.8442 - val_loss: 0.4576 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 493/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8436\n",
            "Epoch 493: val_loss improved from 0.45671 to 0.45668, saving model to model2.h5\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.3599 - accuracy: 0.8442 - val_loss: 0.4567 - val_accuracy: 0.7988 - lr: 1.0000e-06\n",
            "Epoch 494/500\n",
            "209/215 [============================>.] - ETA: 0s - loss: 0.3571 - accuracy: 0.8448\n",
            "Epoch 494: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3576 - accuracy: 0.8436 - val_loss: 0.4572 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 495/500\n",
            "210/215 [============================>.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8420\n",
            "Epoch 495: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.3559 - accuracy: 0.8423 - val_loss: 0.4570 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 496/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3629 - accuracy: 0.8427\n",
            "Epoch 496: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3639 - accuracy: 0.8420 - val_loss: 0.4571 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 497/500\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8446\n",
            "Epoch 497: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3587 - accuracy: 0.8446 - val_loss: 0.4567 - val_accuracy: 0.7983 - lr: 1.0000e-06\n",
            "Epoch 498/500\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8402\n",
            "Epoch 498: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 9ms/step - loss: 0.3627 - accuracy: 0.8406 - val_loss: 0.4568 - val_accuracy: 0.7977 - lr: 1.0000e-06\n",
            "Epoch 499/500\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.3573 - accuracy: 0.8405\n",
            "Epoch 499: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3573 - accuracy: 0.8401 - val_loss: 0.4572 - val_accuracy: 0.7971 - lr: 1.0000e-06\n",
            "Epoch 500/500\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8407\n",
            "Epoch 500: val_loss did not improve from 0.45668\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.3599 - accuracy: 0.8407 - val_loss: 0.4573 - val_accuracy: 0.7971 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "hist2 = model2.fit([train_X, train_X, train_X], train_Y, epochs = 500, batch_size = BS, validation_data = ([val_X, val_X, val_X], val_Y), callbacks = [callbacks_list], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c59uLEVtMJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "5e45f843-b4da-4360-c627-a68c0a1d2730"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b5d437e3-286a-4711-a3a0-19f25b798a7f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b5d437e3-286a-4711-a3a0-19f25b798a7f\")) {                    Plotly.newPlot(                        \"b5d437e3-286a-4711-a3a0-19f25b798a7f\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"loss\",\"y\":[0.6871441006660461,0.6786711812019348,0.6755672097206116,0.672887921333313,0.6697375178337097,0.6678822636604309,0.6668151021003723,0.6655546426773071,0.6633697748184204,0.6621963381767273,0.6613292694091797,0.6596215963363647,0.6570925116539001,0.6567436456680298,0.6537601351737976,0.6531683206558228,0.649885356426239,0.6482153534889221,0.6467058658599854,0.6443716287612915,0.6435415148735046,0.6410439610481262,0.6381928324699402,0.636432945728302,0.6354403495788574,0.6331550478935242,0.6315033435821533,0.6301635503768921,0.6295303702354431,0.6266676187515259,0.6255823969841003,0.6249774098396301,0.6213655471801758,0.6210591197013855,0.619618833065033,0.6179031133651733,0.6158653497695923,0.6149298548698425,0.6128273606300354,0.6116647124290466,0.6097899675369263,0.6068856716156006,0.608085036277771,0.6085216999053955,0.6054893136024475,0.6037836670875549,0.6039719581604004,0.6022258400917053,0.6011088490486145,0.5990424752235413,0.5984982252120972,0.5958773493766785,0.595841109752655,0.5950709581375122,0.5931251645088196,0.5929335355758667,0.5920755863189697,0.5894923210144043,0.589668333530426,0.5883374214172363,0.5870906710624695,0.5848600268363953,0.5845630764961243,0.5813274383544922,0.5811601281166077,0.5800930857658386,0.5779910087585449,0.5770499110221863,0.5756033062934875,0.575682520866394,0.5721951723098755,0.5716105699539185,0.5724325776100159,0.5705022811889648,0.5677362680435181,0.5681471824645996,0.5671497583389282,0.5633844137191772,0.5640613436698914,0.5607706904411316,0.5597885847091675,0.5610361695289612,0.5563284158706665,0.5581995248794556,0.5565381050109863,0.5541208982467651,0.5534566044807434,0.5513637661933899,0.5507746338844299,0.5485716462135315,0.5485067963600159,0.546582043170929,0.5453227758407593,0.5449892282485962,0.5427777171134949,0.542745053768158,0.5404600501060486,0.5400960445404053,0.5380899906158447,0.536749541759491,0.537837028503418,0.5335289239883423,0.5346682667732239,0.5326917171478271,0.5325932502746582,0.5310776829719543,0.5275161862373352,0.5287408232688904,0.5258165597915649,0.5255382657051086,0.5236748456954956,0.523414134979248,0.5225808620452881,0.521399974822998,0.5186517238616943,0.5182671546936035,0.5192826986312866,0.517916202545166,0.5149944424629211,0.5149949789047241,0.5140178799629211,0.5132838487625122,0.5122054815292358,0.5112594366073608,0.510954737663269,0.5100260972976685,0.5066004395484924,0.5042027831077576,0.5049193501472473,0.5079269409179688,0.5033138394355774,0.5011863112449646,0.5034618973731995,0.5030187368392944,0.49940046668052673,0.5013313293457031,0.5001108050346375,0.4978591501712799,0.4978066086769104,0.4962567985057831,0.4974913001060486,0.4951452910900116,0.49418768286705017,0.4921092987060547,0.4918869435787201,0.4928787350654602,0.4922838807106018,0.4908691644668579,0.48983854055404663,0.48849520087242126,0.4866260588169098,0.48846977949142456,0.4870170056819916,0.48538827896118164,0.48584282398223877,0.48467937111854553,0.4828302562236786,0.4814968705177307,0.4808840751647949,0.48090648651123047,0.4814452826976776,0.47862595319747925,0.4783572256565094,0.47719070315361023,0.47836506366729736,0.4756013751029968,0.47673916816711426,0.4757835865020752,0.4740229547023773,0.4711485505104065,0.4727359712123871,0.4728778898715973,0.4744971692562103,0.47146981954574585,0.46971309185028076,0.4711393415927887,0.46966731548309326,0.4705323576927185,0.4715438187122345,0.4706885516643524,0.46751880645751953,0.46608659625053406,0.4689428210258484,0.4685899615287781,0.46481412649154663,0.4644157886505127,0.4642999470233917,0.46465635299682617,0.46217676997184753,0.4625678062438965,0.46270063519477844,0.4597606658935547,0.46053260564804077,0.46136829257011414,0.45875269174575806,0.4588666260242462,0.4565373957157135,0.4560122489929199,0.4566482901573181,0.45662721991539,0.45653557777404785,0.45470598340034485,0.45487549901008606,0.45448437333106995,0.45670273900032043,0.4531543552875519,0.45092272758483887,0.45537641644477844,0.45224252343177795,0.4523199796676636,0.45143771171569824,0.45172956585884094,0.451528936624527,0.4500364363193512,0.44799649715423584,0.45034751296043396,0.4486791789531708,0.44926005601882935,0.4458469748497009,0.4461584687232971,0.445192813873291,0.4451930522918701,0.4455588459968567,0.4446224868297577,0.44520798325538635,0.44485658407211304,0.4438146948814392,0.44350266456604004,0.44109413027763367,0.4423588514328003,0.44106659293174744,0.4396306574344635,0.4408421814441681,0.44035544991493225,0.44085821509361267,0.43742483854293823,0.4420665204524994,0.4381152391433716,0.4379795491695404,0.44203558564186096,0.43669140338897705,0.4357609152793884,0.43628060817718506,0.4379039406776428,0.4355778098106384,0.4353582262992859,0.43453192710876465,0.4339924156665802,0.43242770433425903,0.4320247173309326,0.43759846687316895,0.43434759974479675,0.43258365988731384,0.43376702070236206,0.4311749041080475,0.42796412110328674,0.4307723343372345,0.4315209686756134,0.42899078130722046,0.4296738803386688,0.4298032820224762,0.4294683039188385,0.4283926784992218,0.4281426668167114,0.4248510003089905,0.4275817275047302,0.42594996094703674,0.42716798186302185,0.4286099374294281,0.42399120330810547,0.4243322014808655,0.42727047204971313,0.42655882239341736,0.4234270453453064,0.42625370621681213,0.423519104719162,0.4223145544528961,0.4244089126586914,0.4258424937725067,0.4237196445465088,0.42172420024871826,0.42303094267845154,0.4228058457374573,0.4215645492076874,0.420428067445755,0.42169082164764404,0.42148229479789734,0.41857799887657166,0.41732510924339294,0.4179307818412781,0.4167447090148926,0.4158845841884613,0.41886740922927856,0.4166521430015564,0.41544365882873535,0.4126121699810028,0.41561391949653625,0.41417136788368225,0.4124606251716614,0.4143635630607605,0.41298940777778625,0.4118078947067261,0.4144992530345917,0.4153270423412323,0.4101898670196533,0.4145762026309967,0.40981626510620117,0.4123728573322296,0.41204798221588135,0.4138481318950653,0.41202276945114136,0.4118172526359558,0.4094577431678772,0.4103888273239136,0.41053175926208496,0.40875911712646484,0.40525710582733154,0.4108524024486542,0.4093171954154968,0.4068533480167389,0.40810003876686096,0.4077804982662201,0.4062711000442505,0.40700793266296387,0.4093846082687378,0.4037094712257385,0.40308547019958496,0.4049219787120819,0.4012628197669983,0.4057592451572418,0.40342840552330017,0.4028683304786682,0.3990960717201233,0.4048008322715759,0.4048258662223816,0.4018900394439697,0.4013998210430145,0.4032110571861267,0.40285524725914,0.4020826816558838,0.40201058983802795,0.3991014063358307,0.39942222833633423,0.39898353815078735,0.399996280670166,0.39950716495513916,0.3980018198490143,0.39928001165390015,0.3970770239830017,0.4017881155014038,0.3997921645641327,0.39728304743766785,0.39409148693084717,0.39697450399398804,0.3983612358570099,0.3945923149585724,0.3976847529411316,0.3951400816440582,0.39547112584114075,0.3940999209880829,0.3970393240451813,0.39638787508010864,0.3955758810043335,0.39450597763061523,0.3950461149215698,0.394553005695343,0.3928931653499603,0.39140579104423523,0.39555326104164124,0.39283937215805054,0.39341455698013306,0.389443963766098,0.39023062586784363,0.39286643266677856,0.3928332030773163,0.3909376859664917,0.38711652159690857,0.38811174035072327,0.3907294273376465,0.39034005999565125,0.38978737592697144,0.3883599638938904,0.38916000723838806,0.387784481048584,0.3884616792201996,0.38923463225364685,0.38967883586883545,0.3868848979473114,0.38533851504325867,0.38886407017707825,0.38790106773376465,0.3871166706085205,0.38614922761917114,0.38534608483314514,0.38401731848716736,0.3864685297012329,0.3837094306945801,0.3848223388195038,0.38662511110305786,0.38695812225341797,0.38310012221336365,0.3852076828479767,0.38348743319511414,0.384234219789505,0.3832789659500122,0.3851171135902405,0.3817968964576721,0.38658607006073,0.38104650378227234,0.37929797172546387,0.38126659393310547,0.37800493836402893,0.3774464726448059,0.3842134475708008,0.37821754813194275,0.3776036500930786,0.37796205282211304,0.3809798061847687,0.37521424889564514,0.37817123532295227,0.3753723204135895,0.37784308195114136,0.3781464397907257,0.3780241906642914,0.37475666403770447,0.3767004609107971,0.3747222423553467,0.3791889250278473,0.37998875975608826,0.3785776197910309,0.3747018575668335,0.37350767850875854,0.3769359886646271,0.37724730372428894,0.37755608558654785,0.37522390484809875,0.3775179386138916,0.37512144446372986,0.3747081458568573,0.3719906508922577,0.3730144798755646,0.3694221079349518,0.3712801933288574,0.3714487552642822,0.3726750314235687,0.37029606103897095,0.3742060959339142,0.37325337529182434,0.37127384543418884,0.3682266175746918,0.3693803548812866,0.3681279122829437,0.3720960319042206,0.3696570098400116,0.3641890287399292,0.37243473529815674,0.37006521224975586,0.3698095381259918,0.37137022614479065,0.3640930950641632,0.3683082163333893,0.3710510730743408,0.365517258644104,0.36504027247428894,0.3648570477962494,0.3677055239677429,0.3671415150165558,0.3673938512802124,0.36439067125320435,0.3661212921142578,0.36602872610092163,0.3655782639980316,0.36490464210510254,0.3631419837474823,0.3657844662666321,0.3649507761001587,0.3628652095794678,0.3604291081428528,0.36077171564102173,0.3648674488067627,0.36174190044403076,0.3617806136608124,0.3624519407749176,0.36205241084098816,0.36129605770111084,0.3615669012069702,0.3612751364707947,0.3615720570087433,0.3582369387149811,0.36468565464019775,0.35980427265167236,0.36010465025901794,0.35990336537361145,0.3576050400733948,0.3559230864048004,0.3639262616634369,0.35865020751953125,0.36270496249198914,0.35734668374061584,0.3598760664463043],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_loss\",\"y\":[0.6809169054031372,0.6767429709434509,0.673827052116394,0.671583890914917,0.6698185205459595,0.6681212782859802,0.666628360748291,0.6653658151626587,0.6640852093696594,0.6628590226173401,0.6615074276924133,0.6601510643959045,0.6587307453155518,0.657281756401062,0.6556185483932495,0.6539073586463928,0.652144730091095,0.6504356861114502,0.6486284732818604,0.6468687653541565,0.6451830267906189,0.643477737903595,0.6418032050132751,0.640295684337616,0.6388174891471863,0.637365996837616,0.6359764337539673,0.6346037983894348,0.6332215070724487,0.6319060325622559,0.630662739276886,0.6294772028923035,0.6282785534858704,0.6271527409553528,0.6260155439376831,0.6248756051063538,0.6237779259681702,0.6227723360061646,0.6216970086097717,0.6206291913986206,0.6195869445800781,0.6185258626937866,0.6174977421760559,0.6165136098861694,0.6155028343200684,0.6144505143165588,0.6134729981422424,0.6124375462532043,0.6114673614501953,0.610500156879425,0.6096228957176208,0.6086494326591492,0.6076483726501465,0.6067571043968201,0.6058238744735718,0.6048994064331055,0.6040056347846985,0.6030080914497375,0.6021322011947632,0.6012609601020813,0.6003463864326477,0.599445641040802,0.5984849333763123,0.5975091457366943,0.5965362787246704,0.59556645154953,0.5946592092514038,0.5936613082885742,0.5927262306213379,0.5917924642562866,0.5908361077308655,0.5899770259857178,0.5890704393386841,0.5881580710411072,0.5871922969818115,0.5861802101135254,0.5852231979370117,0.58432936668396,0.5833241939544678,0.5823387503623962,0.5813410878181458,0.5802919864654541,0.5794086456298828,0.5783306956291199,0.5773912668228149,0.5764825940132141,0.5755777955055237,0.5745887756347656,0.5734836459159851,0.5724979639053345,0.5716628432273865,0.570663571357727,0.569756805896759,0.5688563585281372,0.5678235292434692,0.566772997379303,0.565996527671814,0.5651805400848389,0.5642210841178894,0.5633310079574585,0.5624120235443115,0.5614591240882874,0.560663104057312,0.5596745610237122,0.5586647987365723,0.5578330159187317,0.5569930076599121,0.5560122728347778,0.555219829082489,0.5543553829193115,0.5534583926200867,0.5527186989784241,0.5517651438713074,0.5511733889579773,0.5502631664276123,0.549405574798584,0.5484662652015686,0.547777533531189,0.5470165014266968,0.546245813369751,0.5454564094543457,0.5447220206260681,0.5438926815986633,0.5431982278823853,0.5425397753715515,0.5414667725563049,0.5408981442451477,0.5401567220687866,0.539522647857666,0.5387405157089233,0.5381377935409546,0.537464439868927,0.5367237329483032,0.5359466671943665,0.5351961851119995,0.5345104336738586,0.5339440107345581,0.533329963684082,0.5327668786048889,0.5318197011947632,0.5312789678573608,0.5305659174919128,0.5300976634025574,0.5293570756912231,0.5286822319030762,0.5281257629394531,0.5277072191238403,0.5269497632980347,0.5261026620864868,0.5256637930870056,0.5249485373497009,0.5243797898292542,0.5240920782089233,0.5235271453857422,0.5231103301048279,0.522530734539032,0.5218425393104553,0.521178126335144,0.520958423614502,0.5205448865890503,0.5194553136825562,0.5195425152778625,0.5186826586723328,0.5181561708450317,0.5174862146377563,0.5173087120056152,0.5167015790939331,0.5163230895996094,0.5157434940338135,0.5151193141937256,0.5146714448928833,0.5143558382987976,0.5135645866394043,0.5131276845932007,0.5127645134925842,0.5123005509376526,0.5121163725852966,0.5113779306411743,0.5107780694961548,0.5106252431869507,0.5105482935905457,0.5099324584007263,0.5095257759094238,0.5093809962272644,0.5086753964424133,0.5083192586898804,0.5076891183853149,0.5073278546333313,0.5071061253547668,0.5068204402923584,0.5066806077957153,0.5059506297111511,0.5058403611183167,0.5054907202720642,0.5052018165588379,0.5048604011535645,0.5041098594665527,0.503637433052063,0.5028191208839417,0.5028945207595825,0.5026999115943909,0.5020080208778381,0.5016950368881226,0.5013920664787292,0.5014066696166992,0.500667154788971,0.5000453591346741,0.49939411878585815,0.49900582432746887,0.49881598353385925,0.49888283014297485,0.49867913126945496,0.498209685087204,0.49803775548934937,0.4977588951587677,0.4970870018005371,0.49648380279541016,0.4967680871486664,0.49632295966148376,0.49627384543418884,0.4962427616119385,0.49562186002731323,0.49547016620635986,0.4952819049358368,0.49438270926475525,0.49444419145584106,0.4942356050014496,0.49392324686050415,0.4932533800601959,0.4931012988090515,0.4928620159626007,0.49220603704452515,0.492127925157547,0.49240466952323914,0.4916670620441437,0.4916784465312958,0.49107474088668823,0.4909519851207733,0.4904519021511078,0.49034422636032104,0.49047160148620605,0.4898287057876587,0.48933857679367065,0.4891834259033203,0.48867079615592957,0.48845264315605164,0.4882817566394806,0.4877702295780182,0.4878804683685303,0.4881375730037689,0.4879613220691681,0.4877622723579407,0.4874679446220398,0.4871773421764374,0.4864956736564636,0.4865707755088806,0.4865400195121765,0.486034095287323,0.4856976270675659,0.4855552613735199,0.48567286133766174,0.48490071296691895,0.48479869961738586,0.4845694899559021,0.4845232367515564,0.48385804891586304,0.4834674894809723,0.48367780447006226,0.4831445813179016,0.48329439759254456,0.482693612575531,0.48223239183425903,0.4818887710571289,0.48239487409591675,0.48198455572128296,0.48237472772598267,0.482390433549881,0.48120054602622986,0.48195528984069824,0.48130759596824646,0.4809240400791168,0.48094314336776733,0.4806656241416931,0.48081567883491516,0.48072269558906555,0.47983860969543457,0.48025134205818176,0.4800027906894684,0.4800599217414856,0.4794873595237732,0.47900378704071045,0.47854191064834595,0.479113906621933,0.4787062108516693,0.47842293977737427,0.47790828347206116,0.4779341220855713,0.4782761335372925,0.47772201895713806,0.47775253653526306,0.477527379989624,0.4772530198097229,0.47652560472488403,0.4769442677497864,0.47739341855049133,0.47661474347114563,0.4770129323005676,0.47639942169189453,0.4754680097103119,0.47590160369873047,0.4761711061000824,0.4765346050262451,0.4760937988758087,0.4745537340641022,0.4755867123603821,0.47520774602890015,0.47451651096343994,0.47459766268730164,0.47362151741981506,0.47390317916870117,0.4741069972515106,0.47381994128227234,0.47377362847328186,0.47381478548049927,0.47461655735969543,0.4731293320655823,0.47313445806503296,0.47241076827049255,0.4722289741039276,0.47175857424736023,0.4719994068145752,0.47224703431129456,0.4723871350288391,0.4725349545478821,0.4726037085056305,0.47195446491241455,0.47181203961372375,0.47188717126846313,0.47162652015686035,0.47098371386528015,0.47061866521835327,0.4704653322696686,0.47052112221717834,0.4712817966938019,0.4711065888404846,0.4702380895614624,0.4704400300979614,0.47015002369880676,0.46927890181541443,0.4694775938987732,0.4691516160964966,0.4694453477859497,0.46909141540527344,0.4695713520050049,0.46945205330848694,0.46888336539268494,0.4687604606151581,0.4683011472225189,0.46858611702919006,0.46879786252975464,0.46836644411087036,0.4683762192726135,0.46834948658943176,0.4677698314189911,0.46807602047920227,0.46794429421424866,0.46775031089782715,0.46763283014297485,0.46781492233276367,0.4672579765319824,0.4671856462955475,0.46685221791267395,0.4666827321052551,0.46744149923324585,0.46680977940559387,0.46671169996261597,0.46642497181892395,0.466075599193573,0.46598923206329346,0.4661741852760315,0.465524822473526,0.4657191038131714,0.46476712822914124,0.4659407436847687,0.4657951593399048,0.46552523970603943,0.46531009674072266,0.4653870761394501,0.46488678455352783,0.4660875201225281,0.46530720591545105,0.4656531512737274,0.46549418568611145,0.46507689356803894,0.46503108739852905,0.46448084712028503,0.4647504687309265,0.46436434984207153,0.4637865424156189,0.46404194831848145,0.46377530694007874,0.4644429087638855,0.4637058675289154,0.46444353461265564,0.4634473919868469,0.4631246030330658,0.46314647793769836,0.46320122480392456,0.46312251687049866,0.4626171588897705,0.46296244859695435,0.4635971784591675,0.4628887474536896,0.4621928036212921,0.462638258934021,0.46290335059165955,0.4619799256324768,0.4621146023273468,0.4622024595737457,0.4619992673397064,0.4618658423423767,0.46185898780822754,0.46166449785232544,0.46133992075920105,0.4621415138244629,0.46170830726623535,0.46088454127311707,0.46176376938819885,0.46150484681129456,0.46181002259254456,0.4617096185684204,0.4621058404445648,0.4612916111946106,0.4606289267539978,0.4609229862689972,0.4606682360172272,0.4611678123474121,0.46094393730163574,0.4602479338645935,0.4601134955883026,0.4604581594467163,0.46055442094802856,0.46085062623023987,0.46044328808784485,0.4596375524997711,0.4603860676288605,0.45926156640052795,0.4599277973175049,0.46006613969802856,0.4603187143802643,0.4590221643447876,0.4591498374938965,0.4595300257205963,0.45911121368408203,0.46008357405662537,0.45991480350494385,0.45937037467956543,0.458888977766037,0.45973241329193115,0.45989149808883667,0.45851072669029236,0.45888394117355347,0.4583732783794403,0.45788779854774475,0.4590674042701721,0.458516001701355,0.4580952525138855,0.45794177055358887,0.45766758918762207,0.45867618918418884,0.4589199423789978,0.4580113887786865,0.45807945728302,0.45776161551475525,0.4575407803058624,0.4576745629310608,0.4583130478858948,0.45753031969070435,0.4575707018375397,0.4578361213207245,0.4579441547393799,0.4567117393016815,0.45765388011932373,0.4574616253376007,0.45835474133491516,0.4586600065231323,0.45782798528671265,0.4579426944255829,0.45700952410697937,0.45699819922447205,0.45750728249549866,0.4575667679309845,0.4566791355609894,0.45720627903938293,0.4570155739784241,0.45706212520599365,0.45674386620521545,0.45676136016845703,0.4571903645992279,0.4572526812553406],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Loss\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b5d437e3-286a-4711-a3a0-19f25b798a7f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist2.history['loss'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"loss\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist2.history['val_loss'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_loss\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Loss',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLwVbQmCuAUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "14734eea-ca8d-4410-90e8-33e75a7273d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"11220024-ec0a-4461-90ca-d43a9a9f2b3d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"11220024-ec0a-4461-90ca-d43a9a9f2b3d\")) {                    Plotly.newPlot(                        \"11220024-ec0a-4461-90ca-d43a9a9f2b3d\",                        [{\"line\":{\"color\":\"blue\",\"width\":2},\"mode\":\"lines\",\"name\":\"acc\",\"y\":[0.5116279125213623,0.5372093319892883,0.5411337018013,0.5505813956260681,0.5465116500854492,0.5563953518867493,0.5617732405662537,0.5748546719551086,0.5877906680107117,0.5933139324188232,0.611482560634613,0.6154069900512695,0.6353197693824768,0.6395348906517029,0.6504360437393188,0.6569767594337463,0.6694767475128174,0.6742732524871826,0.6786337494850159,0.6795058250427246,0.6834302544593811,0.6809592843055725,0.6870639324188232,0.6809592843055725,0.6780523061752319,0.6845930218696594,0.679651141166687,0.6816860437393188,0.6773256063461304,0.6805232763290405,0.6813953518867493,0.6768895387649536,0.6813953518867493,0.6790697574615479,0.6819767355918884,0.679215133190155,0.684883713722229,0.6821221113204956,0.684883713722229,0.6853197813034058,0.6838662624359131,0.6850290894508362,0.6873546242713928,0.6853197813034058,0.6876453757286072,0.6866279244422913,0.6857557892799377,0.690116286277771,0.6863372325897217,0.6841569542884827,0.6902616024017334,0.6882267594337463,0.6876453757286072,0.6876453757286072,0.6879360675811768,0.6896802186965942,0.6885174512863159,0.6890988349914551,0.6902616024017334,0.6946220993995667,0.6928778886795044,0.6922965049743652,0.6941860318183899,0.6988372206687927,0.7015988230705261,0.6981104612350464,0.7017441987991333,0.7005813717842102,0.7027616500854492,0.7050871849060059,0.7015988230705261,0.7075581550598145,0.7021802067756653,0.7043604850769043,0.7072674632072449,0.7063953280448914,0.7068313956260681,0.7126453518867493,0.7103197574615479,0.7113372087478638,0.7143895626068115,0.7107558250427246,0.7183139324188232,0.7151162624359131,0.7225290536880493,0.7194767594337463,0.7184593081474304,0.7197674512863159,0.7220930457115173,0.7263081669807434,0.7252907156944275,0.7226744294166565,0.7297965288162231,0.7303779125213623,0.7273255586624146,0.7293604612350464,0.7316860556602478,0.7361918687820435,0.7316860556602478,0.7389534711837769,0.7369186282157898,0.7405523061752319,0.7379360198974609,0.7405523061752319,0.7340116500854492,0.7424418330192566,0.7425872087478638,0.7456395626068115,0.7424418330192566,0.7459302544593811,0.7485465407371521,0.7469476461410522,0.7462209463119507,0.7505813837051392,0.7559593319892883,0.752616286277771,0.7478197813034058,0.7518895268440247,0.7521802186965942,0.755232572555542,0.7562500238418579,0.7549418807029724,0.7577034831047058,0.7566860318183899,0.7531976699829102,0.7601743936538696,0.7581395506858826,0.7664244174957275,0.7643895149230957,0.7613372206687927,0.7659883499145508,0.7640988230705261,0.7620639801025391,0.7658430337905884,0.7645348906517029,0.7664244174957275,0.762499988079071,0.7672964930534363,0.770348846912384,0.7680232524871826,0.7683139443397522,0.7680232524871826,0.7718023061752319,0.770348846912384,0.770348846912384,0.7704941630363464,0.7693313956260681,0.7720929980278015,0.7712209224700928,0.7726744413375854,0.7728197574615479,0.7741279006004333,0.7736918330192566,0.7742732763290405,0.773401141166687,0.773401141166687,0.7783430218696594,0.7749999761581421,0.775581419467926,0.7747092843055725,0.7808139324188232,0.7806686162948608,0.7777616381645203,0.7827034592628479,0.7776162624359131,0.7795057892799377,0.7768895626068115,0.7856104373931885,0.7800872325897217,0.7861918807029724,0.7835755944252014,0.7795057892799377,0.7795057892799377,0.7796511650085449,0.7811046242713928,0.7815406918525696,0.7819767594337463,0.78125,0.7827034592628479,0.7843023538589478,0.7851744294166565,0.7860465049743652,0.7859011888504028,0.7813953757286072,0.7863371968269348,0.7873546481132507,0.7857558131217957,0.786482572555542,0.786482572555542,0.786918580532074,0.7811046242713928,0.7873546481132507,0.7914243936538696,0.7875000238418579,0.791715145111084,0.7875000238418579,0.789098858833313,0.7927325367927551,0.7911337018013,0.7886627912521362,0.7875000238418579,0.7927325367927551,0.7875000238418579,0.7930232286453247,0.791715145111084,0.7915697693824768,0.793749988079071,0.7909883856773376,0.7931686043739319,0.7943313717842102,0.7936046719551086,0.7969476580619812,0.7927325367927551,0.7944767475128174,0.7972383499145508,0.792151153087616,0.7956395149230957,0.7925872206687927,0.7965116500854492,0.7969476580619812,0.797383725643158,0.7959302067756653,0.7984011769294739,0.798982560634613,0.7991279363632202,0.7976744174957275,0.8008720874786377,0.7995639443397522,0.802034854888916,0.7982558012008667,0.7994186282157898,0.800000011920929,0.7997093200683594,0.7988371849060059,0.7992732524871826,0.8005813956260681,0.7976744174957275,0.8033429980278015,0.8011627793312073,0.7995639443397522,0.801598846912384,0.8036337494850159,0.8040697574615479,0.8021802306175232,0.8034883737564087,0.8021802306175232,0.804651141166687,0.8017441630363464,0.8075581192970276,0.8034883737564087,0.8047965168952942,0.8024709224700928,0.8023256063461304,0.8002907037734985,0.804215133190155,0.8077034950256348,0.8053779006004333,0.8065406680107117,0.8074128031730652,0.8062499761581421,0.8031976819038391,0.8053779006004333,0.8047965168952942,0.8104650974273682,0.8114825487136841,0.8069767355918884,0.8065406680107117,0.8085755705833435,0.8062499761581421,0.8074128031730652,0.8085755705833435,0.8062499761581421,0.8093023300170898,0.8106104731559753,0.8074128031730652,0.807267427444458,0.8135174512863159,0.8094476461410522,0.8056685924530029,0.8111918568611145,0.8126453757286072,0.8107557892799377,0.8085755705833435,0.8087209463119507,0.8081395626068115,0.8091569542884827,0.8069767355918884,0.8117732405662537,0.8125,0.8168604373931885,0.8175871968269348,0.8149709105491638,0.8113372325897217,0.8140988349914551,0.8130813837051392,0.8162790536880493,0.8107557892799377,0.8122093081474304,0.8172965049743652,0.8155523538589478,0.819767415523529,0.8180232644081116,0.8142442107200623,0.8119186162948608,0.8175871968269348,0.8143895268440247,0.8200581669807434,0.8140988349914551,0.8170058131217957,0.8127906918525696,0.8142442107200623,0.8184593319892883,0.8148255944252014,0.8184593319892883,0.8164244294166565,0.8191860318183899,0.8184593319892883,0.8161337375640869,0.8143895268440247,0.818168580532074,0.8209302425384521,0.8156976699829102,0.8209302425384521,0.8212209343910217,0.8164244294166565,0.8183139562606812,0.8202034831047058,0.8186046481132507,0.8194767236709595,0.8183139562606812,0.8209302425384521,0.8216570019721985,0.8232558369636536,0.8194767236709595,0.8156976699829102,0.8226743936538696,0.8215116262435913,0.8199127912521362,0.8202034831047058,0.8212209343910217,0.8191860318183899,0.8213662505149841,0.8206395506858826,0.8231104612350464,0.8204941749572754,0.8226743936538696,0.8241279125213623,0.8222383856773376,0.8257267475128174,0.8247092962265015,0.8231104612350464,0.820784866809845,0.8265988230705261,0.8244186043739319,0.8225290775299072,0.8255813717842102,0.8251453638076782,0.8263081312179565,0.8238372206687927,0.827616274356842,0.8271802067756653,0.8239825367927551,0.8238372206687927,0.823401153087616,0.822965145111084,0.824999988079071,0.8242732286453247,0.826017439365387,0.822965145111084,0.8265988230705261,0.8242732286453247,0.8284883499145508,0.8248546719551086,0.8280523419380188,0.8248546719551086,0.8242732286453247,0.8268895149230957,0.827616274356842,0.8306686282157898,0.8242732286453247,0.8268895149230957,0.8296511769294739,0.8267441987991333,0.824999988079071,0.8267441987991333,0.8283430337905884,0.8261628150939941,0.8284883499145508,0.8283430337905884,0.8270348906517029,0.8305232524871826,0.8284883499145508,0.8281976580619812,0.8309593200683594,0.8300871849060059,0.8281976580619812,0.832848846912384,0.8341569900512695,0.8271802067756653,0.8324127793312073,0.8303779363632202,0.8315407037734985,0.8313953280448914,0.8293604850769043,0.8322674632072449,0.8293604850769043,0.8306686282157898,0.8293604850769043,0.8319767713546753,0.8327034711837769,0.8311046361923218,0.8372092843055725,0.8351744413375854,0.8315407037734985,0.831250011920929,0.8369185924530029,0.8315407037734985,0.831250011920929,0.8322674632072449,0.8334302306175232,0.8369185924530029,0.8318313956260681,0.8321220874786377,0.8322674632072449,0.832848846912384,0.8340116143226624,0.8361918330192566,0.8322674632072449,0.8303779363632202,0.8273255825042725,0.8305232524871826,0.8370639681816101,0.8327034711837769,0.831250011920929,0.8343023061752319,0.8309593200683594,0.8315407037734985,0.8325581550598145,0.8338662981987,0.8363372087478638,0.8337209224700928,0.8382267355918884,0.8347383737564087,0.8343023061752319,0.8311046361923218,0.835465133190155,0.8344476819038391,0.835465133190155,0.8367732763290405,0.8379360437393188,0.838517427444458,0.838081419467926,0.835901141166687,0.8374999761581421,0.8406976461410522,0.8360465168952942,0.835465133190155,0.8367732763290405,0.8290697932243347,0.8383721113204956,0.8347383737564087,0.8383721113204956,0.8379360437393188,0.8383721113204956,0.8408430218696594,0.8360465168952942,0.8382267355918884,0.8329941630363464,0.8382267355918884,0.8351744413375854,0.8376453518867493,0.8401162624359131,0.8422965407371521,0.8431686162948608,0.8370639681816101,0.8377906680107117,0.8404069542884827,0.8430232405662537,0.84375,0.835465133190155,0.8424418568611145,0.8446220755577087,0.8386628031730652,0.8401162624359131,0.8383721113204956,0.8386628031730652,0.8425872325897217,0.8404069542884827,0.84375,0.8382267355918884,0.841133713722229,0.8441860675811768,0.8441860675811768,0.8436046242713928,0.8422965407371521,0.8420057892799377,0.8446220755577087,0.8405523300170898,0.8401162624359131,0.8406976461410522],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"width\":2},\"mode\":\"lines\",\"name\":\"val_acc\",\"y\":[0.5127906799316406,0.5209302306175232,0.5232558250427246,0.5279069542884827,0.5337209105491638,0.5470930337905884,0.5569767355918884,0.5732558369636536,0.5860465168952942,0.5970930457115173,0.6203488111495972,0.6406976580619812,0.6581395268440247,0.6691860556602478,0.6813953518867493,0.6872093081474304,0.6895349025726318,0.690116286277771,0.6906976699829102,0.6941860318183899,0.6930232644081116,0.6924418807029724,0.6866279244422913,0.6843023300170898,0.6860465407371521,0.6837209463119507,0.6819767355918884,0.679651141166687,0.6813953518867493,0.6813953518867493,0.679651141166687,0.680232584476471,0.6790697574615479,0.679651141166687,0.6808139681816101,0.680232584476471,0.6819767355918884,0.6819767355918884,0.679651141166687,0.680232584476471,0.680232584476471,0.680232584476471,0.680232584476471,0.6808139681816101,0.6813953518867493,0.6825581192970276,0.6837209463119507,0.6825581192970276,0.6831395626068115,0.6837209463119507,0.6860465407371521,0.6854650974273682,0.6872093081474304,0.6889534592628479,0.6889534592628479,0.6895349025726318,0.6889534592628479,0.690116286277771,0.6912790536880493,0.6918604373931885,0.6912790536880493,0.6918604373931885,0.6930232644081116,0.6941860318183899,0.6941860318183899,0.694767415523529,0.6965116262435913,0.6970930099487305,0.6982558369636536,0.699999988079071,0.7011628150939941,0.7023255825042725,0.7017441987991333,0.7023255825042725,0.7040697932243347,0.7058139443397522,0.7069767713546753,0.705232560634613,0.7063953280448914,0.705232560634613,0.7063953280448914,0.7075581550598145,0.7075581550598145,0.7093023061752319,0.7093023061752319,0.7093023061752319,0.7093023061752319,0.7122092843055725,0.7127906680107117,0.7139534950256348,0.7127906680107117,0.7151162624359131,0.7156976461410522,0.7162790894508362,0.7174418568611145,0.7186046242713928,0.7191860675811768,0.7191860675811768,0.7197674512863159,0.7197674512863159,0.7209302186965942,0.7186046242713928,0.7203488349914551,0.7215116024017334,0.7209302186965942,0.7215116024017334,0.7215116024017334,0.7220930457115173,0.7232558131217957,0.7220930457115173,0.7232558131217957,0.7232558131217957,0.724418580532074,0.7238371968269348,0.7273255586624146,0.7284883856773376,0.7290697693824768,0.7290697693824768,0.7313953638076782,0.7319767475128174,0.7313953638076782,0.7325581312179565,0.7331395149230957,0.7331395149230957,0.7343023419380188,0.7337209582328796,0.734883725643158,0.7354651093482971,0.7366279363632202,0.7372093200683594,0.7383720874786377,0.7383720874786377,0.7383720874786377,0.7389534711837769,0.7401162981987,0.7418604493141174,0.7424418330192566,0.7424418330192566,0.7441860437393188,0.744767427444458,0.744767427444458,0.7441860437393188,0.7430232763290405,0.7430232763290405,0.7424418330192566,0.7441860437393188,0.7453488111495972,0.7453488111495972,0.7459302544593811,0.7465116381645203,0.7470930218696594,0.7482557892799377,0.7494186162948608,0.7494186162948608,0.7494186162948608,0.7494186162948608,0.7505813837051392,0.7534883618354797,0.7523255944252014,0.7529069781303406,0.7569767236709595,0.7534883618354797,0.7558139562606812,0.7558139562606812,0.7569767236709595,0.7563953399658203,0.7569767236709595,0.7569767236709595,0.7575581669807434,0.7575581669807434,0.7587209343910217,0.7581395506858826,0.7610465288162231,0.7622092962265015,0.7627906799316406,0.7633720636367798,0.7616279125213623,0.7627906799316406,0.7639535069465637,0.7627906799316406,0.7633720636367798,0.765116274356842,0.7656976580619812,0.765116274356842,0.7662790417671204,0.7674418687820435,0.7680232524871826,0.7691860198974609,0.7691860198974609,0.7697674632072449,0.770348846912384,0.770348846912384,0.7697674632072449,0.7697674632072449,0.7697674632072449,0.770348846912384,0.7715116143226624,0.7726744413375854,0.7732558250427246,0.7726744413375854,0.7726744413375854,0.7720929980278015,0.7726744413375854,0.7732558250427246,0.7738372087478638,0.7732558250427246,0.775581419467926,0.7749999761581421,0.7749999761581421,0.7749999761581421,0.7749999761581421,0.7749999761581421,0.775581419467926,0.7767441868782043,0.7749999761581421,0.7749999761581421,0.7749999761581421,0.7767441868782043,0.7767441868782043,0.7767441868782043,0.7761628031730652,0.7767441868782043,0.7767441868782043,0.7767441868782043,0.7779069542884827,0.7773255705833435,0.7784883975982666,0.7790697813034058,0.7796511650085449,0.7802325487136841,0.7802325487136841,0.7819767594337463,0.7813953757286072,0.7784883975982666,0.7819767594337463,0.7813953757286072,0.7819767594337463,0.7819767594337463,0.7831395268440247,0.7825581431388855,0.7808139324188232,0.7813953757286072,0.7819767594337463,0.7813953757286072,0.7813953757286072,0.7813953757286072,0.7813953757286072,0.7819767594337463,0.7819767594337463,0.7819767594337463,0.7813953757286072,0.7813953757286072,0.7843023538589478,0.7825581431388855,0.7843023538589478,0.7831395268440247,0.7825581431388855,0.7843023538589478,0.7843023538589478,0.7837209105491638,0.7831395268440247,0.7854651212692261,0.7854651212692261,0.7866278886795044,0.7866278886795044,0.7877907156944275,0.7872093319892883,0.7877907156944275,0.7883720993995667,0.7872093319892883,0.7883720993995667,0.7883720993995667,0.7883720993995667,0.7883720993995667,0.7883720993995667,0.7883720993995667,0.7883720993995667,0.7889534831047058,0.789534866809845,0.7889534831047058,0.7889534831047058,0.7883720993995667,0.7889534831047058,0.789534866809845,0.789534866809845,0.7889534831047058,0.789534866809845,0.789534866809845,0.7901162505149841,0.7906976938247681,0.7906976938247681,0.7906976938247681,0.7906976938247681,0.7906976938247681,0.7901162505149841,0.7906976938247681,0.7901162505149841,0.789534866809845,0.7889534831047058,0.7901162505149841,0.789534866809845,0.7889534831047058,0.7906976938247681,0.7889534831047058,0.789534866809845,0.789534866809845,0.789534866809845,0.7901162505149841,0.7912790775299072,0.789534866809845,0.7889534831047058,0.7906976938247681,0.7901162505149841,0.7912790775299072,0.7889534831047058,0.7883720993995667,0.7889534831047058,0.7889534831047058,0.7901162505149841,0.7901162505149841,0.7901162505149841,0.7901162505149841,0.7912790775299072,0.7912790775299072,0.7924418449401855,0.7918604612350464,0.7924418449401855,0.7918604612350464,0.7918604612350464,0.7924418449401855,0.7912790775299072,0.7906976938247681,0.7912790775299072,0.7906976938247681,0.7912790775299072,0.7912790775299072,0.7912790775299072,0.7918604612350464,0.7912790775299072,0.7924418449401855,0.7930232286453247,0.7924418449401855,0.7924418449401855,0.7912790775299072,0.7918604612350464,0.7924418449401855,0.7924418449401855,0.7924418449401855,0.7924418449401855,0.7936046719551086,0.7936046719551086,0.7924418449401855,0.7936046719551086,0.7936046719551086,0.7930232286453247,0.7930232286453247,0.7924418449401855,0.7941860556602478,0.7918604612350464,0.7930232286453247,0.7924418449401855,0.7918604612350464,0.7918604612350464,0.7924418449401855,0.7924418449401855,0.7924418449401855,0.7924418449401855,0.7918604612350464,0.7924418449401855,0.7930232286453247,0.7936046719551086,0.7936046719551086,0.7936046719551086,0.7930232286453247,0.7924418449401855,0.7930232286453247,0.7936046719551086,0.7936046719551086,0.7941860556602478,0.794767439365387,0.7959302067756653,0.7959302067756653,0.7970930337905884,0.7953488230705261,0.7953488230705261,0.7965116500854492,0.7965116500854492,0.7959302067756653,0.7976744174957275,0.7953488230705261,0.7953488230705261,0.7959302067756653,0.7970930337905884,0.7976744174957275,0.7976744174957275,0.7982558012008667,0.7976744174957275,0.7988371849060059,0.7994186282157898,0.7976744174957275,0.7988371849060059,0.7976744174957275,0.7976744174957275,0.7970930337905884,0.7982558012008667,0.7982558012008667,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7982558012008667,0.7994186282157898,0.7976744174957275,0.7988371849060059,0.7988371849060059,0.7994186282157898,0.7994186282157898,0.7988371849060059,0.7988371849060059,0.7994186282157898,0.7994186282157898,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7982558012008667,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7982558012008667,0.7982558012008667,0.7976744174957275,0.7976744174957275,0.7965116500854492,0.7976744174957275,0.7988371849060059,0.7976744174957275,0.7976744174957275,0.7970930337905884,0.7970930337905884,0.7982558012008667,0.7988371849060059,0.7988371849060059,0.7976744174957275,0.7970930337905884,0.7994186282157898,0.7982558012008667,0.7988371849060059,0.7976744174957275,0.7988371849060059,0.7988371849060059,0.7982558012008667,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7970930337905884,0.7982558012008667,0.7988371849060059,0.7988371849060059,0.7982558012008667,0.7965116500854492,0.7982558012008667,0.7976744174957275,0.7988371849060059,0.7988371849060059,0.7970930337905884,0.7976744174957275,0.7976744174957275,0.7982558012008667,0.7982558012008667,0.7970930337905884,0.7970930337905884,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7988371849060059,0.7994186282157898,0.7982558012008667,0.7994186282157898,0.7994186282157898,0.7988371849060059,0.7982558012008667,0.8005813956260681,0.7988371849060059,0.7988371849060059,0.7965116500854492,0.7965116500854492,0.7976744174957275,0.7976744174957275,0.7988371849060059,0.7988371849060059,0.7976744174957275,0.7976744174957275,0.7988371849060059,0.7976744174957275,0.7970930337905884,0.7970930337905884,0.7982558012008667,0.7976744174957275,0.7970930337905884,0.7970930337905884],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"epochs\"}},\"yaxis\":{\"title\":{\"text\":\"\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('11220024-ec0a-4461-90ca-d43a9a9f2b3d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "h1 = go.Scatter(y=hist2.history['accuracy'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='blue'),\n",
        "                    name=\"acc\"\n",
        "                   )\n",
        "h2 = go.Scatter(y=hist2.history['val_accuracy'],\n",
        "                    mode=\"lines\", line=dict(\n",
        "                    width=2,\n",
        "                    color='red'),\n",
        "                    name=\"val_acc\"\n",
        "                   )\n",
        "\n",
        "data = [h1,h2]\n",
        "layout1 = go.Layout(title='Accuracy',\n",
        "                   xaxis=dict(title='epochs'),\n",
        "                   yaxis=dict(title=''))\n",
        "fig1 = go.Figure(data = data, layout=layout1)\n",
        "fig1.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8CvFTD9uLCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b1673d-376f-495d-e622-3a90c02069b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 506)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 506, 256)     3341056     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 506, 256)     3341056     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 506, 256)     3341056     ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 503, 32)      32800       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 501, 32)      49184       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 499, 32)      65568       ['embedding_5[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 503, 32)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 501, 32)      0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 499, 32)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 251, 32)     0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 250, 32)     0           ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 249, 32)     0           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 8032)         0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 8000)         0           ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 7968)         0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 24000)        0           ['flatten_3[0][0]',              \n",
            "                                                                  'flatten_4[0][0]',              \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           240010      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            22          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,410,752\n",
            "Trainable params: 387,584\n",
            "Non-trainable params: 10,023,168\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "predict_model2 = load_model(filename)\n",
        "predict_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DxcBjiCujlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecedb1d8-84d0-42b7-b709-78ec40dea3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate loss: 0.4566791355609894\n",
            "Validate accuracy: 0.7988371849060059\n"
          ]
        }
      ],
      "source": [
        "score = predict_model2.evaluate([val_X, val_X, val_X], val_Y, verbose=0)\n",
        "print('Validate loss:', score[0])\n",
        "print('Validate accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPN_6619undf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80afdfe-7856-4f89-eff2-a641c99c4730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54/54 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1720,)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "predicted_classes = np.argmax(predict_model2.predict([val_X, val_X, val_X]), axis=-1)\n",
        "predicted_classes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cusWtCj9uqZC"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, predicted_classes)\n",
        "np.savetxt(\"confusion_matrix.csv\", cm, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pitwwJuEusjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "598f82b5-c7bc-4d14-9178-0b9e7b316c19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
        "plt.figure(figsize=(20,14))\n",
        "sn.set(font_scale=1.2) # for label size\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g') # for num predict size\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WBdBaD4uumZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67972bbd-a75c-4f9c-bf64-2a30745c942f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.7612    0.8709    0.8124       860\n",
            "         pos     0.8492    0.7267    0.7832       860\n",
            "\n",
            "    accuracy                         0.7988      1720\n",
            "   macro avg     0.8052    0.7988    0.7978      1720\n",
            "weighted avg     0.8052    0.7988    0.7978      1720\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_true, predicted_classes, target_names=label, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3y5BcVJu4Jv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}